{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing the `spacy` library provides tokenization, part-of-speech tagging, named entity recognition, and text classification. The `Example` class allows the example training data to train the models. By converting the text and its annotations into a format, it can be for model updates."
      ],
      "metadata": {
        "id": "FFa9gVNiHKNH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "qX0tNU14gogt"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.training import Example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `nlpTC` is an instance of the spacy model or pipeline. The `spacy.blank(\"en\")` model is for the English language, making it easy to understand what language is in the training data. A `blank` model doesn't have pre-built pipelines so that custom pipelines will be added. The `textcat` is the text classifier that gets added to the `nlpTC` pipeline using `nlpTC.add_pipe(\"textcat\")`. It categorize text into predefined labels such as \"SPAM\" AND \"HAM\"."
      ],
      "metadata": {
        "id": "m2LVxdDcJPFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a blank model and add text classifier\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")"
      ],
      "metadata": {
        "id": "sC7HcLVTg0RG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This codes are adding a label of \"SPAM' and \"HAM\" to the text classification model, the `textcat`. For the Spam label, it will identify if the data is from unknown person or email and for Ham label is for non-spam or regular emails from known person or email. This model will predict either \"SPAM\" or \"HAM\" for the given input text."
      ],
      "metadata": {
        "id": "V3J16q1uL4rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add labels for classification\n",
        "textcat.add_label(\"SPAM\")\n",
        "textcat.add_label(\"HAM\")"
      ],
      "metadata": {
        "id": "jYF820mohFS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b486a2f8-148d-45ec-b5e5-f8de44281bc5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `train_data` is a list of tuples that holds the training text examples with their corresponding labels. Example: \"This is spam\" is the text string, this is text that will train and `cats\":{\"SPAM\":1, \"HAM\":0}` it specifies which category text belongs. This is how to teach the model to train the data if its SPAM or HAM."
      ],
      "metadata": {
        "id": "_blqOzYRM0gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example training data\n",
        "train_data = [\n",
        "    (\"This is spam\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello, how are you?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You won a million dollars!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Claim your free prize now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Meeting at 10 AM tomorrow\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your invoice is attached\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Exclusive offer just for you!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Get a free iPhone today\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we reschedule our call?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Update your account details\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Limited time deal, buy now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your package has been shipped\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Win a trip to Hawaii now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Important meeting agenda\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You've been selected\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we discuss this project?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "]\n"
      ],
      "metadata": {
        "id": "uEHsvGR7hXJP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `optimizer` is used to update the model's weights during training and the model's pipeline is initialized to setting it up for training. As the training data is fed into the model, an optimizer through a control means adjusts the parameters of the model in a way that reduces errors thereby improving its potential to learn correctly. The `for text, annotations in train_data` started a loop where every training example is utilized for model training. At each iteration, train_data are utilized to go through this loop. For every example, an Example object is created by taking its text and annotations. In that case, text and labels are transformed into a format which can be understood by spaCy. Thus `nlpTC.update([example], sgd=optimizer)` modifies the model with use of the Example object as a passing entity. To some extent it becomes clear that model learns how to connect particular patterns found within the text with SPAM or HAM labels. At this point, through optimizer the internal parameters of the model are readjusted so as to enhance performance."
      ],
      "metadata": {
        "id": "ST7PPGOLOFtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "optimizer = nlpTC.initialize()\n",
        "for text, annotations in train_data:\n",
        "  example = Example.from_dict(nlp.make_doc(text), annotations)\n",
        "  nlpTC.update([example], sgd=optimizer)\n"
      ],
      "metadata": {
        "id": "Pwb1mFBGhWju"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prints the message prior to demonstrating how well the model performs on a new example. The doc is a processed document object that holds the model’s prediction for the input text. A Doc object is returned when applying to any string by using nlpTC (for example, `doc = nlpTC(\"Claim your prize now!\")`), which contains a processed text. Important things like tokenization, category predictions (SPAM or HAM), and other language features extracted by the model are included in this object."
      ],
      "metadata": {
        "id": "-lMcCjjyP8VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the model\n",
        "print(\"Sample Prediction Output with probabilities: \")\n",
        "doc = nlpTC(\"Claim your prize now!\")"
      ],
      "metadata": {
        "id": "C31XZDDchWrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30fff762-661f-4a45-fdd5-7369bbb9af2d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Prediction Output with probabilities: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model, `classify_email(email)`, processes the text and attempts to categorize it. The category scores for SPAM and HAM are extracted from it by processing the input email through the model. For `spam_score =doc.cats[‘SPAM’]`, it gets SPAM score of the model which represents how likely that email is going to be categorized as SPAM. The HAM score shows the probability that an email will not be classified as spam. The code `if spam_score > ham_score:` indicates if the SPAM score is higher than the HAM score, the function will return \"SPAM\", classifying the email as spam. While the else is identifying if the HAM score is higher, the email is classified as \"HAM\" (not spam)."
      ],
      "metadata": {
        "id": "P0GJY5mBP8xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to classify user input emails\n",
        "def classify_email(email):\n",
        "  doc = nlpTC(email)\n",
        "  spam_score =doc.cats['SPAM']\n",
        "  ham_score =doc.cats['HAM']\n",
        "\n",
        "  if spam_score > ham_score:\n",
        "    return \"SPAM\"\n",
        "  else:\n",
        "      return \"HAM\""
      ],
      "metadata": {
        "id": "BFjHvz1zjHGh"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the line `while true`, an infinite loop begins and users can keep testing the model by entering email texts until they type “exit”. Under `user_input`, users are prompted to input a sample email text to classify. When the user types \"exit\", the loop stops and the program ends. The `classify_email` function should be called on user’s input in order to classify the email at hand. This result (SPAM or HAM) will be printed by this function on console interface.\n"
      ],
      "metadata": {
        "id": "nmSfNLpMP9cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Allow users to test the model by inputting their own data\n",
        "while True:\n",
        "  user_input = input(\"Now,enter a sample email for classification (or type 'exit' to quit):\")\n",
        "  if user_input.lower() == 'exit':\n",
        "    break\n",
        "  classification = classify_email(user_input)\n",
        "  print(f\"The email is classified as: {classification}\")"
      ],
      "metadata": {
        "id": "K41CubwEk0Xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5245d3b-2fe2-4b15-fca1-0990fc2cc71e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now,enter a sample email for classification (or type 'exit' to quit):We are excited to inform you that you have been selected to receive an exclusive offer! Click the link below to claim your free gift. This is a limited-time offer, so don’t miss out!\n",
            "The email is classified as: SPAM\n",
            "Now,enter a sample email for classification (or type 'exit' to quit):EXIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses spaCy to perform Named Entity Recognition (NER), extracting and identifying important elements such as names, organizations, and dates from user-provided text."
      ],
      "metadata": {
        "id": "Vfl-e6YtnhOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "9JUqYNk7mgVN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load pre-trained spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "MX1ov62-mnEn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to define the function `‘analyze_text()’`, which will take in an input string and return a list of named entities together with their corresponding labels. The input text is processed into a doc object using spaCy’s pre-trained model (presumably saved in the nlp object). It contains processed text that consists of tokens and named entities. The code  then extracts these named entities from doc.ents attribute and stores them as tuples in entities_list. Each tuple comprises the entity’s text (e.g., for an individual it can be someone’s name or for a corporation it might be a company), and its label (e.g., PERSON, ORG, DATE)."
      ],
      "metadata": {
        "id": "Bd70OrefVGEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After collecting the named entities, the function returns the entities_list for further use. The script then prompts the user to input a text string using the `input()` function. The user's input is passed to the `analyze_text()` function, which processes the text and extracts the named entities and their labels. Finally, the extracted entities are printed, preceded by a heading (\"Named Entities and labels:\") for clarity. The result is a list of named entities found in the user's input, along with the corresponding type of each entity, such as a person's name, an organization, or a date. This process effectively showcases how spaCy can be used for entity recognition in natural language processing applications.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4lxqyZigVLGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to analyze user input text and return entities as list\n",
        "def analyze_text(text):\n",
        "  doc = nlp(text)\n",
        "  entities_list = [(ent.text, ent.label_)for ent in doc.ents]\n",
        "  return entities_list\n",
        "\n",
        "# Allow user input and analyze\n",
        "user_input = input (\"Enter a text for named entity analysis:\")\n",
        "entities = analyze_text(user_input)\n",
        "\n",
        "#Display the result as a list\n",
        "\n",
        "print(\"\\nNamed Entities and labels:\")\n",
        "print(entities)"
      ],
      "metadata": {
        "id": "r1KDKyeGmzvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6a6a24-a56e-485a-9be4-98aa54ce1a18"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text for named entity analysis:Barack Obama was born in Honolulu, Hawaii, on August 4, 1961. He served as the 44th President of the United States from 2009 to 2017. During his presidency, he lived in Washington, D.C., and frequently visited Chicago, Illinois.\n",
            "\n",
            "Named Entities and labels:\n",
            "[('Barack Obama', 'PERSON'), ('Honolulu', 'GPE'), ('Hawaii', 'GPE'), ('August 4, 1961', 'DATE'), ('44th', 'ORDINAL'), ('the United States', 'GPE'), ('2009', 'DATE'), ('2017', 'DATE'), ('Washington', 'GPE'), ('D.C.', 'GPE'), ('Chicago', 'GPE'), ('Illinois', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs part-of-speech tagging using spaCy. It begins by loading the pre-trained small English model `en_core_web_sm`. This model is capable of analyzing text, and in this case, it is used to tokenize the user input and assign a part-of-speech (POS) tag to each token. The analyze_text function processes the input text using the nlp model and iterates through each token in the doc object (which holds the processed text). For each token, the text and its corresponding POS tag are stored in a list called pos_list. This list is then returned and displayed to the user, showing the individual tokens alongside their grammatical roles, such as nouns, verbs, adjectives, etc."
      ],
      "metadata": {
        "id": "zhcU97sruTaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "OgF8mvLEngLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load pre-trained spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "A4i0PPVQoeCc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to analyze user input text and return tokens with POS tags as a list\n",
        "def analyze_text(text):\n",
        "  doc = nlp(text)\n",
        "  pos_list = [(token.text, token.pos_)for token in doc]\n",
        "  return pos_list\n",
        "\n",
        "# Allow user input and analyze\n",
        "user_input = input (\"Enter a text for named entity analysis:\")\n",
        "pos_tags = analyze_text(user_input)\n",
        "\n",
        "#Display the result as a list\n",
        "\n",
        "print(\"\\nTokens and POS Tags:\")\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "id": "zAkDF5nNofWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd7e2fb-48b3-4777-ad67-b061a9a2002e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text for named entity analysis:Barack Obama was born in Honolulu, Hawaii, on August 4, 1961. He served as the 44th President of the United States from 2009 to 2017. During his presidency, he lived in Washington, D.C., and frequently visited Chicago, Illinois.\n",
            "\n",
            "Tokens and POS Tags:\n",
            "[('Barack', 'PROPN'), ('Obama', 'PROPN'), ('was', 'AUX'), ('born', 'VERB'), ('in', 'ADP'), ('Honolulu', 'PROPN'), (',', 'PUNCT'), ('Hawaii', 'PROPN'), (',', 'PUNCT'), ('on', 'ADP'), ('August', 'PROPN'), ('4', 'NUM'), (',', 'PUNCT'), ('1961', 'NUM'), ('.', 'PUNCT'), ('He', 'PRON'), ('served', 'VERB'), ('as', 'ADP'), ('the', 'DET'), ('44th', 'ADJ'), ('President', 'PROPN'), ('of', 'ADP'), ('the', 'DET'), ('United', 'PROPN'), ('States', 'PROPN'), ('from', 'ADP'), ('2009', 'NUM'), ('to', 'ADP'), ('2017', 'NUM'), ('.', 'PUNCT'), ('During', 'ADP'), ('his', 'PRON'), ('presidency', 'NOUN'), (',', 'PUNCT'), ('he', 'PRON'), ('lived', 'VERB'), ('in', 'ADP'), ('Washington', 'PROPN'), (',', 'PUNCT'), ('D.C.', 'PROPN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('frequently', 'ADV'), ('visited', 'VERB'), ('Chicago', 'PROPN'), (',', 'PUNCT'), ('Illinois', 'PROPN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code focuses on text classification using a blank spaCy model for sentiment analysis. A new model nlpTC is created from scratch, and a text classification component (textcat) is added to the pipeline. The classifier is trained to predict two categories: \"POSITIVE\" and \"NEGATIVE\", which are added as labels. Training data is defined with a set of example texts labeled as \"SPAM\" or \"HAM\", although these could be adapted for sentiment analysis. The train_model function is responsible for training the model over multiple iterations , where it shuffles the training data and updates the model’s parameters using the optimizer. In each epoch, the model is updated, and the training loss is tracked. After training, the predict_sentiment function is used to predict the sentiment of user-provided text. The function returns the sentiment categories with their respective confidence scores. When the user inputs a sentence, the model predicts whether it is positive or negative."
      ],
      "metadata": {
        "id": "bkQZk45dUtjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "import random\n",
        "\n",
        "# Load a blank model and add text classifier\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGATIVE\")\n",
        "\n",
        "\n",
        "#Example training data\n",
        "train_data = [\n",
        "    (\"This is spam\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello, how are you?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You won a million dollars!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Claim your free prize now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Meeting at 10 AM tomorrow\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your invoice is attached\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Exclusive offer just for you!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Get a free iPhone today\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we reschedule our call?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Update your account details\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Limited time deal, buy now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your package has been shipped\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Win a trip to Hawaii now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Important meeting agenda\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You've been selected\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we discuss this project?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "]"
      ],
      "metadata": {
        "id": "2V1Ewkuspb3c"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "def train_model(data, n_iter=10):\n",
        "  random.shuffle(data)\n",
        "  optimizer = nlp.begin_training()\n",
        "  for epoch in range(n_iter):\n",
        "    losses = {}\n",
        "    for text, annotation in data:\n",
        "      doc = nlp.make_doc(text)\n",
        "      example = Example.from_dict(doc, annotations)\n",
        "      nlp.update([example], drop=0.5, losses=losses)\n",
        "      print(f\"Epoch {epoch+1}/{n_iter} - losses: {losses}\")\n",
        "\n",
        "      train_model(train_data)\n",
        "\n",
        "  # Function to predict sentiment\n",
        "      def predict_sentiment(text):\n",
        "        doc = nlp(text)\n",
        "        return doc.cats\n",
        "\n",
        "# user input and sentiment analysis\n",
        "user_input = input(\"Enter a text for sentiment analysis: \")\n",
        "prediction = predict_sentiment(user_input)\n",
        "print(\"\\nSentiment Prediction: \")\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "6VCcLAA4p0rj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61be4ac2-b845-4397-cc20-feb3033d6ff6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text for sentiment analysis: I had an amazing experience at the restaurant last night! The food was delicious, and the service was exceptional\n",
            "\n",
            "Sentiment Prediction: \n",
            "{'POSITIVE': 0.5, 'NEGATIVE': 0.5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs text summarization. Once again, the pre-trained en_core_web_sm model is loaded. The summarize function processes the input text and uses token frequency to score sentences. For each sentence in the text, it counts how many important tokens (excluding stop words and punctuation) appear in the sentence. The sentences with the highest scores are selected as the most important and returned as the summary. The user inputs the text they want to summarize, and the function returns the top N sentences based on token frequency, giving a shorter version of the text."
      ],
      "metadata": {
        "id": "nPuo7epxsTv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "#Load pre-trained spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "zE90eJrWsUYq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize text\n",
        "def summarize(text, n_sentence = 2):\n",
        "  doc = nlp(text)\n",
        "  sentence_scores = Counter()\n",
        "\n",
        "  # Score sentence based on token frequency\n",
        "  for sent in doc.sents:\n",
        "    for token in sent:\n",
        "      if not token.is_stop and not token.is_punct:\n",
        "        sentence_scores[sent] += 1\n",
        "\n",
        "  # Select top N sentences\n",
        "  top_sentences = [sent.text for sent, score in sentence_scores.most_common(n_sentence)]\n",
        "  return \" \".join(top_sentences)\n",
        "\n",
        "# User input for summarization\n",
        "user_text = input(\"Enter the text you want to summarize: \")\n",
        "summary = summarize(user_text)\n",
        "print(\"\\nSummary: \")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "94A7_xYas8n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4aca4d7-2539-495e-c09d-3fa53ffbbfef"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text you want to summarize: Barack Obama was born in Honolulu, Hawaii, on August 4, 1961. He served as the 44th President of the United States from 2009 to 2017. During his presidency, he lived in Washington, D.C., and frequently visited Chicago, Illinois.\n",
            "\n",
            "Summary: \n",
            "Barack Obama was born in Honolulu, Hawaii, on August 4, 1961. During his presidency, he lived in Washington, D.C., and frequently visited Chicago, Illinois.\n"
          ]
        }
      ]
    }
  ]
}