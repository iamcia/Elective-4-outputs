{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXpsft4Ki93K"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HtBsoo0jCm1"
      },
      "source": [
        "# Subset 1: Small Size, Low Quality (20 manual data)\n",
        "\n",
        "# Subset 2: Medium Size, Low Quality (50 manual data)\n",
        "\n",
        "# Subset 3: Large Size, Low Quality (100 manual data)\n",
        "\n",
        "# Subset 4: Small Size, High Quality (20 manual data)\n",
        "\n",
        "# Subset 5: Large Size, High Quality (100 manual data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDpYCxelWe_9"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bBBblOnYl5v"
      },
      "source": [
        "# **NLP Application Models**\n",
        "\n",
        "\n",
        "# 1.   Text Classification\n",
        "# 2.   Named Entity Recognition\n",
        "# 3.   Part of Speech Tagging\n",
        "# 4.   Sentiment Analysis\n",
        "# 5.   Text Summarizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdt5LcF_YtPy"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4mzn8hhkHU-"
      },
      "source": [
        "# **Subset 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5LDjak8blLm"
      },
      "source": [
        "## Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model is a text classification application using SpaCy with a main objective of classifying emails into ‘SPAM’ and ‘HAM’ classes. First, a blank model is created in SpaCy and then a text classifier is attached with two orthodox models which are SPAM and HAM. The training data consists of 20 pieces of short email like of texts with mapped categories and no other processing is done other than making text lowercase. The data is then divided into 3 portions and used for training, validation and testing, with 60% reserved for training, 20% for validation and 20% for testing.\n",
        "\n",
        "Due to the small dataset size the model is trained in small batches, and the loss for each iteration is printed as a way to monitor the progress. Once the training ends, the performance of the model can be quantified with user inputs or available samples while providing the probabilities for each output label.\n",
        "\n",
        "The code also contains a method for predicting the class for a new email given the model built whether the email is SPAM or HAM. Last but not least, tests are performed on the test dataset in order to obtain other relevant metrics such as accuracy, precision, recall and F1 score. This makes it possible to assess the performance of this model in a numerical way. In addition to this, the model also allows one to type in text and attempts to provide the class of the model based on its internal logic. Therefore the training set contains 20 short available labelled samples."
      ],
      "metadata": {
        "id": "p9DL3Mgf0DiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "# Create a blank SpaCy model and add the text classifier component\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"SPAM\")\n",
        "textcat.add_label(\"HAM\")\n",
        "\n",
        "# Define a minimal preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Example training data\n",
        "train_data = [\n",
        "    (\"This is spam\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello, how are you?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You WON a million dollars!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Claim YOUR free PRIZE now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Meeting at 10 AM tomorrow\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your INVOICE is attached\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"EXCLUSIVE offer just for you!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Get a FREE iPhone today\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we reschedule our call?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Update your account details\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Limited time deal, buy now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your PACKAGE has been shipped\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Win a TRIP to Hawaii now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Important meeting AGENDA\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You've been SELECTED\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we DISCUSS this project?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I am waiting machan. Call me once you free.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}})\n",
        "]\n",
        "\n",
        "# Apply minimal text preprocessing\n",
        "train_data = [(preprocess_text(text), annotations) for text, annotations in train_data]\n",
        "\n",
        "# Prepare training data into SpaCy's Example format\n",
        "train_examples = []\n",
        "for text, annotations in train_data:\n",
        "    doc = nlpTC.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    train_examples.append(example)\n",
        "\n",
        "# Split data into training, validation, and testing sets\n",
        "train_examples, test_examples = train_test_split(train_examples, test_size=0.2, random_state=42)\n",
        "train_examples, val_examples = train_test_split(train_examples, test_size=0.25, random_state=42)  # 20% of the remaining data is used for validation\n",
        "\n",
        "# Print the split data to visualize each set\n",
        "print(\"TRAINING SET (60% of the data):\")\n",
        "for example in train_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "print(\"\\nVALIDATION SET (20% of the data):\")\n",
        "for example in val_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "print(\"\\nTESTING SET (20% of the data):\")\n",
        "for example in test_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "# Training the model with more iterations for small datasets\n",
        "n_iter = 10  # Set iterations\n",
        "optimizer = nlpTC.initialize()\n",
        "\n",
        "for i in range(n_iter):\n",
        "    losses = {}\n",
        "    for batch in spacy.util.minibatch(train_examples, size=2):  # Small batch size for small data\n",
        "        for example in batch:\n",
        "            nlpTC.update([example], sgd=optimizer, losses=losses)\n",
        "    print(f\"Iteration {i+1}/{n_iter} - Loss: {losses['textcat']}\")\n",
        "\n",
        "# Testing the model\n",
        "print(\"\\nSample Prediction Output with probabilities:\")\n",
        "doc = nlpTC(\"Claim your prize now!\")\n",
        "print(doc.cats)\n",
        "\n",
        "# Function to classify user input emails\n",
        "def classify_email(email):\n",
        "    email = preprocess_text(email)\n",
        "    doc = nlpTC(email)\n",
        "    spam_score = doc.cats['SPAM']\n",
        "    ham_score = doc.cats['HAM']\n",
        "\n",
        "    if spam_score > ham_score:\n",
        "        return \"SPAM\"\n",
        "    else:\n",
        "        return \"HAM\"\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score on the test set\n",
        "true_labels = [1 if example.reference.cats['SPAM'] == 1 else 0 for example in test_examples]\n",
        "predicted_labels = [1 if classify_email(example.reference.text) == 'SPAM' else 0 for example in test_examples]\n",
        "\n",
        "# Calculate and print metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nAccuracy: {accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")\n",
        "\n",
        "# Allow users to test the model by inputting their own data\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter a sample email for classification (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    classification = classify_email(user_input)\n",
        "    print(f\"The email is classified as: {classification}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBOxxdMZxYZY",
        "outputId": "db556371-a627-4dd0-db46-695b7e781a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING SET (60% of the data):\n",
            "Text: limited time deal, buy now! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: win a trip to hawaii now - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: update your account details - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: i am waiting machan. call me once you free. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: your package has been shipped - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: exclusive offer just for you! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: hello! how's you and how did saturday go? i was just texting to see if you'd decided to do anything tomo. not that i'm trying to invite myself or anything! - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: you won a million dollars! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: meeting at 10 am tomorrow - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: get a free iphone today - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: claim your free prize now! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: important meeting agenda - Label: {'SPAM': 0, 'HAM': 1}\n",
            "\n",
            "VALIDATION SET (20% of the data):\n",
            "Text: can we reschedule our call? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: your invoice is attached - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: did you catch the bus ? are you frying an egg ? did you make a tea? are you eating your mom's left over dinner ? do you feel my love ? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: congratulations! you've been selected - Label: {'SPAM': 1, 'HAM': 0}\n",
            "\n",
            "TESTING SET (20% of the data):\n",
            "Text: this is spam - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: urgent! you have won a 1 week free membership in our £100,000 prize jackpot! txt the word: claim to no: 81010 t&c www.dbuk.net lccltd pobox 4403ldnw1a7rw18 - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: can we discuss this project? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: hello, how are you? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Iteration 1/10 - Loss: 2.848337769508362\n",
            "Iteration 2/10 - Loss: 0.9146533198654652\n",
            "Iteration 3/10 - Loss: 0.03778356610746414\n",
            "Iteration 4/10 - Loss: 0.00038377565243763456\n",
            "Iteration 5/10 - Loss: 2.2740763995443558e-05\n",
            "Iteration 6/10 - Loss: 6.218560628212799e-06\n",
            "Iteration 7/10 - Loss: 3.369693367005766e-06\n",
            "Iteration 8/10 - Loss: 2.3635946249100925e-06\n",
            "Iteration 9/10 - Loss: 1.8246595967497115e-06\n",
            "Iteration 10/10 - Loss: 1.4645256714018728e-06\n",
            "\n",
            "Sample Prediction Output with probabilities:\n",
            "{'SPAM': 0.9998730421066284, 'HAM': 0.0001269534113816917}\n",
            "\n",
            "Accuracy: 75.0000%\n",
            "Precision: 83.3333%\n",
            "Recall: 75.0000%\n",
            "F1 Score: 73.3333%\n",
            "\n",
            "Enter a sample email for classification (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSEWl8yRg7nW"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model assesses the performance of named entity recognition (NER) based on spaCy’s pre-trained model of `en_core_web_sm`. The training data comprises 20 sentences which contain various named entities including organizations, locations, dates, people, etc. with document entity annotations which comprise of indexes of beginning and ending characters of the respective entity and their respective labels. The data is first processed by changing all the texts to output lower case characters only to standardize the text. The NER model takes in each sentence and predicts the entities therein by recognizing and labeling a span of text. For the purposes of evaluation, entities of both the ground truth dataset (in this case annotated by the ghostwriter) and those predicted by the spaCy tool are collected.\n",
        "\n",
        "In order to assess the results of the model below, the entities are assessed by converting the outputs into a binary system that registers 1 if both counts of the true and the predicted entity are matched and registries 0 otherwise. The precision, recall and F1 measures are then calculated from the matches of the ground truth and the outputs of the model with respect to entities present. Precision is the fraction of relevant entities retrieved out of the total number of entities presented by the model, recall is the fraction of relevant entities retrieved over the total amount of relevant entities present and the F1 score is the average of precision and recall with respect to the NER model."
      ],
      "metadata": {
        "id": "7yyJlWvuywh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load a pre-trained NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample training data (text and true entity annotations)\n",
        "training_data = [\n",
        "    (\"Microsoft announced a new AI initiative in Seattle.\", [(0, 9, \"ORG\"), (39, 46, \"GPE\")]),\n",
        "    (\"Google I/O will take place in May 2023.\", [(0, 10, \"EVENT\"), (29, 37, \"DATE\")]),\n",
        "    (\"The unemployment rate in the U.S. dropped to 3.5%.\", [(34, 38, \"PERCENT\"), (27, 31, \"GPE\")]),\n",
        "    (\"The Chinese economy grew by 5% last year.\", [(4, 11, \"NORP\")]),\n",
        "    (\"Sundar Pichai is the CEO of Google.\", [(0, 13, \"PERSON\"), (28, 34, \"ORG\")]),\n",
        "    (\"Tesla secured $2 billion in new funding.\", [(14, 22, \"MONEY\")]),\n",
        "    (\"Amazon is opening a new office in Vancouver.\", [(0, 6, \"ORG\"), (36, 45, \"GPE\")]),\n",
        "    (\"Samsung released its new Galaxy S22 phone.\", [(0, 7, \"ORG\"), (23, 32, \"PRODUCT\")]),\n",
        "    (\"The Pacific Ocean is the largest body of water on Earth.\", [(4, 17, \"LOC\")]),\n",
        "    (\"The headquarters of IBM is in New York City.\", [(21, 24, \"ORG\"), (31, 44, \"GPE\")]),\n",
        "\n",
        "    (\"Satya Nadella leads Microsoft Corporation.\", [(0, 12, \"PERSON\"), (19, 38, \"ORG\")]),\n",
        "    (\"The FIFA World Cup will be held in Qatar in 2022.\", [(4, 18, \"EVENT\"), (34, 39, \"GPE\"), (43, 47, \"DATE\")]),\n",
        "    (\"Apple plans to invest $10 billion in manufacturing.\", [(23, 32, \"MONEY\")]),\n",
        "    (\"A new skyscraper is being built in Dubai.\", [(33, 38, \"GPE\")]),\n",
        "    (\"70% of the world's population is now online.\", [(0, 3, \"PERCENT\")]),\n",
        "    (\"Elon Musk founded SpaceX and Tesla.\", [(0, 9, \"PERSON\"), (17, 23, \"ORG\"), (28, 33, \"ORG\")]),\n",
        "    (\"The startup raised $50 million in Series B.\", [(15, 25, \"MONEY\")]),\n",
        "    (\"The next Apple event is scheduled for March 25th.\", [(9, 14, \"ORG\"), (39, 48, \"DATE\")]),\n",
        "    (\"The new company is aiming for a 15% market share.\", [(28, 31, \"PERCENT\")]),\n",
        "    (\"Apple's iPhone 14 is expected to launch in 2023.\", [(0, 5, \"ORG\"), (7, 15, \"PRODUCT\"), (46, 50, \"DATE\")])\n",
        "]\n",
        "\n",
        "# Preprocess: Convert all texts to lowercase\n",
        "preprocessed_data = [(text.lower(), entities) for text, entities in training_data]\n",
        "\n",
        "# Initialize lists for storing true and predicted entities\n",
        "all_true_entities = []\n",
        "all_pred_entities = []\n",
        "\n",
        "# Iterate through training data\n",
        "for text, true_entities in training_data:\n",
        "    # Run NER model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Predicted entities from the model\n",
        "    pred_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Store true and predicted entities for all examples\n",
        "    all_true_entities.extend([(ent[0], ent[1], ent[2]) for ent in true_entities])\n",
        "    all_pred_entities.extend([(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents])\n",
        "\n",
        "# Convert to binary classification (1 for correct entity, 0 for incorrect)\n",
        "y_true = [1 if ent in all_true_entities else 0 for ent in all_pred_entities]\n",
        "y_pred = [1 for _ in all_pred_entities]  # Assuming all predictions are correct for now\n",
        "\n",
        "# Calculate Precision, Recall, F1\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oBYDidA1wbe",
        "outputId": "297e6d56-7a95-47f2-cdde-3cb24d971bd5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 36.8421%\n",
            "Recall: 100.0000%\n",
            "F1 Score: 53.8462%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "028qbU91kU8i"
      },
      "source": [
        "## Part of Speech Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a model for tagging parts of speech based on the built-in POS tagger trained in advance by spaCy. The dataset consists of 20 sentences containing POS tags for every word within them which are gathered manually. The textual content is preprocessed to lower case and jumbled in order to maintain randomness in the training, validation, and test sets - as well as any other data. Decidedly 60% was assigned for the training process, 20% for the validation while the last 20% was allocated for testing.\n",
        "\n",
        "The critical section is the process_data function, which takes each sentence in a dataset, processes it with spaCy, obtains POS tags suggested by a model, and performs a comparison with the corresponding true ones. The function named ensure_equal_length guarantees that both true tags and predicted tags lists are of the same size and any bulge or excess is cut off to prevent inaccuracies within the metric evaluation process.\n",
        "\n",
        "Accuracy, precision, recall, and F1 score for training validation and testing are calculated separately using the evaluate_metrics function. This output then combines the results of all three sets in order to provide the overall assessment of the model’s effectiveness by taking the mean values of the metrics across all those three sets. Finally, the obtained values are displayed in the last stage, providing the various metrics of the model concerning accurate, precision, recall, and F1 score for speech tagging classification in the three datasets."
      ],
      "metadata": {
        "id": "oAuNBd1O1l1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load spaCy's POS tagging model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample expanded training data: list of (text, true_pos_tags) pairs\n",
        "training_data = [\n",
        "    (\"She sells seashells by the seashore.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The quick brown fox jumps over the lazy dog.\", ['DET', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"I love coding in Python.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'PROPN']),\n",
        "    (\"Birds fly in the sky.\", ['NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"Alice and Bob went to the market.\", ['PROPN', 'CCONJ', 'PROPN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"Reading books is fun.\", ['VERB', 'NOUN', 'AUX', 'ADJ']),\n",
        "    (\"My car is very fast.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"We are going to the zoo.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"It is raining today.\", ['PRON', 'AUX', 'VERB', 'NOUN']),\n",
        "    (\"Programming languages are interesting.\", ['NOUN', 'NOUN', 'AUX', 'ADJ']),\n",
        "\n",
        "    (\"The cat sleeps on the mat.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"John likes to play soccer.\", ['PROPN', 'VERB', 'PART', 'VERB', 'NOUN']),\n",
        "    (\"She is learning French.\", ['PRON', 'AUX', 'VERB', 'PROPN']),\n",
        "    (\"The weather is nice today.\", ['DET', 'NOUN', 'AUX', 'ADJ', 'NOUN']),\n",
        "    (\"He bought a new laptop yesterday.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN']),\n",
        "    (\"They are swimming in the pool.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The pizza smells delicious.\", ['DET', 'NOUN', 'VERB', 'ADJ']),\n",
        "    (\"Can you help me with this project?\", ['AUX', 'PRON', 'VERB', 'PRON', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"This task is quite difficult.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"He enjoys reading books.\", ['PRON', 'VERB', 'VERB', 'NOUN']),\n",
        "]\n",
        "\n",
        "# Preprocess text: convert to lowercase and shuffle the training data\n",
        "training_data = [(text.lower(), tags) for text, tags in training_data]\n",
        "random.shuffle(training_data)\n",
        "\n",
        "# Split data into training, validation, and test sets (60% train, 20% validation, 20% test)\n",
        "train_data, temp_data = train_test_split(training_data, test_size=0.4, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Initialize lists to store true and predicted POS tags for all sets\n",
        "all_true_pos_tags_train, all_predicted_pos_tags_train = [], []\n",
        "all_true_pos_tags_val, all_predicted_pos_tags_val = [], []\n",
        "all_true_pos_tags_test, all_predicted_pos_tags_test = [], []\n",
        "\n",
        "# Function to process data and evaluate POS tagging\n",
        "def process_data(data, all_true_pos_tags, all_predicted_pos_tags):\n",
        "    for text, true_pos_tags in data:\n",
        "        # Process the text with spaCy\n",
        "        doc = nlp(text)\n",
        "        # Extract predicted POS tags\n",
        "        predicted_pos_tags = [token.pos_ for token in doc]\n",
        "        # Extend lists with true and predicted tags for evaluation\n",
        "        all_true_pos_tags.extend(true_pos_tags)\n",
        "        all_predicted_pos_tags.extend(predicted_pos_tags)\n",
        "\n",
        "# Process training, validation, and test data\n",
        "process_data(train_data, all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "process_data(val_data, all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "process_data(test_data, all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Ensure both lists are the same length to avoid ValueError\n",
        "def ensure_equal_length(true_tags, predicted_tags):\n",
        "    if len(true_tags) != len(predicted_tags):\n",
        "        min_length = min(len(true_tags), len(predicted_tags))\n",
        "        true_tags = true_tags[:min_length]\n",
        "        predicted_tags = predicted_tags[:min_length]\n",
        "    return true_tags, predicted_tags\n",
        "\n",
        "# Ensure correct lengths for all sets\n",
        "all_true_pos_tags_train, all_predicted_pos_tags_train = ensure_equal_length(all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "all_true_pos_tags_val, all_predicted_pos_tags_val = ensure_equal_length(all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "all_true_pos_tags_test, all_predicted_pos_tags_test = ensure_equal_length(all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Function to calculate metrics for a dataset\n",
        "def evaluate_metrics(true_tags, predicted_tags):\n",
        "    accuracy = accuracy_score(true_tags, predicted_tags)\n",
        "    precision = precision_score(true_tags, predicted_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, predicted_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, predicted_tags, average='weighted')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate on training, validation, and test sets\n",
        "metrics_train = evaluate_metrics(all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "metrics_val = evaluate_metrics(all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "metrics_test = evaluate_metrics(all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Combine all metrics into single print statement\n",
        "total_accuracy = (metrics_train[0] + metrics_val[0] + metrics_test[0]) / 3\n",
        "total_precision = (metrics_train[1] + metrics_val[1] + metrics_test[1]) / 3\n",
        "total_recall = (metrics_train[2] + metrics_val[2] + metrics_test[2]) / 3\n",
        "total_f1 = (metrics_train[3] + metrics_val[3] + metrics_test[3]) / 3\n",
        "\n",
        "# Print consolidated metrics\n",
        "print(\"Consolidated Metrics across Training, Validation, and Test Data:\")\n",
        "print(f\"Accuracy: {total_accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {total_precision * 100:.4f}%\")\n",
        "print(f\"Recall: {total_recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {total_f1 * 100:.4f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7Ujs-eO5az0",
        "outputId": "c2910a0d-fbdf-4491-c62d-d1dca985b85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consolidated Metrics across Training, Validation, and Test Data:\n",
            "Accuracy: 30.7145%\n",
            "Precision: 36.3046%\n",
            "Recall: 30.7145%\n",
            "F1 Score: 32.9712%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_OMXZW5kp8W"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a model of sentiment analysis based on spaCy’s text classification system and a Naive Bayes classifier provided by the scikit-learn library. It begins with instantiating a blank-scratched spaCy model to which a text classification component is incorporated to classify text as POSITIVE or NEGATIVE. The training data is composed of a number of sentences with a certain attitudinal label which is provided in a dict format.\n",
        "\n",
        "In between training the model and fitting it to available data, all the sentences in the text are changed to lower case during text preparation. Next, ostensible text is transformed into vector form using CountVectorizer, which involves changing a feature text dataset into a form that can be processed algorithmically. The entire data is divided into two parts where one part is use for training and the other testing, each part contributing half of the available data.\n",
        "\n",
        "A Naive Bayes model is applied to the training data that has been vectorized, and results are provided for the test data. The results of the experiment are reported interms of accuracy, precision, recall and F1 measure. Lastly, these measures are displayed to assess their usefulness in sentiment classification by the model."
      ],
      "metadata": {
        "id": "xVkWYAaz2yks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load a blank model and add text classifier\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGATIVE\")\n",
        "\n",
        "train_data = [\n",
        "    (\"I'm so frustrated with how slow my internet is.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I'm so happy with my new job!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The customer service at that store is excellent.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The movie was a complete waste of time.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That movie was truly heartwarming and beautiful.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been feeling really down lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The food at the new restaurant was absolutely delicious.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t get the job, and I feel so defeated.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The sunset this evening was breathtaking.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m really upset that I missed the deadline.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"I finally finished the book, and it was such a rewarding read.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"This weather is terrible, I can’t wait for it to end.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The surprise party was such a success!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My laptop crashed again, and I lost all my work.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The concert was absolutely mind-blowing!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been struggling with my workload lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The flowers you sent me are absolutely stunning.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I regret spending money on that product.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just found out I won the contest! I’m over the moon.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"Everything seems to be going wrong lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "]\n",
        "\n",
        "#Lowercasing\n",
        "train_data = [(text.lower(), labels) for text, labels in train_data]\n"
      ],
      "metadata": {
        "id": "3h9WevcTF3hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Extract text data from train_data\n",
        "text = [data[0] for data in train_data]\n",
        "labels = [data[1]['cats']['POSITIVE'] for data in train_data] # Extract labels\n",
        "\n",
        "# Vectorize text data using the extracted text list\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict sentiments\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48di5rg88Ukw",
        "outputId": "4ee4fa94-01b3-4ea4-8c60-de0290088264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 70.0000%\n",
            "Precision: 60.0000%\n",
            "Recall: 75.0000%\n",
            "F1 Score: 66.6667%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj0jTJHHktX_"
      },
      "source": [
        "## Text Summarizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a model of Text summarization using the natural language processing toolspaCy. The essence of the tool is to create a summary of a piece of text in relation to a provided summary. The tapplying summarization code focuses on the case of climate change – outlining the causes of such a phenomenon, the effects it can have and the ways in which one needs to act swiftly to mitigate such detrimental changes; how to put together everything in the…\n",
        "\n",
        "`Extractive_summary` function deals with the input text by transforming it into lower-case letters, stripping the punctuation, and then, tokenizing it into separate sentences, the first few of which, were use as a summary. In addition, once the summary has been created an attempt is made to check how it fares in comparison with a pre set summary and thus derive measures like precision, recall and f1 score aimed at assessing the performance of the summary created. The obtained values regarding these ratings are presented in the form of percentages.\n",
        "\n",
        "This summarization approach can be implemented likewise very easily to any other subsets of data which makes it useful for many text analytics tasks in different situations. Using the same code structure. The code will also work for different topics or datasets which is why it is functional.\n"
      ],
      "metadata": {
        "id": "7K1mh3T83R4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download the punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example text and reference summary\n",
        "text = \"\"\"Climate change is one of the most pressing issues of our time. The increasing levels of greenhouse gases in the atmosphere\n",
        "have led to rising global temperatures. As a result, glaciers are melting, sea levels are rising, and extreme weather events\n",
        "are becoming more frequent. Many governments around the world have pledged to reduce carbon emissions, but progress has been slow.\n",
        "Renewable energy sources such as solar and wind power offer hope, but their adoption has not been widespread enough to make a significant impact yet.\n",
        "Urgent action is needed to address this global crisis before it’s too late.\"\"\"\n",
        "\n",
        "reference_summary = \"\"\"Climate change is caused by greenhouse gases and is leading to rising temperatures and extreme weather.\n",
        "Renewable energy offers hope, but its adoption is slow.\"\"\"\n",
        "\n",
        "# Extractive summarization function with lowercase preprocessing\n",
        "def extractive_summary(text, num_sentences=3):\n",
        "    doc = nlp(text.lower())  # Convert text to lowercase before processing\n",
        "    sentences = [sent.text.lower() for sent in doc.sents]  # Convert sentences to lowercase\n",
        "    return sentences[:num_sentences]  # Return the first `num_sentences` as the summary\n",
        "\n",
        "# Tokenizing the reference and generated summaries into sentences\n",
        "generated_summary = extractive_summary(text)  # Summary in lowercase\n",
        "reference_sentences = [sent.lower() for sent in sent_tokenize(reference_summary)]  # Reference in lowercase\n",
        "\n",
        "# Convert to binary relevance: 1 if the sentence appears in the reference summary, 0 otherwise\n",
        "y_true = [1 if sent in reference_sentences else 0 for sent in sent_tokenize(text.lower())]  # Compare with reference\n",
        "y_pred = [1 if sent in generated_summary else 0 for sent in sent_tokenize(text.lower())]  # Compare with generated summary\n",
        "\n",
        "# Ensure y_true and y_pred are of the same length\n",
        "if len(y_true) != len(y_pred):\n",
        "    min_length = min(len(y_true), len(y_pred))\n",
        "    y_true = y_true[:min_length]\n",
        "    y_pred = y_pred[:min_length]\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred) * 100  # Convert to percentage\n",
        "recall = recall_score(y_true, y_pred) * 100  # Convert to percentage\n",
        "f1 = f1_score(y_true, y_pred) * 100  # Convert to percentage\n",
        "\n",
        "# Output results\n",
        "print(f\"Generated Summary: {' '.join(generated_summary)}\")\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"Recall: {recall:.2f}%\")\n",
        "print(f\"F1 Score: {f1:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQELzoUqBOdT",
        "outputId": "6f159a43-ff6b-4a17-9ee6-5d319af60e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: climate change is one of the most pressing issues of our time. the increasing levels of greenhouse gases in the atmosphere \n",
            "have led to rising global temperatures. as a result, glaciers are melting, sea levels are rising, and extreme weather events \n",
            "are becoming more frequent.\n",
            "Precision: 0.00%\n",
            "Recall: 0.00%\n",
            "F1 Score: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXnzIe2iGRX0"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCyFdV7yG08P"
      },
      "source": [
        "# **Subset 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc2i1NAaH3L6"
      },
      "source": [
        "## Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code from the subset 1 is also the same in this code but the training data is consist of 50 short emails."
      ],
      "metadata": {
        "id": "arCfObkW4XMN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo3ilXOuHj1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186e29e3-e253-4a70-c82e-636fb02ac76c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING SET (60% of the data):\n",
            "Text: you won a million dollars! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: can we discuss this project? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: where are you?when wil you reach here? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: you will be in the place of that man - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: sorry, i'll call later - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: no problem. how are you doing? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: dont worry. i guess he's busy. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: winner!! as a valued network customer you have been selected to receivea £900 prize reward! to claim call 09061701461. claim code kl341. valid 12 hours only. - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: win a trip to hawaii now - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: busy here. trying to finish for new year. i am looking forward to finally meeting you - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: exclusive offer just for you! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: did you catch the bus ? are you frying an egg ? did you make a tea? are you eating your mom's left over dinner ? do you feel my love ? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: meeting at 10 am tomorrow - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: just sent it. so what type of food do you like? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: your package has been shipped - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: i'm back, lemme know when you're ready - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: limited time deal, buy now! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: free message activate your 500 free text messages by replying to this message with the word free for terms & conditions, visit www.07781482378.com - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: can we reschedule our call? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: get a free iphone today - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: hello, how are you? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: hello! how's you and how did saturday go? i was just texting to see if you'd decided to do anything tomo. not that i'm trying to invite myself or anything! - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: update your account details - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: what you doing? how are you? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: tired. i haven't slept well the past few nights. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: good stuff, will do. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: great news! you have been pre-approved for a personal loan of $10,000 with a low-interest rate! no credit check required. apply today at www.getmymoney.com and get instant cash! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: sounds great! are you home now? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! c suprman v, matrix3, starwars3, etc all 4 free! bx420-ip4-5we. 150pm. dont miss out! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: as a valued customer, i am pleased to advise you that following recent review of your mob no. you are awarded with a £1500 bonus prize, call 09066364589 - Label: {'SPAM': 1, 'HAM': 0}\n",
            "\n",
            "VALIDATION SET (20% of the data):\n",
            "Text: your invoice is attached - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: he says he'll give me a call when his friend's got the money but that he's definitely buying before the end of the week - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: this is spam - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: i cant pick the phone right now. pls send a message - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: claim your free prize now! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: please don't text me anymore. i have nothing else to say. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: congratulations! you've been selected - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: please call our customer service representative on freephone 0808 145 4742 between 9am-11pm as you have won a guaranteed £1000 cash or £5000 prize! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: lose 20 pounds in just 2 weeks with our miracle weight loss pills! 100% natural and safe. order today and get a special discount: www.weightlosspills.com. hurry, offer expires soon! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: i accidentally deleted the message. resend please. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "\n",
            "TESTING SET (20% of the data):\n",
            "Text: important meeting agenda - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: awesome, i'll see you in a bit - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: what time you coming down later? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: you made my day. do have a great day too. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: urgent! you have won a 1 week free membership in our £100,000 prize jackpot! txt the word: claim to no: 81010 t&c www.dbuk.net lccltd pobox 4403ldnw1a7rw18 - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: just sleeping and surfing - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: aight, i'll hit you up when i get some cash - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: thanks a lot for your wishes on my birthday. thanks you for making my birthday truly memorable. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: finally the match heading towards draw as your prediction. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: i am waiting machan. call me once you free. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Iteration 1/10 - Loss: 6.799504302442074\n",
            "Iteration 2/10 - Loss: 1.070469415793923\n",
            "Iteration 3/10 - Loss: 0.0006661840155928189\n",
            "Iteration 4/10 - Loss: 2.300785521125448e-05\n",
            "Iteration 5/10 - Loss: 1.084801613693287e-05\n",
            "Iteration 6/10 - Loss: 6.414183101544069e-06\n",
            "Iteration 7/10 - Loss: 4.101313424431652e-06\n",
            "Iteration 8/10 - Loss: 2.763186169429943e-06\n",
            "Iteration 9/10 - Loss: 1.9403549482710503e-06\n",
            "Iteration 10/10 - Loss: 1.4098838052589713e-06\n",
            "\n",
            "Sample Prediction Output with probabilities:\n",
            "{'SPAM': 0.9997743964195251, 'HAM': 0.0002256703155580908}\n",
            "\n",
            "Accuracy: 90.0000%\n",
            "Precision: 95.0000%\n",
            "Recall: 90.0000%\n",
            "F1 Score: 91.3725%\n",
            "\n",
            "Enter a sample email for classification (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "# Create a blank SpaCy model and add the text classifier component\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"SPAM\")\n",
        "textcat.add_label(\"HAM\")\n",
        "\n",
        "# Define a minimal preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "\n",
        "#Example Training data\n",
        "train_data = [\n",
        "    (\"This is spam\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello, how are you?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You won a million dollars!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Claim your free prize now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Meeting at 10 AM tomorrow\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your invoice is attached\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Exclusive offer just for you!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Get a free iPhone today\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we reschedule our call?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Update your account details\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Limited time deal, buy now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your package has been shipped\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Win a trip to Hawaii now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Important meeting agenda\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You've been selected\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we discuss this project?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I am waiting machan. Call me once you free.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Sorry, I'll call later\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"You will be in the place of that man\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Please don't text me anymore. I have nothing else to say.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Thanks a lot for your wishes on my birthday. Thanks you for making my birthday truly memorable.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Aight, I'll hit you up when I get some cash\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Dont worry. I guess he's busy.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a £1500 Bonus Prize, call 09066364589\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Good stuff, will do.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"What time you coming down later?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Sounds great! Are you home now?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Finally the match heading towards draw as your prediction.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Tired. I haven't slept well the past few nights.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Where are you?when wil you reach here?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Please call our customer service representative on FREEPHONE 0808 145 4742 between 9am-11pm as you have WON a guaranteed £1000 cash or £5000 prize!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"What you doing? how are you?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I'm back, lemme know when you're ready\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Lose 20 pounds in just 2 weeks with our miracle weight loss pills! 100% natural and safe. Order today and get a special discount: www.weightlosspills.com. Hurry, offer expires soon!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Awesome, I'll see you in a bit\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Just sent it. So what type of food do you like?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I accidentally deleted the message. Resend please.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For terms & conditions, visit www.07781482378.com\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"I cant pick the phone right now. Pls send a message\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"He says he'll give me a call when his friend's got the money but that he's definitely buying before the end of the week\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You made my day. Do have a great day too.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Great news! You have been pre-approved for a personal loan of $10,000 with a low-interest rate! No credit check required. Apply today at www.getmymoney.com and get instant cash!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"No problem. How are you doing?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Just sleeping and surfing\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Busy here. Trying to finish for new year. I am looking forward to finally meeting you\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}})\n",
        "]\n",
        "\n",
        "# Apply minimal text preprocessing\n",
        "train_data = [(preprocess_text(text), annotations) for text, annotations in train_data]\n",
        "\n",
        "# Prepare training data into SpaCy's Example format\n",
        "train_examples = []\n",
        "for text, annotations in train_data:\n",
        "    doc = nlpTC.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    train_examples.append(example)\n",
        "\n",
        "# Split data into training, validation, and testing sets\n",
        "train_examples, test_examples = train_test_split(train_examples, test_size=0.2, random_state=42)\n",
        "train_examples, val_examples = train_test_split(train_examples, test_size=0.25, random_state=42)  # 20% of the remaining data is used for validation\n",
        "\n",
        "# Print the split data to visualize each set\n",
        "print(\"TRAINING SET (60% of the data):\")\n",
        "for example in train_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "print(\"\\nVALIDATION SET (20% of the data):\")\n",
        "for example in val_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "print(\"\\nTESTING SET (20% of the data):\")\n",
        "for example in test_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "# Training the model with more iterations for small datasets\n",
        "n_iter = 10  # Set iterations\n",
        "optimizer = nlpTC.initialize()\n",
        "\n",
        "for i in range(n_iter):\n",
        "    losses = {}\n",
        "    for batch in spacy.util.minibatch(train_examples, size=2):  # Small batch size for small data\n",
        "        for example in batch:\n",
        "            nlpTC.update([example], sgd=optimizer, losses=losses)\n",
        "    print(f\"Iteration {i+1}/{n_iter} - Loss: {losses['textcat']}\")\n",
        "\n",
        "# Testing the model\n",
        "print(\"\\nSample Prediction Output with probabilities:\")\n",
        "doc = nlpTC(\"Claim your prize now!\")\n",
        "print(doc.cats)\n",
        "\n",
        "# Function to classify user input emails\n",
        "def classify_email(email):\n",
        "    email = preprocess_text(email)\n",
        "    doc = nlpTC(email)\n",
        "    spam_score = doc.cats['SPAM']\n",
        "    ham_score = doc.cats['HAM']\n",
        "\n",
        "    if spam_score > ham_score:\n",
        "        return \"SPAM\"\n",
        "    else:\n",
        "        return \"HAM\"\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score on the test set\n",
        "true_labels = [1 if example.reference.cats['SPAM'] == 1 else 0 for example in test_examples]\n",
        "predicted_labels = [1 if classify_email(example.reference.text) == 'SPAM' else 0 for example in test_examples]\n",
        "\n",
        "# Calculate and print metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nAccuracy: {accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")\n",
        "\n",
        "# Allow users to test the model by inputting their own data\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter a sample email for classification (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    classification = classify_email(user_input)\n",
        "    print(f\"The email is classified as: {classification}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GABu7eY2H6eP"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code from the subset 1 is also the same in this code but the training data is consist of 50 data.\n"
      ],
      "metadata": {
        "id": "Ecs9v5s-5mli"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKa7kBxoH_D1",
        "outputId": "6554d491-56cc-4e13-8a5d-b08af3493efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.7.1\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Precision: 36.9565%\n",
            "Recall: 100.0000%\n",
            "F1 Score: 53.9683%\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load a pre-trained NER model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Sample training data (text and true entity annotations)\n",
        "training_data = [\n",
        "    (\"Microsoft announced a new AI initiative in Seattle.\", [(0, 9, \"ORG\"), (39, 46, \"GPE\")]),\n",
        "    (\"Google I/O will take place in May 2023.\", [(0, 10, \"EVENT\"), (29, 37, \"DATE\")]),\n",
        "    (\"The unemployment rate in the U.S. dropped to 3.5%.\", [(34, 38, \"PERCENT\"), (27, 31, \"GPE\")]),\n",
        "    (\"The Chinese economy grew by 5% last year.\", [(4, 11, \"NORP\")]),\n",
        "    (\"Sundar Pichai is the CEO of Google.\", [(0, 13, \"PERSON\"), (28, 34, \"ORG\")]),\n",
        "    (\"Tesla secured $2 billion in new funding.\", [(14, 22, \"MONEY\")]),\n",
        "    (\"Amazon is opening a new office in Vancouver.\", [(0, 6, \"ORG\"), (36, 45, \"GPE\")]),\n",
        "    (\"Samsung released its new Galaxy S22 phone.\", [(0, 7, \"ORG\"), (23, 32, \"PRODUCT\")]),\n",
        "    (\"The Pacific Ocean is the largest body of water on Earth.\", [(4, 17, \"LOC\")]),\n",
        "    (\"The headquarters of IBM is in New York City.\", [(21, 24, \"ORG\"), (31, 44, \"GPE\")]),\n",
        "\n",
        "    (\"Satya Nadella leads Microsoft Corporation.\", [(0, 12, \"PERSON\"), (19, 38, \"ORG\")]),\n",
        "    (\"The FIFA World Cup will be held in Qatar in 2022.\", [(4, 18, \"EVENT\"), (34, 39, \"GPE\"), (43, 47, \"DATE\")]),\n",
        "    (\"Apple plans to invest $10 billion in manufacturing.\", [(23, 32, \"MONEY\")]),\n",
        "    (\"A new skyscraper is being built in Dubai.\", [(33, 38, \"GPE\")]),\n",
        "    (\"70% of the world's population is now online.\", [(0, 3, \"PERCENT\")]),\n",
        "    (\"Elon Musk founded SpaceX and Tesla.\", [(0, 9, \"PERSON\"), (17, 23, \"ORG\"), (28, 33, \"ORG\")]),\n",
        "    (\"The startup raised $50 million in Series B.\", [(15, 25, \"MONEY\")]),\n",
        "    (\"The next Apple event is scheduled for March 25th.\", [(9, 14, \"ORG\"), (39, 48, \"DATE\")]),\n",
        "    (\"The new company is aiming for a 15% market share.\", [(28, 31, \"PERCENT\")]),\n",
        "    (\"Apple's iPhone 14 is expected to launch in 2023.\", [(0, 5, \"ORG\"), (7, 15, \"PRODUCT\"), (46, 50, \"DATE\")]),\n",
        "\n",
        "    (\"A German scientist won the Nobel Prize.\", [(2, 8, \"NORP\")]),\n",
        "    (\"Facebook plans to launch new features in June.\", [(0, 7, \"ORG\"), (30, 35, \"DATE\")]),\n",
        "    (\"The CEO of Apple, Tim Cook, announced new products.\", [(14, 22, \"PERSON\"), (4, 9, \"ORG\")]),\n",
        "    (\"NASA's Perseverance rover landed on Mars.\", [(0, 4, \"ORG\"), (34, 38, \"GPE\")]),\n",
        "    (\"The 2024 Summer Olympics will take place in Paris.\", [(4, 24, \"EVENT\"), (40, 45, \"GPE\")]),\n",
        "    (\"The inflation rate reached 8.6% last month.\", [(28, 32, \"PERCENT\")]),\n",
        "    (\"Coca-Cola launched a new flavor this spring.\", [(0, 10, \"ORG\"), (36, 41, \"DATE\")]),\n",
        "    (\"The World Health Organization declared a health emergency.\", [(4, 30, \"ORG\")]),\n",
        "    (\"Berkshire Hathaway's stock price increased by $500.\", [(0, 23, \"ORG\"), (37, 40, \"MONEY\")]),\n",
        "    (\"In 2020, remote work became the new normal.\", [(3, 7, \"DATE\")]),\n",
        "\n",
        "    (\"Mark Zuckerberg met with world leaders to discuss technology.\", [(0, 15, \"PERSON\")]),\n",
        "    (\"The Great Wall of China is a popular tourist attraction.\", [(4, 20, \"LOC\")]),\n",
        "    (\"The Grammy Awards will be held in Los Angeles.\", [(0, 14, \"EVENT\"), (30, 43, \"GPE\")]),\n",
        "    (\"Intel announced a new chip that will improve processing speed.\", [(0, 5, \"ORG\")]),\n",
        "    (\"The stock market saw a decline of 4% today.\", [(28, 31, \"PERCENT\")]),\n",
        "    (\"Microsoft is acquiring LinkedIn for $26.2 billion.\", [(0, 9, \"ORG\"), (26, 39, \"ORG\"), (44, 57, \"MONEY\")]),\n",
        "    (\"SpaceX plans to launch its Starship rocket next year.\", [(0, 6, \"ORG\"), (34, 39, \"DATE\")]),\n",
        "    (\"The next big tech conference is set for September.\", [(9, 13, \"EVENT\"), (38, 47, \"DATE\")]),\n",
        "    (\"The United Nations addresses global challenges.\", [(4, 17, \"ORG\")]),\n",
        "    (\"Bill Gates founded Microsoft in 1975.\", [(0, 10, \"PERSON\"), (21, 29, \"ORG\"), (32, 36, \"DATE\")]),\n",
        "\n",
        "    (\"A recent study showed that 60% of students prefer online classes.\", [(36, 38, \"PERCENT\")]),\n",
        "    (\"The Louvre Museum is located in Paris.\", [(4, 22, \"ORG\"), (30, 35, \"GPE\")]),\n",
        "    (\"The 2022 World Cup will be hosted in Qatar.\", [(4, 18, \"EVENT\"), (35, 40, \"GPE\")]),\n",
        "    (\"Netflix added 8 million new subscribers in 2021.\", [(7, 14, \"ORG\"), (23, 24, \"MONEY\"), (29, 33, \"DATE\")]),\n",
        "    (\"The first electric car was launched by Tesla in 2008.\", [(29, 34, \"ORG\"), (39, 43, \"DATE\")]),\n",
        "    (\"Researchers found a new species of frog in Madagascar.\", [(36, 49, \"LOC\")]),\n",
        "    (\"In 2019, the world saw significant advancements in AI.\", [(3, 7, \"DATE\")]),\n",
        "    (\"The White House issued a statement regarding climate change.\", [(4, 15, \"GPE\")]),\n",
        "    (\"Elon Musk is the founder of SpaceX and Tesla.\", [(0, 9, \"PERSON\"), (23, 29, \"ORG\"), (34, 39, \"ORG\")])\n",
        "    (\"Bill Gates pledged $100 million to fight malaria.\", [(0, 10, \"PERSON\"), (24, 34, \"MONEY\")]),\n",
        "]\n",
        "\n",
        "# Preprocess: Convert all texts to lowercase\n",
        "preprocessed_data = [(text.lower(), entities) for text, entities in training_data]\n",
        "\n",
        "# Initialize lists for storing true and predicted entities\n",
        "all_true_entities = []\n",
        "all_pred_entities = []\n",
        "\n",
        "# Iterate through training data\n",
        "for text, true_entities in training_data:\n",
        "    # Run NER model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Predicted entities from the model\n",
        "    pred_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Store true and predicted entities for all examples\n",
        "    all_true_entities.extend([(ent[0], ent[1], ent[2]) for ent in true_entities])\n",
        "    all_pred_entities.extend([(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents])\n",
        "\n",
        "# Convert to binary classification (1 for correct entity, 0 for incorrect)\n",
        "y_true = [1 if ent in all_true_entities else 0 for ent in all_pred_entities]\n",
        "y_pred = [1 for _ in all_pred_entities]  # Assuming all predictions are correct for now\n",
        "\n",
        "# Calculate Precision, Recall, F1\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf-pmbbRIDgs"
      },
      "source": [
        "## Part of Speech Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code from the subset 1 is also the same in this code but the training data is consist of 50 data.\n"
      ],
      "metadata": {
        "id": "ZCSuFS9d5vVK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zElRinqtIoJg",
        "outputId": "43c24a79-60e3-48a1-f7ba-38021ca70155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.7.1\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Consolidated Metrics across Training, Validation, and Test Data:\n",
            "Accuracy: 16.3483%\n",
            "Precision: 18.7806%\n",
            "Recall: 16.3483%\n",
            "F1 Score: 17.4103%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load spaCy's POS tagging model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Sample expanded training data: list of (text, true_pos_tags) pairs\n",
        "training_data = [\n",
        "    (\"She sells seashells by the seashore.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The quick brown fox jumps over the lazy dog.\", ['DET', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"I love coding in Python.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'PROPN']),\n",
        "    (\"Birds fly in the sky.\", ['NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"Alice and Bob went to the market.\", ['PROPN', 'CCONJ', 'PROPN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"Reading books is fun.\", ['VERB', 'NOUN', 'AUX', 'ADJ']),\n",
        "    (\"My car is very fast.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"We are going to the zoo.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"It is raining today.\", ['PRON', 'AUX', 'VERB', 'NOUN']),\n",
        "    (\"Programming languages are interesting.\", ['NOUN', 'NOUN', 'AUX', 'ADJ']),\n",
        "\n",
        "    (\"The cat sleeps on the mat.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"John likes to play soccer.\", ['PROPN', 'VERB', 'PART', 'VERB', 'NOUN']),\n",
        "    (\"She is learning French.\", ['PRON', 'AUX', 'VERB', 'PROPN']),\n",
        "    (\"The weather is nice today.\", ['DET', 'NOUN', 'AUX', 'ADJ', 'NOUN']),\n",
        "    (\"He bought a new laptop yesterday.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN']),\n",
        "    (\"They are swimming in the pool.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The pizza smells delicious.\", ['DET', 'NOUN', 'VERB', 'ADJ']),\n",
        "    (\"Can you help me with this project?\", ['AUX', 'PRON', 'VERB', 'PRON', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"This task is quite difficult.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"He enjoys reading books.\", ['PRON', 'VERB', 'VERB', 'NOUN']),\n",
        "\n",
        "    (\"The dog barked loudly at the strangers.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"I have a meeting tomorrow.\", ['PRON', 'AUX', 'DET', 'NOUN', 'ADJ']),\n",
        "    (\"They will travel to Spain next year.\", ['PRON', 'AUX', 'VERB', 'ADP', 'PROPN', 'ADV', 'NOUN']),\n",
        "    (\"He plays the guitar beautifully.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADV']),\n",
        "    (\"The book on the shelf is mine.\", ['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'AUX', 'PRON']),\n",
        "    (\"Jessica ran a marathon last summer.\", ['PROPN', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN']),\n",
        "    (\"Cooking is a wonderful hobby.\", ['VERB', 'AUX', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"The stars shine brightly in the night sky.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN', 'NOUN']),\n",
        "    (\"I am learning how to code.\", ['PRON', 'AUX', 'VERB', 'ADV', 'ADP', 'VERB']),\n",
        "    (\"The flowers bloom in spring.\", ['DET', 'NOUN', 'VERB', 'ADP', 'NOUN']),\n",
        "\n",
        "    (\"My friends enjoy hiking on weekends.\", ['DET', 'NOUN', 'VERB', 'VERB', 'ADP', 'NOUN']),\n",
        "    (\"Dogs are great companions.\", ['NOUN', 'AUX', 'ADJ', 'NOUN']),\n",
        "    (\"She wrote an amazing story.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"The sun rises in the east.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"He plays soccer every weekend.\", ['PRON', 'VERB', 'NOUN', 'ADV', 'NOUN']),\n",
        "    (\"Reading novels helps improve vocabulary.\", ['VERB', 'NOUN', 'VERB', 'VERB', 'NOUN']),\n",
        "    (\"My family enjoys movie nights.\", ['DET', 'NOUN', 'VERB', 'NOUN', 'NOUN']),\n",
        "    (\"She is very talented in music.\", ['PRON', 'AUX', 'ADV', 'ADJ', 'ADP', 'NOUN']),\n",
        "    (\"We will celebrate his birthday soon.\", ['PRON', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADV']),\n",
        "    (\"The wind blew fiercely during the storm.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "\n",
        "    (\"They went hiking in the mountains.\", ['PRON', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"This recipe is quite easy.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"The teacher explains concepts clearly.\", ['DET', 'NOUN', 'VERB', 'NOUN', 'ADV']),\n",
        "    (\"We have been working on this project.\", ['PRON', 'AUX', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"He prefers tea over coffee.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'NOUN']),\n",
        "    (\"The child laughed joyfully at the joke.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She likes to dance at parties.\", ['PRON', 'VERB', 'PART', 'VERB', 'ADP', 'NOUN']),\n",
        "    (\"They are playing video games right now.\", ['PRON', 'AUX', 'VERB', 'NOUN', 'ADV', 'ADV']),\n",
        "    (\"The baby laughed at the puppy.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She danced gracefully across the stage.\", ['PRON', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN'])\n",
        "]\n",
        "\n",
        "# Preprocess text: convert to lowercase and shuffle the training data\n",
        "training_data = [(text.lower(), tags) for text, tags in training_data]\n",
        "random.shuffle(training_data)\n",
        "\n",
        "# Split data into training, validation, and test sets (60% train, 20% validation, 20% test)\n",
        "train_data, temp_data = train_test_split(training_data, test_size=0.4, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Initialize lists to store true and predicted POS tags for all sets\n",
        "all_true_pos_tags_train, all_predicted_pos_tags_train = [], []\n",
        "all_true_pos_tags_val, all_predicted_pos_tags_val = [], []\n",
        "all_true_pos_tags_test, all_predicted_pos_tags_test = [], []\n",
        "\n",
        "# Function to process data and evaluate POS tagging\n",
        "def process_data(data, all_true_pos_tags, all_predicted_pos_tags):\n",
        "    for text, true_pos_tags in data:\n",
        "        # Process the text with spaCy\n",
        "        doc = nlp(text)\n",
        "        # Extract predicted POS tags\n",
        "        predicted_pos_tags = [token.pos_ for token in doc]\n",
        "        # Extend lists with true and predicted tags for evaluation\n",
        "        all_true_pos_tags.extend(true_pos_tags)\n",
        "        all_predicted_pos_tags.extend(predicted_pos_tags)\n",
        "\n",
        "# Process training, validation, and test data\n",
        "process_data(train_data, all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "process_data(val_data, all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "process_data(test_data, all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Ensure both lists are the same length to avoid ValueError\n",
        "def ensure_equal_length(true_tags, predicted_tags):\n",
        "    if len(true_tags) != len(predicted_tags):\n",
        "        min_length = min(len(true_tags), len(predicted_tags))\n",
        "        true_tags = true_tags[:min_length]\n",
        "        predicted_tags = predicted_tags[:min_length]\n",
        "    return true_tags, predicted_tags\n",
        "\n",
        "# Ensure correct lengths for all sets\n",
        "all_true_pos_tags_train, all_predicted_pos_tags_train = ensure_equal_length(all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "all_true_pos_tags_val, all_predicted_pos_tags_val = ensure_equal_length(all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "all_true_pos_tags_test, all_predicted_pos_tags_test = ensure_equal_length(all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Function to calculate metrics for a dataset\n",
        "def evaluate_metrics(true_tags, predicted_tags):\n",
        "    accuracy = accuracy_score(true_tags, predicted_tags)\n",
        "    precision = precision_score(true_tags, predicted_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, predicted_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, predicted_tags, average='weighted')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate on training, validation, and test sets\n",
        "metrics_train = evaluate_metrics(all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "metrics_val = evaluate_metrics(all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "metrics_test = evaluate_metrics(all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Combine all metrics into single print statement\n",
        "total_accuracy = (metrics_train[0] + metrics_val[0] + metrics_test[0]) / 3\n",
        "total_precision = (metrics_train[1] + metrics_val[1] + metrics_test[1]) / 3\n",
        "total_recall = (metrics_train[2] + metrics_val[2] + metrics_test[2]) / 3\n",
        "total_f1 = (metrics_train[3] + metrics_val[3] + metrics_test[3]) / 3\n",
        "\n",
        "# Print consolidated metrics\n",
        "print(\"Consolidated Metrics across Training, Validation, and Test Data:\")\n",
        "print(f\"Accuracy: {total_accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {total_precision * 100:.4f}%\")\n",
        "print(f\"Recall: {total_recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {total_f1 * 100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Arx69GWVIGCZ"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code from the subset 1 is also the same in this code but the training data is consist of 50 data."
      ],
      "metadata": {
        "id": "1euu7g3y5yrW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNG9Lh21Iqz_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load a blank model and add text classifier\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGATIVE\")\n",
        "\n",
        "\n",
        "train_data = [\n",
        "    (\"I'm so frustrated with how slow my internet is.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I'm so happy with my new job!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The customer service at that store is excellent.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The movie was a complete waste of time.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That movie was truly heartwarming and beautiful.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been feeling really down lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The food at the new restaurant was absolutely delicious.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t get the job, and I feel so defeated.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The sunset this evening was breathtaking.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m really upset that I missed the deadline.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"I finally finished the book, and it was such a rewarding read.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"This weather is terrible, I can’t wait for it to end.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The surprise party was such a success!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My laptop crashed again, and I lost all my work.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The concert was absolutely mind-blowing!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been struggling with my workload lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The flowers you sent me are absolutely stunning.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I regret spending money on that product.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just found out I won the contest! I’m over the moon.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"Everything seems to be going wrong lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"You did a fantastic job on that presentation.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m tired of dealing with all this stress.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I couldn’t be happier with how everything turned out.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My car broke down again, and I’m so frustrated.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That was one of the most enjoyable dinners I’ve had in ages.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m feeling really overwhelmed with everything going on.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m incredibly grateful for the support I’ve received.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t enjoy the event; it was a total letdown.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m so proud of everything we’ve accomplished this year.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"That comment really hurt my feelings.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"That was the best coffee I’ve had in a while!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I can’t believe how rude they were to me.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I got a promotion at work, and I couldn’t be more thrilled.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My phone screen cracked, and now I have to get it replaced.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"What a beautiful and sunny day!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m really anxious about everything going on.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"Spending time with family over the holidays was perfect.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t get enough sleep last night, and now I’m exhausted.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"This new app makes my life so much easier.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been really unmotivated lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"That vacation was exactly what I needed.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"That presentation did not go well at all.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I appreciate all the effort you put into this project.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My relationship with my friends hasn’t been great lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’ve made some great new friends recently.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The traffic was horrible, and I barely made it on time.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I had such a fun time with the kids at the park today.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t enjoy the book at all; it was so boring.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just got my dream job, and I’m beyond excited!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I feel like I’ve been making one mistake after another.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}})\n",
        "]\n",
        "\n",
        "#Lowercasing\n",
        "train_data = [(text.lower(), labels) for text, labels in train_data]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text data from train_data\n",
        "text = [data[0] for data in train_data]\n",
        "labels = [data[1]['cats']['POSITIVE'] for data in train_data] # Extract labels\n",
        "\n",
        "# Vectorize text data using the extracted text list\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict sentiments\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")"
      ],
      "metadata": {
        "id": "QuFOUOdHaxFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db29d3a-f752-46bb-ab7e-889d5722a73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 68.0000%\n",
            "Precision: 62.5000%\n",
            "Recall: 83.3333%\n",
            "F1 Score: 71.4286%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqYKKjxwIIit"
      },
      "source": [
        "## Text Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "calJugaOItop",
        "outputId": "cb6cab49-a61f-492c-d214-a4ebd2f60dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: climate change is one of the most pressing issues of our time. the increasing levels of greenhouse gases in the atmosphere \n",
            "have led to rising global temperatures. as a result, glaciers are melting, sea levels are rising, and extreme weather events \n",
            "are becoming more frequent.\n",
            "Precision: 0.00%\n",
            "Recall: 0.00%\n",
            "F1 Score: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download the punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Example text and reference summary\n",
        "text = \"\"\"Climate change is one of the most pressing issues of our time. The increasing levels of greenhouse gases in the atmosphere\n",
        "have led to rising global temperatures. As a result, glaciers are melting, sea levels are rising, and extreme weather events\n",
        "are becoming more frequent. Many governments around the world have pledged to reduce carbon emissions, but progress has been slow.\n",
        "Renewable energy sources such as solar and wind power offer hope, but their adoption has not been widespread enough to make a significant impact yet.\n",
        "Urgent action is needed to address this global crisis before it’s too late.\"\"\"\n",
        "\n",
        "reference_summary = \"\"\"Climate change is caused by greenhouse gases and is leading to rising temperatures and extreme weather.\n",
        "Renewable energy offers hope, but its adoption is slow.\"\"\"\n",
        "\n",
        "# Extractive summarization function with lowercase preprocessing\n",
        "def extractive_summary(text, num_sentences=3):\n",
        "    doc = nlp(text.lower())  # Convert text to lowercase before processing\n",
        "    sentences = [sent.text.lower() for sent in doc.sents]  # Convert sentences to lowercase\n",
        "    return sentences[:num_sentences]  # Return the first `num_sentences` as the summary\n",
        "\n",
        "# Tokenizing the reference and generated summaries into sentences\n",
        "generated_summary = extractive_summary(text)  # Summary in lowercase\n",
        "reference_sentences = [sent.lower() for sent in sent_tokenize(reference_summary)]  # Reference in lowercase\n",
        "\n",
        "# Convert to binary relevance: 1 if the sentence appears in the reference summary, 0 otherwise\n",
        "y_true = [1 if sent in reference_sentences else 0 for sent in sent_tokenize(text.lower())]  # Compare with reference\n",
        "y_pred = [1 if sent in generated_summary else 0 for sent in sent_tokenize(text.lower())]  # Compare with generated summary\n",
        "\n",
        "# Ensure y_true and y_pred are of the same length\n",
        "if len(y_true) != len(y_pred):\n",
        "    min_length = min(len(y_true), len(y_pred))\n",
        "    y_true = y_true[:min_length]\n",
        "    y_pred = y_pred[:min_length]\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred) * 100  # Convert to percentage\n",
        "recall = recall_score(y_true, y_pred) * 100  # Convert to percentage\n",
        "f1 = f1_score(y_true, y_pred) * 100  # Convert to percentage\n",
        "\n",
        "# Output results\n",
        "print(f\"Generated Summary: {' '.join(generated_summary)}\")\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"Recall: {recall:.2f}%\")\n",
        "print(f\"F1 Score: {f1:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtvs1XcSkCr_"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt1UVyg3IvNa"
      },
      "source": [
        "# **Subset 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE5EKjDJI6Ml"
      },
      "source": [
        "## Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code from the subset 1 is also the same in this code but the training data is consist of 100 data."
      ],
      "metadata": {
        "id": "wTzV_ypX55pL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUf9ibVpI4kd",
        "outputId": "0a89df73-65e1-4755-89c7-d4e71a29ec89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING SET (60% of the data):\n",
            "Text: dear user, we have detected suspicious activity in your bank account. to prevent your account from being suspended, please click on the link below and verify your details: www.banksecure.com. failure to do so within 24 hours will result in account suspension. - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: hello, how are you? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: can we discuss this project? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: thank you for applying for the marketing manager position at xyz company. we are pleased to invite you for an interview on monday, october 5th, at 2:00 pm. the interview will be conducted via zoom, and the details will be sent to you soon. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: can we reschedule our call? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: i'm back, lemme know when you're ready - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: just a quick reminder about our meeting tomorrow at 10:00 am in the conference room. please bring the project update documents with you. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: here are the notes from our weekly team meeting held today. please review and let me know if there are any changes or additions. we’ll be following up on these action items next week. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: i hope you’re all doing well. i wanted to share a quick update on the project status. we’re on track to complete the next phase by the end of the week. i’ll schedule a meeting for next monday to discuss the next steps. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: wishing you a very happy birthday! i hope you have a fantastic day filled with joy, laughter, and cake! let’s catch up soon. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: just a reminder that our family vacation is coming up soon! we’ll be flying to florida on the 15th, so make sure to pack everything by then. also, don’t forget to bring sunscreen and your camera! - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: urgent! your mobile number has been awarded with a £2000 prize guaranteed. call 09058094455 from land line. claim 3030. valid 12hrs only - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: you’re all invited to our annual block party on saturday, october 16th! we’ll have food, games, and music from 12 pm to 6 pm. bring your family, and let’s have some fun! - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: your travel itinerary has been confirmed. you are scheduled to depart from new york on flight 5678 at 9:00 am on october 20th. your return flight from london will be on flight 6789 at 5:00 pm on october 27th. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: just sleeping and surfing - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: did you catch the bus ? are you frying an egg ? did you make a tea? are you eating your mom's left over dinner ? do you feel my love ? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: winner!! as a valued network customer you have been selected to receivea £900 prize reward! to claim call 09061701461. claim code kl341. valid 12 hours only. - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: i am waiting machan. call me once you free. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: cool, text me when you're ready - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: as a valued customer, i am pleased to advise you that following recent review of your mob no. you are awarded with a £1500 bonus prize, call 09066364589 - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: claim your free 1999 bonus without deposit! download the rg777 app now for an 188p bonus. install here: https://bit.ly/3tn80nc. enjoy your rewards! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: free message activate your 500 free text messages by replying to this message with the word free for terms & conditions, visit www.07781482378.com - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: welcome to our weekly newsletter! this week, we’re sharing tips on how to improve productivity and stay organized. be sure to check out our latest articles and join our upcoming webinar on time management. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: what you doing? how are you? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: update your account details - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: thanks for this hope you had a good day today - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: just sent it. so what type of food do you like? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: exclusive offer just for you! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: i hope you are doing well. i wanted to let you know that i have submitted my final report for the course. please confirm when you receive it. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: this is a reminder of your upcoming dentist appointment on monday, october 18th, at 9:30 am. please contact us if you need to reschedule. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: great news! you have been pre-approved for a personal loan of $10,000 with a low-interest rate! no credit check required. apply today at www.getmymoney.com and get instant cash! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: gcash: account verification needed due to suspicious transaction. kindly visit: gcares-protect-ph.li to continue using our services - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: your invoice is attached - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: thanks a lot for your wishes on my birthday. thanks you for making my birthday truly memorable. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: your reservation at oceanview resort has been confirmed for october 10th to october 15th. we look forward to welcoming you. if you need assistance or have special requests, feel free to contact us. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: search i88j1l1! mag l0gin get free b0nus p8888! promo code: 2mqs0cs live n0w!claim unlimited b0nus n0w d0nt miss 0ut limited days 0nly - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: i hope this message finds you well. i wanted to follow up on my application for the software engineer position. i’m very interested in the role and would appreciate any updates on the hiring process. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: dont worry. i guess he's busy. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: i cant pick the phone right now. pls send a message - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: congratulations! you've been selected - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: finally the match heading towards draw as your prediction. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: no problem. how are you doing? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: urgent! you have won a 1 week free membership in our £100,000 prize jackpot! txt the word: claim to no: 81010 t&c www.dbuk.net lccltd pobox 4403ldnw1a7rw18 - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: just wanted to let you know that we arrived safely at the cabin. the weather is beautiful, and we’re planning to go hiking tomorrow. i’ll send you some pictures later. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: you’re invited to our annual company holiday party! join us on december 15th at 7:00 pm for an evening of food, drinks, and fun. please rsvp by december 1st. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: good stuff, will do. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: please find attached the agenda for tomorrow’s meeting. we’ll be discussing the q4 sales targets and the marketing strategy for the new product launch. let me know if you have any points to add. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: peryagame 5% cashback with unlimited bonus! peryagame offer guaranteed highest daily rebates in the ph! check out more promos now: https://peryagame.com - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: important meeting agenda - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: hurry! get a 90% discount on all our products! this is a one-time offer just for you! visit www.superdeals.com and use code save90 at checkout. don't miss out on this amazing opportunity! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: i wanted to give you a quick update on the project. we’ve completed the design phase and will be moving into development next week. let me know if you have any questions or need clarification on your tasks. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: where are you?when wil you reach here? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: aight, i'll hit you up when i get some cash - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: this is a reminder that the book you borrowed, “the catcher in the rye,” is due for return on october 7th. please return or renew it by then to avoid any late fees. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: sorry, i'll call later - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: busy here. trying to finish for new year. i am looking forward to finally meeting you - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: i’ve booked my tickets for the trip to hawaii! we’re flying out on the 12th and coming back on the 18th. let me know if you’re still interested in joining us—it’s going to be a great trip! - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: your appointment with dr. smith has been confirmed for thursday, october 8th, at 10:00 am. if you need to reschedule, please contact us at least 24 hours in advance. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: please note that the lecture scheduled for tuesday, october 12th, has been moved to thursday, october 14th, at 3 pm. the classroom remains the same. i apologize for any inconvenience. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: i accidentally deleted the message. resend please. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "\n",
            "VALIDATION SET (20% of the data):\n",
            "Text: please don't text me anymore. i have nothing else to say. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: do you want to grab lunch today? i’m thinking of trying that new italian place near the office. let me know if you’re up for it! - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: get a free iphone today - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: claim your free prize now! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: please call our customer service representative on freephone 0808 145 4742 between 9am-11pm as you have won a guaranteed £1000 cash or £5000 prize! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: thank you for placing an order with us! your order #78965 has been confirmed and is currently being processed. you will receive a shipping notification once it has been dispatched. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: your package has been shipped - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: we’re planning a family gathering at grandma’s house next sunday. we’ll have lunch around 1 pm, and it’ll be great to catch up with everyone. let me know if you can make it! - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: just a quick reminder that your homework assignment on chapter 3 is due by friday. make sure to review the key concepts and submit your work on time. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: just a quick reminder about the office potluck on friday! don’t forget to bring a dish to share with your colleagues. looking forward to seeing everyone’s culinary creations! - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: hi there! you have been selected for an all-expense-paid trip to the bahamas! to claim your free vacation, all you need to do is fill out a quick survey. click here now: www.freevacation.com. - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: lose 20 pounds in just 2 weeks with our miracle weight loss pills! 100% natural and safe. order today and get a special discount: www.weightlosspills.com. hurry, offer expires soon! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: you won a million dollars! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: we hope you’re enjoying your recent purchase from our store. we’d love to hear your feedback! please take a moment to complete our brief survey, and let us know how we did. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! c suprman v, matrix3, starwars3, etc all 4 free! bx420-ip4-5we. 150pm. dont miss out! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: are we still on for dinner tonight? i’ve made a reservation at 7:30 pm at the new thai place downtown. let me know if anything changes. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: our next book club meeting is scheduled for tuesday, october 6th, at 6:00 pm. we’ll be discussing “the alchemist” by paulo coelho. make sure to finish reading it before the meeting, and bring your thoughts and questions for the discussion. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: congratulations! you have been selected as the winner of our $1,000,000 prize! click here to claim your reward now: www.claimprize.com. act fast! offer expires soon. - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: guaranteed highest daily rebates in the ph! bet daily on any games and get up to 0.8% with unlimited bonus! check out more promos now: https://peryagame.com - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: thank you for your recent purchase! attached is the invoice for your order #45678. your items will be shipped within 3-5 business days. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "\n",
            "TESTING SET (20% of the data):\n",
            "Text: thanks for sending over the initial draft. i’ve made a few changes to the document, and you can find the updated version attached. let me know if you have any questions. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: i hope you that's the result of being consistently intelligent and kind. start asking him about practicum links and keep your ears open and all the best. ttyl - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: we are writing to inform you that a new software update is available for your device. version 4.3 includes bug fixes, security enhancements, and new features. please update your software at your earliest convenience to ensure optimal performance. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: you made my day. do have a great day too. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: he says he'll give me a call when his friend's got the money but that he's definitely buying before the end of the week - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: awesome, i'll see you in a bit - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: you will be in the place of that man - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: just a reminder to rsvp for our wedding on november 12th! we’re so excited to celebrate this special day with our family and friends. please let us know if you’ll be attending by october 1st. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: limited time deal, buy now! - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: this is spam - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: hello! how's you and how did saturday go? i was just texting to see if you'd decided to do anything tomo. not that i'm trying to invite myself or anything! - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: what time you coming down later? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: we’d like to invite you to our upcoming webinar on 'building effective remote teams.' the session will take place on september 30th at 11:00 am. you’ll learn tips and strategies for managing remote employees and improving team collaboration. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: tired. i haven't slept well the past few nights. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: i just wanted to follow up on our last meeting. have you had a chance to review the proposal we sent over? we’d love to hear your thoughts and discuss the next steps. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: meeting at 10 am tomorrow - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: earn $5,000 per week working from home! no experience needed. start today and make money by simply filling out surveys. click here to learn more: www.earnmoneyathome.com. - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: just a heads up, we’re meeting at the library on friday at 3:00 pm to review for the final exam. i’ll bring my notes, and we can go over the key chapters together. let me know if that time works for everyone. - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: win a trip to hawaii now - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: sounds great! are you home now? - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Iteration 1/10 - Loss: 9.471623254699807\n",
            "Iteration 2/10 - Loss: 1.9382312679309237\n",
            "Iteration 3/10 - Loss: 0.000259315174234942\n",
            "Iteration 4/10 - Loss: 1.493598823643083e-05\n",
            "Iteration 5/10 - Loss: 6.810365595910639e-06\n",
            "Iteration 6/10 - Loss: 3.6711297484171013e-06\n",
            "Iteration 7/10 - Loss: 2.1905157090440497e-06\n",
            "Iteration 8/10 - Loss: 1.4002837369075927e-06\n",
            "Iteration 9/10 - Loss: 9.423620081494555e-07\n",
            "Iteration 10/10 - Loss: 6.605131346038018e-07\n",
            "\n",
            "Sample Prediction Output with probabilities:\n",
            "{'SPAM': 0.9997079968452454, 'HAM': 0.00029201694997027516}\n",
            "\n",
            "Accuracy: 90.0000%\n",
            "Precision: 91.1111%\n",
            "Recall: 90.0000%\n",
            "F1 Score: 88.6275%\n",
            "\n",
            "Enter a sample email for classification (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "# Create a blank SpaCy model and add the text classifier component\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"SPAM\")\n",
        "textcat.add_label(\"HAM\")\n",
        "\n",
        "# Define a minimal preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "#Example Training data\n",
        "train_data = [\n",
        "    (\"This is spam\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello, how are you?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You won a million dollars!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Claim your free prize now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Meeting at 10 AM tomorrow\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your invoice is attached\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Exclusive offer just for you!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Get a free iPhone today\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we reschedule our call?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Update your account details\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Limited time deal, buy now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your package has been shipped\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Win a trip to Hawaii now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Important meeting agenda\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You've been selected\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we discuss this project?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I am waiting machan. Call me once you free.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Sorry, I'll call later\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"You will be in the place of that man\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Please don't text me anymore. I have nothing else to say.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Thanks a lot for your wishes on my birthday. Thanks you for making my birthday truly memorable.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Aight, I'll hit you up when I get some cash\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Dont worry. I guess he's busy.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a £1500 Bonus Prize, call 09066364589\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Good stuff, will do.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"What time you coming down later?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Sounds great! Are you home now?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Finally the match heading towards draw as your prediction.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Tired. I haven't slept well the past few nights.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Where are you?when wil you reach here?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Please call our customer service representative on FREEPHONE 0808 145 4742 between 9am-11pm as you have WON a guaranteed £1000 cash or £5000 prize!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"What you doing? how are you?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I'm back, lemme know when you're ready\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Lose 20 pounds in just 2 weeks with our miracle weight loss pills! 100% natural and safe. Order today and get a special discount: www.weightlosspills.com. Hurry, offer expires soon!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Awesome, I'll see you in a bit\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Just sent it. So what type of food do you like?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I accidentally deleted the message. Resend please.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For terms & conditions, visit www.07781482378.com\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"I cant pick the phone right now. Pls send a message\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"He says he'll give me a call when his friend's got the money but that he's definitely buying before the end of the week\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You made my day. Do have a great day too.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Great news! You have been pre-approved for a personal loan of $10,000 with a low-interest rate! No credit check required. Apply today at www.getmymoney.com and get instant cash!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"No problem. How are you doing?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Just sleeping and surfing\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Busy here. Trying to finish for new year. I am looking forward to finally meeting you\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Cool, text me when you're ready\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"URGENT! Your Mobile number has been awarded with a £2000 prize GUARANTEED. Call 09058094455 from land line. Claim 3030. Valid 12hrs only\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Thanks for this hope you had a good day today\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I hope you that's the result of being consistently intelligent and kind. Start asking him about practicum links and keep your ears open and all the best. ttyl\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Just a quick reminder about our meeting tomorrow at 10:00 AM in the conference room. Please bring the project update documents with you.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Do you want to grab lunch today? I’m thinking of trying that new Italian place near the office. Let me know if you’re up for it!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Thank you for your recent purchase! Attached is the invoice for your order #45678. Your items will be shipped within 3-5 business days.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Guaranteed Highest Daily Rebates in the PH! Bet daily on any games and get up to 0.8% with UNLIMITED bonus! Check out more promos now: https://peryagame.com\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"I wanted to give you a quick update on the project. We’ve completed the design phase and will be moving into development next week. Let me know if you have any questions or need clarification on your tasks.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You’re invited to our annual company holiday party! Join us on December 15th at 7:00 PM for an evening of food, drinks, and fun. Please RSVP by December 1st.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Just wanted to let you know that we arrived safely at the cabin. The weather is beautiful, and we’re planning to go hiking tomorrow. I’ll send you some pictures later.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I hope this message finds you well. I wanted to follow up on my application for the software engineer position. I’m very interested in the role and would appreciate any updates on the hiring process.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Thank you for placing an order with us! Your order #78965 has been confirmed and is currently being processed. You will receive a shipping notification once it has been dispatched.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I’ve booked my tickets for the trip to Hawaii! We’re flying out on the 12th and coming back on the 18th. Let me know if you’re still interested in joining us—it’s going to be a great trip!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"PeryaGame 5% Cashback with Unlimited Bonus! PeryaGame offer Guaranteed Highest Daily Rebates in the PH! Check out more promos now: https://peryagame.com\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Welcome to our weekly newsletter! This week, we’re sharing tips on how to improve productivity and stay organized. Be sure to check out our latest articles and join our upcoming webinar on time management.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I hope you’re all doing well. I wanted to share a quick update on the project status. We’re on track to complete the next phase by the end of the week. I’ll schedule a meeting for next Monday to discuss the next steps.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Are we still on for dinner tonight? I’ve made a reservation at 7:30 PM at the new Thai place downtown. Let me know if anything changes.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Thank you for applying for the Marketing Manager position at XYZ Company. We are pleased to invite you for an interview on Monday, October 5th, at 2:00 PM. The interview will be conducted via Zoom, and the details will be sent to you soon.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Just a quick reminder that your homework assignment on Chapter 3 is due by Friday. Make sure to review the key concepts and submit your work on time.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"We are writing to inform you that a new software update is available for your device. Version 4.3 includes bug fixes, security enhancements, and new features. Please update your software at your earliest convenience to ensure optimal performance.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Claim your free 1999 bonus without deposit! Download the RG777 APP now for an 188P bonus. Install here: https://bit.ly/3Tn80Nc. Enjoy your rewards!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Dear User, we have detected suspicious activity in your bank account. To prevent your account from being suspended, please click on the link below and verify your details: www.banksecure.com. Failure to do so within 24 hours will result in account suspension.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"We’d like to invite you to our upcoming webinar on 'Building Effective Remote Teams.' The session will take place on September 30th at 11:00 AM. You’ll learn tips and strategies for managing remote employees and improving team collaboration.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your appointment with Dr. Smith has been confirmed for Thursday, October 8th, at 10:00 AM. If you need to reschedule, please contact us at least 24 hours in advance.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Just a reminder that our family vacation is coming up soon! We’ll be flying to Florida on the 15th, so make sure to pack everything by then. Also, don’t forget to bring sunscreen and your camera!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Earn $5,000 per week working from home! No experience needed. Start today and make money by simply filling out surveys. Click here to learn more: www.earnmoneyathome.com.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Just a heads up, we’re meeting at the library on Friday at 3:00 PM to review for the final exam. I’ll bring my notes, and we can go over the key chapters together. Let me know if that time works for everyone.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"We hope you’re enjoying your recent purchase from our store. We’d love to hear your feedback! Please take a moment to complete our brief survey, and let us know how we did.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your travel itinerary has been confirmed. You are scheduled to depart from New York on Flight 5678 at 9:00 AM on October 20th. Your return flight from London will be on Flight 6789 at 5:00 PM on October 27th.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Just a reminder to RSVP for our wedding on November 12th! We’re so excited to celebrate this special day with our family and friends. Please let us know if you’ll be attending by October 1st.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Search I88J1L1! Mag l0gin Get FREE B0nus P8888! PROMO CODE: 2MQS0CS live N0w!Claim unlimited B0nus N0w D0nt Miss 0ut Limited Days 0nly\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Our next book club meeting is scheduled for Tuesday, October 6th, at 6:00 PM. We’ll be discussing “The Alchemist” by Paulo Coelho. Make sure to finish reading it before the meeting, and bring your thoughts and questions for the discussion.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Thanks for sending over the initial draft. I’ve made a few changes to the document, and you can find the updated version attached. Let me know if you have any questions.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You have been selected as the winner of our $1,000,000 prize! Click here to claim your reward now: www.claimprize.com. Act fast! Offer expires soon.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Just a quick reminder about the office potluck on Friday! Don’t forget to bring a dish to share with your colleagues. Looking forward to seeing everyone’s culinary creations!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I hope you are doing well. I wanted to let you know that I have submitted my final report for the course. Please confirm when you receive it.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Please find attached the agenda for tomorrow’s meeting. We’ll be discussing the Q4 sales targets and the marketing strategy for the new product launch. Let me know if you have any points to add.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Hurry! Get a 90% discount on all our products! This is a one-time offer just for you! Visit www.superdeals.com and use code SAVE90 at checkout. Don't miss out on this amazing opportunity!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Wishing you a very happy birthday! I hope you have a fantastic day filled with joy, laughter, and cake! Let’s catch up soon.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"I just wanted to follow up on our last meeting. Have you had a chance to review the proposal we sent over? We’d love to hear your thoughts and discuss the next steps.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"We’re planning a family gathering at Grandma’s house next Sunday. We’ll have lunch around 1 PM, and it’ll be great to catch up with everyone. Let me know if you can make it!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"GCash: Account verification needed due to suspicious transaction. Kindly Visit: gcares-protect-ph.li to continue using our services\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Please note that the lecture scheduled for Tuesday, October 12th, has been moved to Thursday, October 14th, at 3 PM. The classroom remains the same. I apologize for any inconvenience.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"This is a reminder of your upcoming dentist appointment on Monday, October 18th, at 9:30 AM. Please contact us if you need to reschedule.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"This is a reminder that the book you borrowed, “The Catcher in the Rye,” is due for return on October 7th. Please return or renew it by then to avoid any late fees.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You’re all invited to our annual block party on Saturday, October 16th! We’ll have food, games, and music from 12 PM to 6 PM. Bring your family, and let’s have some fun!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Hi there! You have been selected for an all-expense-paid trip to the Bahamas! To claim your FREE vacation, all you need to do is fill out a quick survey. Click here now: www.freevacation.com.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Here are the notes from our weekly team meeting held today. Please review and let me know if there are any changes or additions. We’ll be following up on these action items next week.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your reservation at Oceanview Resort has been confirmed for October 10th to October 15th. We look forward to welcoming you. If you need assistance or have special requests, feel free to contact us.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}})\n",
        "]\n",
        "\n",
        "# Apply minimal text preprocessing\n",
        "train_data = [(preprocess_text(text), annotations) for text, annotations in train_data]\n",
        "\n",
        "# Prepare training data into SpaCy's Example format\n",
        "train_examples = []\n",
        "for text, annotations in train_data:\n",
        "    doc = nlpTC.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    train_examples.append(example)\n",
        "\n",
        "# Split data into training, validation, and testing sets\n",
        "train_examples, test_examples = train_test_split(train_examples, test_size=0.2, random_state=42)\n",
        "train_examples, val_examples = train_test_split(train_examples, test_size=0.25, random_state=42)  # 20% of the remaining data is used for validation\n",
        "\n",
        "# Print the split data to visualize each set\n",
        "print(\"TRAINING SET (60% of the data):\")\n",
        "for example in train_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "print(\"\\nVALIDATION SET (20% of the data):\")\n",
        "for example in val_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "print(\"\\nTESTING SET (20% of the data):\")\n",
        "for example in test_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "# Training the model with more iterations for small datasets\n",
        "n_iter = 10  # Set iterations\n",
        "optimizer = nlpTC.initialize()\n",
        "\n",
        "for i in range(n_iter):\n",
        "    losses = {}\n",
        "    for batch in spacy.util.minibatch(train_examples, size=2):  # Small batch size for small data\n",
        "        for example in batch:\n",
        "            nlpTC.update([example], sgd=optimizer, losses=losses)\n",
        "    print(f\"Iteration {i+1}/{n_iter} - Loss: {losses['textcat']}\")\n",
        "\n",
        "# Testing the model\n",
        "print(\"\\nSample Prediction Output with probabilities:\")\n",
        "doc = nlpTC(\"Claim your prize now!\")\n",
        "print(doc.cats)\n",
        "\n",
        "# Function to classify user input emails\n",
        "def classify_email(email):\n",
        "    email = preprocess_text(email)\n",
        "    doc = nlpTC(email)\n",
        "    spam_score = doc.cats['SPAM']\n",
        "    ham_score = doc.cats['HAM']\n",
        "\n",
        "    if spam_score > ham_score:\n",
        "        return \"SPAM\"\n",
        "    else:\n",
        "        return \"HAM\"\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score on the test set\n",
        "true_labels = [1 if example.reference.cats['SPAM'] == 1 else 0 for example in test_examples]\n",
        "predicted_labels = [1 if classify_email(example.reference.text) == 'SPAM' else 0 for example in test_examples]\n",
        "\n",
        "# Calculate and print metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nAccuracy: {accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")\n",
        "\n",
        "# Allow users to test the model by inputting their own data\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter a sample email for classification (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    classification = classify_email(user_input)\n",
        "    print(f\"The email is classified as: {classification}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLFctQrMI9q7"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code from the subset 1 is also the same in this code but the training data is consist of 100 data."
      ],
      "metadata": {
        "id": "dow52O8d5-Pc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNNMYwLAJAmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "613bde71-041f-4d32-9280-412bb16dd73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Precision: 35.3933%\n",
            "Recall: 100.0000%\n",
            "F1 Score: 52.2822%\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load a pre-trained NER model\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Sample training data (text and true entity annotations)\n",
        "training_data = [\n",
        "    (\"Microsoft announced a new AI initiative in Seattle.\", [(0, 9, \"ORG\"), (39, 46, \"GPE\")]),\n",
        "    (\"Google I/O will take place in May 2023.\", [(0, 10, \"EVENT\"), (29, 37, \"DATE\")]),\n",
        "    (\"The unemployment rate in the U.S. dropped to 3.5%.\", [(34, 38, \"PERCENT\"), (27, 31, \"GPE\")]),\n",
        "    (\"The Chinese economy grew by 5% last year.\", [(4, 11, \"NORP\")]),\n",
        "    (\"Sundar Pichai is the CEO of Google.\", [(0, 13, \"PERSON\"), (28, 34, \"ORG\")]),\n",
        "    (\"Tesla secured $2 billion in new funding.\", [(14, 22, \"MONEY\")]),\n",
        "    (\"Amazon is opening a new office in Vancouver.\", [(0, 6, \"ORG\"), (36, 45, \"GPE\")]),\n",
        "    (\"Samsung released its new Galaxy S22 phone.\", [(0, 7, \"ORG\"), (23, 32, \"PRODUCT\")]),\n",
        "    (\"The Pacific Ocean is the largest body of water on Earth.\", [(4, 17, \"LOC\")]),\n",
        "    (\"The headquarters of IBM is in New York City.\", [(21, 24, \"ORG\"), (31, 44, \"GPE\")]),\n",
        "\n",
        "    (\"Satya Nadella leads Microsoft Corporation.\", [(0, 12, \"PERSON\"), (19, 38, \"ORG\")]),\n",
        "    (\"The FIFA World Cup will be held in Qatar in 2022.\", [(4, 18, \"EVENT\"), (34, 39, \"GPE\"), (43, 47, \"DATE\")]),\n",
        "    (\"Apple plans to invest $10 billion in manufacturing.\", [(23, 32, \"MONEY\")]),\n",
        "    (\"A new skyscraper is being built in Dubai.\", [(33, 38, \"GPE\")]),\n",
        "    (\"70% of the world's population is now online.\", [(0, 3, \"PERCENT\")]),\n",
        "    (\"Elon Musk founded SpaceX and Tesla.\", [(0, 9, \"PERSON\"), (17, 23, \"ORG\"), (28, 33, \"ORG\")]),\n",
        "    (\"The startup raised $50 million in Series B.\", [(15, 25, \"MONEY\")]),\n",
        "    (\"The next Apple event is scheduled for March 25th.\", [(9, 14, \"ORG\"), (39, 48, \"DATE\")]),\n",
        "    (\"The new company is aiming for a 15% market share.\", [(28, 31, \"PERCENT\")]),\n",
        "    (\"Apple's iPhone 14 is expected to launch in 2023.\", [(0, 5, \"ORG\"), (7, 15, \"PRODUCT\"), (46, 50, \"DATE\")]),\n",
        "\n",
        "    (\"A German scientist won the Nobel Prize.\", [(2, 8, \"NORP\")]),\n",
        "    (\"Facebook plans to launch new features in June.\", [(0, 7, \"ORG\"), (30, 35, \"DATE\")]),\n",
        "    (\"The CEO of Apple, Tim Cook, announced new products.\", [(14, 22, \"PERSON\"), (4, 9, \"ORG\")]),\n",
        "    (\"NASA's Perseverance rover landed on Mars.\", [(0, 4, \"ORG\"), (34, 38, \"GPE\")]),\n",
        "    (\"The 2024 Summer Olympics will take place in Paris.\", [(4, 24, \"EVENT\"), (40, 45, \"GPE\")]),\n",
        "    (\"The inflation rate reached 8.6% last month.\", [(28, 32, \"PERCENT\")]),\n",
        "    (\"Coca-Cola launched a new flavor this spring.\", [(0, 10, \"ORG\"), (36, 41, \"DATE\")]),\n",
        "    (\"The World Health Organization declared a health emergency.\", [(4, 30, \"ORG\")]),\n",
        "    (\"Berkshire Hathaway's stock price increased by $500.\", [(0, 23, \"ORG\"), (37, 40, \"MONEY\")]),\n",
        "    (\"In 2020, remote work became the new normal.\", [(3, 7, \"DATE\")]),\n",
        "\n",
        "    (\"Mark Zuckerberg met with world leaders to discuss technology.\", [(0, 15, \"PERSON\")]),\n",
        "    (\"The Great Wall of China is a popular tourist attraction.\", [(4, 20, \"LOC\")]),\n",
        "    (\"The Grammy Awards will be held in Los Angeles.\", [(0, 14, \"EVENT\"), (30, 43, \"GPE\")]),\n",
        "    (\"Intel announced a new chip that will improve processing speed.\", [(0, 5, \"ORG\")]),\n",
        "    (\"The stock market saw a decline of 4% today.\", [(28, 31, \"PERCENT\")]),\n",
        "    (\"Microsoft is acquiring LinkedIn for $26.2 billion.\", [(0, 9, \"ORG\"), (26, 39, \"ORG\"), (44, 57, \"MONEY\")]),\n",
        "    (\"SpaceX plans to launch its Starship rocket next year.\", [(0, 6, \"ORG\"), (34, 39, \"DATE\")]),\n",
        "    (\"The next big tech conference is set for September.\", [(9, 13, \"EVENT\"), (38, 47, \"DATE\")]),\n",
        "    (\"The United Nations addresses global challenges.\", [(4, 17, \"ORG\")]),\n",
        "    (\"Bill Gates founded Microsoft in 1975.\", [(0, 10, \"PERSON\"), (21, 29, \"ORG\"), (32, 36, \"DATE\")]),\n",
        "\n",
        "    (\"A recent study showed that 60% of students prefer online classes.\", [(36, 38, \"PERCENT\")]),\n",
        "    (\"The Louvre Museum is located in Paris.\", [(4, 22, \"ORG\"), (30, 35, \"GPE\")]),\n",
        "    (\"The 2022 World Cup will be hosted in Qatar.\", [(4, 18, \"EVENT\"), (35, 40, \"GPE\")]),\n",
        "    (\"Netflix added 8 million new subscribers in 2021.\", [(7, 14, \"ORG\"), (23, 24, \"MONEY\"), (29, 33, \"DATE\")]),\n",
        "    (\"The first electric car was launched by Tesla in 2008.\", [(29, 34, \"ORG\"), (39, 43, \"DATE\")]),\n",
        "    (\"Researchers found a new species of frog in Madagascar.\", [(36, 49, \"LOC\")]),\n",
        "    (\"In 2019, the world saw significant advancements in AI.\", [(3, 7, \"DATE\")]),\n",
        "    (\"The White House issued a statement regarding climate change.\", [(4, 15, \"GPE\")]),\n",
        "    (\"Elon Musk is the founder of SpaceX and Tesla.\", [(0, 9, \"PERSON\"), (23, 29, \"ORG\"), (34, 39, \"ORG\")]),\n",
        "    (\"Tesla plans to produce 20 million cars by 2030.\", [(0, 5, \"ORG\"), (34, 40, \"PERCENT\"), (44, 48, \"DATE\")]),\n",
        "\n",
        "    (\"The next FIFA World Cup will be in 2026.\", [(9, 13, \"EVENT\"), (25, 29, \"DATE\")]),\n",
        "    (\"Apple's market share reached an all-time high.\", [(0, 5, \"ORG\"), (27, 35, \"PERCENT\")]),\n",
        "    (\"Amazon Prime Video will launch new shows this fall.\", [(0, 6, \"ORG\"), (36, 41, \"DATE\")]),\n",
        "    (\"Google's headquarters is in Mountain View.\", [(0, 6, \"ORG\"), (29, 32, \"GPE\")]),\n",
        "    (\"Facebook was founded by Mark Zuckerberg.\", [(0, 8, \"ORG\"), (22, 36, \"PERSON\")]),\n",
        "    (\"The United Kingdom is hosting the G7 summit.\", [(4, 17, \"GPE\"), (31, 35, \"EVENT\")]),\n",
        "    (\"Sony released the PlayStation 5 in late 2020.\", [(0, 4, \"ORG\"), (16, 30, \"PRODUCT\"), (34, 38, \"DATE\")]),\n",
        "    (\"The next lunar eclipse will be on November 8th.\", [(9, 14, \"EVENT\"), (27, 34, \"DATE\")]),\n",
        "    (\"Elon Musk is developing a new satellite internet service.\", [(0, 9, \"PERSON\"), (30, 40, \"PRODUCT\")]),\n",
        "    (\"The Amazon rainforest is crucial for biodiversity.\", [(4, 10, \"LOC\")]),\n",
        "\n",
        "    (\"Toyota unveiled its electric car lineup this year.\", [(0, 6, \"ORG\"), (39, 43, \"DATE\")]),\n",
        "    (\"The Summer Olympics will take place in Tokyo in 2021.\", [(4, 24, \"EVENT\"), (38, 43, \"GPE\"), (46, 50, \"DATE\")]),\n",
        "    (\"The Eiffel Tower is one of the most visited monuments.\", [(4, 15, \"LOC\")]),\n",
        "    (\"NASA's Artemis program aims to return humans to the Moon.\", [(0, 4, \"ORG\"), (35, 50, \"EVENT\")]),\n",
        "    (\"The stock market experienced a significant downturn.\", [(4, 9, \"LOC\")]),\n",
        "    (\"Gold prices surged to an all-time high this week.\", [(0, 4, \"MONEY\"), (25, 35, \"DATE\")]),\n",
        "    (\"The Met Gala is a major fundraising event.\", [(4, 12, \"EVENT\")]),\n",
        "    (\"The Berlin Wall fell in 1989.\", [(4, 15, \"LOC\"), (19, 23, \"DATE\")]),\n",
        "    (\"Instagram was acquired by Facebook in 2012.\", [(0, 9, \"ORG\"), (23, 30, \"ORG\"), (34, 38, \"DATE\")]),\n",
        "    (\"Microsoft will invest in renewable energy projects.\", [(0, 9, \"ORG\")]),\n",
        "\n",
        "    (\"The World Cup is set to take place in Qatar.\", [(4, 10, \"EVENT\"), (26, 32, \"GPE\")]),\n",
        "    (\"The Great Barrier Reef is located off the coast of Australia.\", [(4, 21, \"LOC\")]),\n",
        "    (\"Bill Gates and Melinda Gates announced their divorce.\", [(0, 10, \"PERSON\"), (15, 27, \"PERSON\")]),\n",
        "    (\"The 2024 presidential election will be highly competitive.\", [(4, 36, \"EVENT\"), (40, 50, \"DATE\")]),\n",
        "    (\"SpaceX's Falcon Heavy launched successfully last year.\", [(0, 6, \"ORG\"), (7, 17, \"PRODUCT\"), (36, 41, \"DATE\")]),\n",
        "    (\"The new iPhone model features advanced camera technology.\", [(4, 9, \"ORG\"), (20, 27, \"PRODUCT\")]),\n",
        "    (\"Alibaba's revenue soared during the pandemic.\", [(0, 7, \"ORG\")]),\n",
        "    (\"The Cannes Film Festival is a prestigious event.\", [(4, 27, \"EVENT\")]),\n",
        "    (\"The Tesla Model 3 has become very popular.\", [(0, 5, \"ORG\"), (10, 22, \"PRODUCT\")]),\n",
        "    (\"Virtual reality is gaining traction in gaming.\", [(0, 7, \"LOC\")]),\n",
        "\n",
        "    (\"The COVID-19 vaccine rollout has accelerated globally.\", [(4, 12, \"EVENT\")]),\n",
        "    (\"Google's Android operating system dominates the market.\", [(0, 6, \"ORG\")]),\n",
        "    (\"The Nobel Peace Prize was awarded to Malala Yousafzai.\", [(4, 28, \"EVENT\"), (33, 50, \"PERSON\")]),\n",
        "    (\"The tech industry is evolving rapidly with AI advancements.\", [(4, 12, \"LOC\")]),\n",
        "    (\"Elon Musk plans to send humans to Mars.\", [(0, 9, \"PERSON\"), (23, 27, \"GPE\")]),\n",
        "    (\"The 2021 Tokyo Olympics faced many challenges.\", [(4, 25, \"EVENT\"), (31, 36, \"DATE\")]),\n",
        "    (\"The British Royal Family attended the funeral of Prince Philip.\", [(4, 31, \"GPE\"), (39, 50, \"PERSON\")]),\n",
        "    (\"Netflix is producing a new documentary series.\", [(0, 7, \"ORG\")]),\n",
        "    (\"The Paris Agreement addresses climate change issues.\", [(4, 18, \"EVENT\")]),\n",
        "    (\"The Olympic Games in Paris are highly anticipated.\", [(4, 20, \"EVENT\"), (26, 31, \"GPE\")]),\n",
        "\n",
        "    (\"The smartphone market is becoming saturated.\", [(4, 14, \"LOC\")]),\n",
        "    (\"Amazon is facing increased competition from Walmart.\", [(0, 6, \"ORG\"), (33, 39, \"ORG\")]),\n",
        "    (\"The United Nations General Assembly meets annually.\", [(4, 36, \"ORG\")]),\n",
        "    (\"The 2023 Cricket World Cup will be hosted by India.\", [(4, 27, \"EVENT\"), (40, 45, \"GPE\")]),\n",
        "    (\"Tesla's stock prices have fluctuated dramatically.\", [(0, 5, \"ORG\")]),\n",
        "    (\"The Grammy Awards are held every year.\", [(4, 18, \"EVENT\")]),\n",
        "    (\"The Eiffel Tower attracts millions of tourists every year.\", [(4, 15, \"LOC\"), (36, 41, \"DATE\")]),\n",
        "    (\"NASA's Mars Rover is searching for signs of life.\", [(0, 4, \"ORG\")]),\n",
        "    (\"The 2024 U.S. Presidential election is coming up.\", [(4, 26, \"EVENT\"), (30, 34, \"DATE\")]),\n",
        "    (\"Tesla is set to launch its new Cybertruck.\", [(0, 5, \"ORG\"), (30, 34, \"PRODUCT\")]),\n",
        "]\n",
        "\n",
        "\n",
        "# Preprocess: Convert all texts to lowercase\n",
        "preprocessed_data = [(text.lower(), entities) for text, entities in training_data]\n",
        "\n",
        "# Initialize lists for storing true and predicted entities\n",
        "all_true_entities = []\n",
        "all_pred_entities = []\n",
        "\n",
        "# Iterate through training data\n",
        "for text, true_entities in training_data:\n",
        "    # Run NER model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Predicted entities from the model\n",
        "    pred_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Store true and predicted entities for all examples\n",
        "    all_true_entities.extend([(ent[0], ent[1], ent[2]) for ent in true_entities])\n",
        "    all_pred_entities.extend([(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents])\n",
        "\n",
        "# Convert to binary classification (1 for correct entity, 0 for incorrect)\n",
        "y_true = [1 if ent in all_true_entities else 0 for ent in all_pred_entities]\n",
        "y_pred = [1 for _ in all_pred_entities]  # Assuming all predictions are correct for now\n",
        "\n",
        "# Calculate Precision, Recall, F1\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUMZmS92JCD5"
      },
      "source": [
        "## Part of Speech Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code from the subset 1 is also the same in this code but the training data is consist of 100 data."
      ],
      "metadata": {
        "id": "Fi0cauHH5_tN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v1YBftGJDwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "578f5395-6675-4113-a6f0-bde7fa401011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Consolidated Metrics across Training, Validation, and Test Data:\n",
            "Accuracy: 13.6840%\n",
            "Precision: 15.6382%\n",
            "Recall: 13.6840%\n",
            "F1 Score: 14.5731%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load spaCy's POS tagging model\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Sample expanded training data: list of (text, true_pos_tags) pairs\n",
        "training_data = [\n",
        "    (\"She sells seashells by the seashore.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The quick brown fox jumps over the lazy dog.\", ['DET', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"I love coding in Python.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'PROPN']),\n",
        "    (\"Birds fly in the sky.\", ['NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"Alice and Bob went to the market.\", ['PROPN', 'CCONJ', 'PROPN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"Reading books is fun.\", ['VERB', 'NOUN', 'AUX', 'ADJ']),\n",
        "    (\"My car is very fast.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"We are going to the zoo.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"It is raining today.\", ['PRON', 'AUX', 'VERB', 'NOUN']),\n",
        "    (\"Programming languages are interesting.\", ['NOUN', 'NOUN', 'AUX', 'ADJ']),\n",
        "\n",
        "    (\"The cat sleeps on the mat.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"John likes to play soccer.\", ['PROPN', 'VERB', 'PART', 'VERB', 'NOUN']),\n",
        "    (\"She is learning French.\", ['PRON', 'AUX', 'VERB', 'PROPN']),\n",
        "    (\"The weather is nice today.\", ['DET', 'NOUN', 'AUX', 'ADJ', 'NOUN']),\n",
        "    (\"He bought a new laptop yesterday.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN']),\n",
        "    (\"They are swimming in the pool.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The pizza smells delicious.\", ['DET', 'NOUN', 'VERB', 'ADJ']),\n",
        "    (\"Can you help me with this project?\", ['AUX', 'PRON', 'VERB', 'PRON', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"This task is quite difficult.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"He enjoys reading books.\", ['PRON', 'VERB', 'VERB', 'NOUN']),\n",
        "\n",
        "    (\"The dog barked loudly at the strangers.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"I have a meeting tomorrow.\", ['PRON', 'AUX', 'DET', 'NOUN', 'ADJ']),\n",
        "    (\"They will travel to Spain next year.\", ['PRON', 'AUX', 'VERB', 'ADP', 'PROPN', 'ADV', 'NOUN']),\n",
        "    (\"He plays the guitar beautifully.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADV']),\n",
        "    (\"The book on the shelf is mine.\", ['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'AUX', 'PRON']),\n",
        "    (\"Jessica ran a marathon last summer.\", ['PROPN', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN']),\n",
        "    (\"Cooking is a wonderful hobby.\", ['VERB', 'AUX', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"The stars shine brightly in the night sky.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN', 'NOUN']),\n",
        "    (\"I am learning how to code.\", ['PRON', 'AUX', 'VERB', 'ADV', 'ADP', 'VERB']),\n",
        "    (\"The flowers bloom in spring.\", ['DET', 'NOUN', 'VERB', 'ADP', 'NOUN']),\n",
        "\n",
        "    (\"My friends enjoy hiking on weekends.\", ['DET', 'NOUN', 'VERB', 'VERB', 'ADP', 'NOUN']),\n",
        "    (\"Dogs are great companions.\", ['NOUN', 'AUX', 'ADJ', 'NOUN']),\n",
        "    (\"She wrote an amazing story.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"The sun rises in the east.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"He plays soccer every weekend.\", ['PRON', 'VERB', 'NOUN', 'ADV', 'NOUN']),\n",
        "    (\"Reading novels helps improve vocabulary.\", ['VERB', 'NOUN', 'VERB', 'VERB', 'NOUN']),\n",
        "    (\"My family enjoys movie nights.\", ['DET', 'NOUN', 'VERB', 'NOUN', 'NOUN']),\n",
        "    (\"She is very talented in music.\", ['PRON', 'AUX', 'ADV', 'ADJ', 'ADP', 'NOUN']),\n",
        "    (\"We will celebrate his birthday soon.\", ['PRON', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADV']),\n",
        "    (\"The wind blew fiercely during the storm.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "\n",
        "    (\"They went hiking in the mountains.\", ['PRON', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"This recipe is quite easy.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"The teacher explains concepts clearly.\", ['DET', 'NOUN', 'VERB', 'NOUN', 'ADV']),\n",
        "    (\"We have been working on this project.\", ['PRON', 'AUX', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"He prefers tea over coffee.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'NOUN']),\n",
        "    (\"The child laughed joyfully at the joke.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She likes to dance at parties.\", ['PRON', 'VERB', 'PART', 'VERB', 'ADP', 'NOUN']),\n",
        "    (\"They are playing video games right now.\", ['PRON', 'AUX', 'VERB', 'NOUN', 'ADV', 'ADV']),\n",
        "    (\"The cat chased the mouse.\", ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']),\n",
        "    (\"Jack and Jill went up the hill.\", ['PROPN', 'CCONJ', 'PROPN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "\n",
        "    (\"The children are laughing in the park.\", ['DET', 'NOUN', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She loves to read novels.\", ['PRON', 'VERB', 'PART', 'VERB', 'NOUN']),\n",
        "    (\"The fish swims in the ocean.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"He wrote a letter yesterday.\", ['PRON', 'VERB', 'DET', 'NOUN', 'NOUN']),\n",
        "    (\"They are playing soccer after school.\", ['PRON', 'AUX', 'VERB', 'NOUN', 'ADP', 'NOUN']),\n",
        "    (\"The chef prepares delicious meals.\", ['DET', 'NOUN', 'VERB', 'ADJ', 'NOUN']),\n",
        "    (\"We will visit our grandparents next weekend.\", ['PRON', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADV', 'NOUN']),\n",
        "    (\"The dog fetches the ball.\", ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']),\n",
        "    (\"She enjoys painting landscapes.\", ['PRON', 'VERB', 'VERB', 'NOUN']),\n",
        "    (\"The phone rang unexpectedly.\", ['DET', 'NOUN', 'VERB', 'ADV']),\n",
        "\n",
        "    (\"They will join us for dinner.\", ['PRON', 'AUX', 'VERB', 'PRON', 'ADP', 'NOUN']),\n",
        "    (\"He is running very fast.\", ['PRON', 'AUX', 'VERB', 'ADV', 'ADJ']),\n",
        "    (\"The train arrives at the station.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She sings beautifully on stage.\", ['PRON', 'VERB', 'ADV', 'ADP', 'NOUN']),\n",
        "    (\"The baby cried all night.\", ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']),\n",
        "    (\"We are going shopping tomorrow.\", ['PRON', 'AUX', 'VERB', 'VERB', 'NOUN']),\n",
        "    (\"He finished his homework before dinner.\", ['PRON', 'VERB', 'PRON', 'NOUN', 'ADP', 'NOUN']),\n",
        "    (\"The sun sets in the west.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She has a beautiful voice.\", ['PRON', 'AUX', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"The garden is full of flowers.\", ['DET', 'NOUN', 'AUX', 'ADJ', 'ADP', 'NOUN']),\n",
        "\n",
        "    (\"They watched a movie last night.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN']),\n",
        "    (\"The children played happily at the playground.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"We are studying for the exam.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"He kicked the ball into the goal.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She is going to the concert tonight.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN', 'ADV']),\n",
        "    (\"The computer crashed during the update.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"I saw a shooting star.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"They will attend the meeting next week.\", ['PRON', 'AUX', 'VERB', 'DET', 'NOUN', 'ADV', 'NOUN']),\n",
        "    (\"The mountain trail is steep.\", ['DET', 'NOUN', 'AUX', 'ADJ']),\n",
        "    (\"He traveled to Paris last summer.\", ['PRON', 'VERB', 'ADP', 'PROPN', 'ADJ', 'NOUN']),\n",
        "\n",
        "    (\"She baked cookies for her friends.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'PRON', 'NOUN']),\n",
        "    (\"The artist painted a stunning mural.\", ['DET', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"We enjoy exploring new places.\", ['PRON', 'VERB', 'VERB', 'ADJ', 'NOUN']),\n",
        "    (\"He repaired the broken fence.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"They discovered a hidden treasure.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"The baby laughed at the puppy.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She danced gracefully across the stage.\", ['PRON', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The team won the championship.\", ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']),\n",
        "    (\"I found a great restaurant.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"He enjoys hiking during the summer.\", ['PRON', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "\n",
        "    (\"The car sped down the highway.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She likes to play the piano.\", ['PRON', 'VERB', 'PART', 'VERB', 'DET', 'NOUN']),\n",
        "    (\"They ran a marathon in record time.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'NOUN']),\n",
        "    (\"The flowers bloomed beautifully in spring.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'NOUN']),\n",
        "    (\"She plays the violin effortlessly.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADV']),\n",
        "    (\"We visited the art museum yesterday.\", ['PRON', 'VERB', 'DET', 'NOUN', 'NOUN', 'NOUN']),\n",
        "    (\"The car sped down the highway.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She likes to play the piano.\", ['PRON', 'VERB', 'PART', 'VERB', 'DET', 'NOUN']),\n",
        "    (\"They ran a marathon in record time.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'NOUN']),\n",
        "    (\"The flowers bloomed beautifully in spring.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'NOUN']),\n",
        "]\n",
        "\n",
        "# Preprocess text: convert to lowercase and shuffle the training data\n",
        "training_data = [(text.lower(), tags) for text, tags in training_data]\n",
        "random.shuffle(training_data)\n",
        "\n",
        "# Split data into training, validation, and test sets (60% train, 20% validation, 20% test)\n",
        "train_data, temp_data = train_test_split(training_data, test_size=0.4, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Initialize lists to store true and predicted POS tags for all sets\n",
        "all_true_pos_tags_train, all_predicted_pos_tags_train = [], []\n",
        "all_true_pos_tags_val, all_predicted_pos_tags_val = [], []\n",
        "all_true_pos_tags_test, all_predicted_pos_tags_test = [], []\n",
        "\n",
        "# Function to process data and evaluate POS tagging\n",
        "def process_data(data, all_true_pos_tags, all_predicted_pos_tags):\n",
        "    for text, true_pos_tags in data:\n",
        "        # Process the text with spaCy\n",
        "        doc = nlp(text)\n",
        "        # Extract predicted POS tags\n",
        "        predicted_pos_tags = [token.pos_ for token in doc]\n",
        "        # Extend lists with true and predicted tags for evaluation\n",
        "        all_true_pos_tags.extend(true_pos_tags)\n",
        "        all_predicted_pos_tags.extend(predicted_pos_tags)\n",
        "\n",
        "# Process training, validation, and test data\n",
        "process_data(train_data, all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "process_data(val_data, all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "process_data(test_data, all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Ensure both lists are the same length to avoid ValueError\n",
        "def ensure_equal_length(true_tags, predicted_tags):\n",
        "    if len(true_tags) != len(predicted_tags):\n",
        "        min_length = min(len(true_tags), len(predicted_tags))\n",
        "        true_tags = true_tags[:min_length]\n",
        "        predicted_tags = predicted_tags[:min_length]\n",
        "    return true_tags, predicted_tags\n",
        "\n",
        "# Ensure correct lengths for all sets\n",
        "all_true_pos_tags_train, all_predicted_pos_tags_train = ensure_equal_length(all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "all_true_pos_tags_val, all_predicted_pos_tags_val = ensure_equal_length(all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "all_true_pos_tags_test, all_predicted_pos_tags_test = ensure_equal_length(all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Function to calculate metrics for a dataset\n",
        "def evaluate_metrics(true_tags, predicted_tags):\n",
        "    accuracy = accuracy_score(true_tags, predicted_tags)\n",
        "    precision = precision_score(true_tags, predicted_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, predicted_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, predicted_tags, average='weighted')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate on training, validation, and test sets\n",
        "metrics_train = evaluate_metrics(all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "metrics_val = evaluate_metrics(all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "metrics_test = evaluate_metrics(all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Combine all metrics into single print statement\n",
        "total_accuracy = (metrics_train[0] + metrics_val[0] + metrics_test[0]) / 3\n",
        "total_precision = (metrics_train[1] + metrics_val[1] + metrics_test[1]) / 3\n",
        "total_recall = (metrics_train[2] + metrics_val[2] + metrics_test[2]) / 3\n",
        "total_f1 = (metrics_train[3] + metrics_val[3] + metrics_test[3]) / 3\n",
        "\n",
        "# Print consolidated metrics\n",
        "print(\"Consolidated Metrics across Training, Validation, and Test Data:\")\n",
        "print(f\"Accuracy: {total_accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {total_precision * 100:.4f}%\")\n",
        "print(f\"Recall: {total_recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {total_f1 * 100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM8dTOxtJGnX"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code from the subset 1 is also the same in this code but the training data is consist of 100 data."
      ],
      "metadata": {
        "id": "fUu5JDiv6BIc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zINv65K7JFPb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load a blank model and add text classifier\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGATIVE\")\n",
        "\n",
        "train_data = [\n",
        "    (\"I'm so frustrated with how slow my internet is.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I'm so happy with my new job!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The customer service at that store is excellent.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The movie was a complete waste of time.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That movie was truly heartwarming and beautiful.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been feeling really down lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The food at the new restaurant was absolutely delicious.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t get the job, and I feel so defeated.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The sunset this evening was breathtaking.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m really upset that I missed the deadline.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"I finally finished the book, and it was such a rewarding read.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"This weather is terrible, I can’t wait for it to end.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The surprise party was such a success!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My laptop crashed again, and I lost all my work.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The concert was absolutely mind-blowing!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been struggling with my workload lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The flowers you sent me are absolutely stunning.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I regret spending money on that product.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just found out I won the contest! I’m over the moon.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"Everything seems to be going wrong lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"You did a fantastic job on that presentation.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m tired of dealing with all this stress.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I couldn’t be happier with how everything turned out.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My car broke down again, and I’m so frustrated.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That was one of the most enjoyable dinners I’ve had in ages.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m feeling really overwhelmed with everything going on.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m incredibly grateful for the support I’ve received.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t enjoy the event; it was a total letdown.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m so proud of everything we’ve accomplished this year.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"That comment really hurt my feelings.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"That was the best coffee I’ve had in a while!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I can’t believe how rude they were to me.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I got a promotion at work, and I couldn’t be more thrilled.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My phone screen cracked, and now I have to get it replaced.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"What a beautiful and sunny day!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m really anxious about everything going on.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"Spending time with family over the holidays was perfect.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t get enough sleep last night, and now I’m exhausted.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"This new app makes my life so much easier.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been really unmotivated lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"That vacation was exactly what I needed.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"That presentation did not go well at all.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I appreciate all the effort you put into this project.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My relationship with my friends hasn’t been great lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’ve made some great new friends recently.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The traffic was horrible, and I barely made it on time.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I had such a fun time with the kids at the park today.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t enjoy the book at all; it was so boring.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just got my dream job, and I’m beyond excited!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I feel like I’ve been making one mistake after another.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"I had a terrible experience with the customer service rep.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’ve been feeling so energetic and positive lately!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I'm feeling completely hopeless today.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That was the most amazing concert I’ve ever attended!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I can’t believe I lost my wallet again.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"My birthday party was so much fun, I loved it!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The service at the restaurant was terrible, I’m never going back.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’ve been so productive today, I got everything done!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I had an argument with my best friend, and now I feel awful.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I received a surprise gift, and it made my entire day.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "\n",
        "    (\"I didn’t get the promotion, and now I’m feeling defeated.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That was the best vacation I’ve had in years.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been feeling anxious and restless lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m extremely proud of how far I’ve come.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I don’t know why, but I’m feeling really down today.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The presentation went really well, I’m so relieved.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m struggling to stay positive with everything going wrong.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I feel incredibly blessed to have such supportive friends.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My flight got canceled, and now I’m stuck at the airport.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I got a big raise at work, I’m so happy!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "\n",
        "    (\"I’ve been feeling very isolated and alone recently.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just adopted a puppy, and I’m beyond excited!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve had the worst headache all day, it won’t go away.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m thrilled to have finished that project ahead of time.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been getting really stressed about all my deadlines.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That new movie was so entertaining, I loved every minute.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m so frustrated with how long this process is taking.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’ve never been more excited for the future.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The noise in my neighborhood is driving me crazy.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I finally achieved my fitness goals, I feel amazing.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "\n",
        "    (\"I’ve been having such a hard time balancing work and life.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I got to reconnect with an old friend today, it was so great.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m feeling really insecure about everything right now.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That new café is fantastic, I’ll definitely be going back.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m so upset that my favorite show got canceled.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just finished a great workout, I feel so energized!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m so disappointed in how things turned out.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That speech was truly inspiring, I’m so motivated now.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m incredibly tired of dealing with all these problems.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just finished reading an amazing book, I couldn’t put it down.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "\n",
        "    (\"I feel like everything is falling apart.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just won tickets to see my favorite band live!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been feeling really disconnected from everyone lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just got engaged, and I couldn’t be happier!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m really disappointed with the service I received today.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m so excited to start this new chapter in my life.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m feeling really anxious about the upcoming event.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That was the most fun I’ve had in years.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m feeling really low and unsure about everything right now.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m grateful for all the blessings in my life.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}})\n",
        "]\n",
        "\n",
        "#Lowercasing\n",
        "train_data = [(text.lower(), labels) for text, labels in train_data]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text data from train_data\n",
        "text = [data[0] for data in train_data]\n",
        "labels = [data[1]['cats']['POSITIVE'] for data in train_data] # Extract labels\n",
        "\n",
        "# Vectorize text data using the extracted text list\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict sentiments\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")"
      ],
      "metadata": {
        "id": "zjGRu0yUcCPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd139973-7301-44ca-e468-3345ca4306bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 80.0000%\n",
            "Precision: 73.3333%\n",
            "Recall: 91.6667%\n",
            "F1 Score: 81.4815%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hplr8kIoJMkY"
      },
      "source": [
        "## Text Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Lxa6UCS3JJ89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ffb5a3-6311-45ec-c4f7-30180d81a815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: climate change is one of the most pressing issues of our time. the increasing levels of greenhouse gases in the atmosphere\n",
            "have led to rising global temperatures. as a result, glaciers are melting, sea levels are rising, and extreme weather events\n",
            "are becoming more frequent.\n",
            "Precision: 0.00%\n",
            "Recall: 0.00%\n",
            "F1 Score: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download the punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Example text and reference summary\n",
        "text = \"\"\"Climate change is one of the most pressing issues of our time. The increasing levels of greenhouse gases in the atmosphere\n",
        "have led to rising global temperatures. As a result, glaciers are melting, sea levels are rising, and extreme weather events\n",
        "are becoming more frequent. Many governments around the world have pledged to reduce carbon emissions, but progress has been slow.\n",
        "Renewable energy sources such as solar and wind power offer hope, but their adoption has not been widespread enough to make a significant impact yet.\n",
        "Urgent action is needed to address this global crisis before it’s too late.\"\"\"\n",
        "\n",
        "reference_summary = \"\"\"Climate change is caused by greenhouse gases and is leading to rising temperatures and extreme weather.\n",
        "Renewable energy offers hope, but its adoption is slow.\"\"\"\n",
        "\n",
        "# Extractive summarization function with lowercase preprocessing\n",
        "def extractive_summary(text, num_sentences=3):\n",
        "    doc = nlp(text.lower())  # Convert text to lowercase before processing\n",
        "    sentences = [sent.text.lower() for sent in doc.sents]  # Convert sentences to lowercase\n",
        "    return sentences[:num_sentences]  # Return the first `num_sentences` as the summary\n",
        "\n",
        "# Tokenizing the reference and generated summaries into sentences\n",
        "generated_summary = extractive_summary(text)  # Summary in lowercase\n",
        "reference_sentences = [sent.lower() for sent in sent_tokenize(reference_summary)]  # Reference in lowercase\n",
        "\n",
        "# Convert to binary relevance: 1 if the sentence appears in the reference summary, 0 otherwise\n",
        "y_true = [1 if sent in reference_sentences else 0 for sent in sent_tokenize(text.lower())]  # Compare with reference\n",
        "y_pred = [1 if sent in generated_summary else 0 for sent in sent_tokenize(text.lower())]  # Compare with generated summary\n",
        "\n",
        "# Ensure y_true and y_pred are of the same length\n",
        "if len(y_true) != len(y_pred):\n",
        "    min_length = min(len(y_true), len(y_pred))\n",
        "    y_true = y_true[:min_length]\n",
        "    y_pred = y_pred[:min_length]\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred) * 100  # Convert to percentage\n",
        "recall = recall_score(y_true, y_pred) * 100  # Convert to percentage\n",
        "f1 = f1_score(y_true, y_pred) * 100  # Convert to percentage\n",
        "\n",
        "# Output results\n",
        "print(f\"Generated Summary: {' '.join(generated_summary)}\")\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"Recall: {recall:.2f}%\")\n",
        "print(f\"F1 Score: {f1:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgPbK1KJGvPB"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78A3osq3JO3l"
      },
      "source": [
        "# **Subset 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aszu3K2WJynq"
      },
      "source": [
        "## Text Classification\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is same as the code from subset 1 but the training data contains a total of 20 samples. A detailed preprocessing function is provided, which aims at lowering the case of text, scrapping special signs and stopwords from the text respectively which aims at improving the quality of the text before it is fed for training.\n"
      ],
      "metadata": {
        "id": "R5EbTSeC8KCZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCITXOO7J0_9",
        "outputId": "c3d18429-c6c7-4ef3-bae3-0ce5a6f85df9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING SET (60% of the data):\n",
            "Text: limited time deal buy - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: win trip hawaii - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: update account details - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: waiting machan free - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: package shipped - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: exclusive offer - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: hello s saturday texting d decided tomo m trying invite - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: won million dollars - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: meeting 10 tomorrow - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: free iphone today - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: claim free prize - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: important meeting agenda - Label: {'SPAM': 0, 'HAM': 1}\n",
            "\n",
            "VALIDATION SET (20% of the data):\n",
            "Text: reschedule - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: invoice attached - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: catch bus   frying egg   tea eating moms left dinner   feel love - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: congratulations ve selected - Label: {'SPAM': 1, 'HAM': 0}\n",
            "\n",
            "TESTING SET (20% of the data):\n",
            "Text: spam - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: urgent won 1 week free membership 100000 prize jackpot txt word claim 81010 tc wwwdbuknet lccltd pobox 4403ldnw1a7rw18 - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: discuss project - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: hello - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Iteration 1/10 - Loss: 3.024702176451683\n",
            "Iteration 2/10 - Loss: 1.4147557206451893\n",
            "Iteration 3/10 - Loss: 0.14218342397361994\n",
            "Iteration 4/10 - Loss: 0.0010761727990029613\n",
            "Iteration 5/10 - Loss: 4.089708335186515e-05\n",
            "Iteration 6/10 - Loss: 9.413961251425462e-06\n",
            "Iteration 7/10 - Loss: 4.770714582491564e-06\n",
            "Iteration 8/10 - Loss: 3.2560074316734244e-06\n",
            "Iteration 9/10 - Loss: 2.4826530804489266e-06\n",
            "Iteration 10/10 - Loss: 1.9787617819844172e-06\n",
            "\n",
            "Sample Prediction Output with probabilities:\n",
            "{'SPAM': 0.9411798715591431, 'HAM': 0.05882018059492111}\n",
            "\n",
            "Accuracy: 75.0000%\n",
            "Precision: 83.3333%\n",
            "Recall: 75.0000%\n",
            "F1 Score: 73.3333%\n",
            "\n",
            "Enter a sample email for classification (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "# Create a blank SpaCy model and add the text classifier component\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"SPAM\")\n",
        "textcat.add_label(\"HAM\")\n",
        "\n",
        "# Example Training data\n",
        "train_data = [\n",
        "    (\"This is spam\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello, how are you?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You won a million dollars!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Claim your free prize now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Meeting at 10 AM tomorrow\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your invoice is attached\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Exclusive offer just for you!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Get a free iPhone today\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we reschedule our call?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Update your account details\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Limited time deal, buy now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your package has been shipped\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Win a trip to Hawaii now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Important meeting agenda\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You've been selected\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we discuss this project?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I am waiting machan. Call me once you free.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "]\n",
        "\n",
        "# Prepare training data into SpaCy's Example format\n",
        "train_examples = []\n",
        "for text, annotations in train_data:\n",
        "    doc = nlpTC.make_doc(text) # create doc prior to preprocessing\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    train_examples.append(example)\n",
        "\n",
        "# Initialize the textcat component with the training examples\n",
        "nlpTC.initialize(lambda: train_examples)\n",
        "\n",
        "# Define a comprehensive preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = [token.text for token in nlpTC(text) if not token.is_stop]  # Directly extract tokens\n",
        "    # Join tokens back to a string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply minimal text preprocessing to train_examples after initializing the pipeline\n",
        "train_examples_preprocessed = []\n",
        "for example in train_examples:\n",
        "    # Process the text and create a new Doc object\n",
        "    preprocessed_text = preprocess_text(example.reference.text)\n",
        "    preprocessed_doc = nlpTC.make_doc(preprocessed_text)\n",
        "\n",
        "    # Create a new Example with the preprocessed Doc and original annotations\n",
        "    # Instead of example.reference.cats, use Example.from_dict with manual setting of cats\n",
        "    new_example = Example.from_dict(preprocessed_doc, {\"cats\": example.reference.cats})\n",
        "    train_examples_preprocessed.append(new_example)\n",
        "\n",
        "train_examples = train_examples_preprocessed # replace with preprocessed examples\n",
        "# Apply the comprehensive text preprocessing\n",
        "train_data = [(preprocess_text(text), annotations) for text, annotations in train_data]\n",
        "\n",
        "# Prepare training data into SpaCy's Example format\n",
        "train_examples = []\n",
        "for text, annotations in train_data:\n",
        "    doc = nlpTC.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    train_examples.append(example)\n",
        "\n",
        "# Split data into training, validation, and testing sets\n",
        "train_examples, test_examples = train_test_split(train_examples, test_size=0.2, random_state=42)\n",
        "train_examples, val_examples = train_test_split(train_examples, test_size=0.25, random_state=42)  # 20% of the remaining data is used for validation\n",
        "\n",
        "# Print the split data to visualize each set\n",
        "print(\"TRAINING SET (60% of the data):\")\n",
        "for example in train_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "print(\"\\nVALIDATION SET (20% of the data):\")\n",
        "for example in val_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "print(\"\\nTESTING SET (20% of the data):\")\n",
        "for example in test_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "# Training the model with more iterations for small datasets\n",
        "n_iter = 10  # Set iterations\n",
        "optimizer = nlpTC.initialize()\n",
        "\n",
        "for i in range(n_iter):\n",
        "    losses = {}\n",
        "    for batch in spacy.util.minibatch(train_examples, size=2):  # Small batch size for small data\n",
        "        for example in batch:\n",
        "            nlpTC.update([example], sgd=optimizer, losses=losses)\n",
        "    print(f\"Iteration {i+1}/{n_iter} - Loss: {losses['textcat']}\")\n",
        "\n",
        "# Testing the model\n",
        "print(\"\\nSample Prediction Output with probabilities:\")\n",
        "doc = nlpTC(\"Claim your prize now!\")\n",
        "print(doc.cats)\n",
        "\n",
        "# Function to classify user input emails\n",
        "def classify_email(email):\n",
        "    email = preprocess_text(email)\n",
        "    doc = nlpTC(email)\n",
        "    spam_score = doc.cats['SPAM']\n",
        "    ham_score = doc.cats['HAM']\n",
        "\n",
        "    if spam_score > ham_score:\n",
        "        return \"SPAM\"\n",
        "    else:\n",
        "        return \"HAM\"\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score on the test set\n",
        "true_labels = [1 if example.reference.cats['SPAM'] == 1 else 0 for example in test_examples]\n",
        "predicted_labels = [1 if classify_email(example.reference.text) == 'SPAM' else 0 for example in test_examples]\n",
        "\n",
        "# Calculate and print metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nAccuracy: {accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")\n",
        "\n",
        "# Allow users to test the model by inputting their own data\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter a sample email for classification (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    classification = classify_email(user_input)\n",
        "    print(f\"The email is classified as: {classification}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hAOyBYHJ3H1"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is same as the code from subset 1 but the training data contains a total of 20 samples. A detailed preprocessing function is provided, which aims at lowering the case of text, scrapping special signs and stopwords from the text respectively which aims at improving the quality of the text before it is fed for training."
      ],
      "metadata": {
        "id": "HGfVYRli8z6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki-ebE3bJ4VH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db05ca42-3184-4fb4-85b3-8bd1cbaaf519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 16.0000%\n",
            "Recall: 100.0000%\n",
            "F1 Score: 27.5862%\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import re\n",
        "\n",
        "# Load a pre-trained NER model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample training data (text and true entity annotations)\n",
        "training_data = [\n",
        "    (\"Microsoft announced a new AI initiative in Seattle.\", [(0, 9, \"ORG\"), (39, 46, \"GPE\")]),\n",
        "    (\"Google I/O will take place in May 2023.\", [(0, 10, \"EVENT\"), (29, 37, \"DATE\")]),\n",
        "    (\"The unemployment rate in the U.S. dropped to 3.5%.\", [(34, 38, \"PERCENT\"), (27, 31, \"GPE\")]),\n",
        "    (\"The Chinese economy grew by 5% last year.\", [(4, 11, \"NORP\")]),\n",
        "    (\"Sundar Pichai is the CEO of Google.\", [(0, 13, \"PERSON\"), (28, 34, \"ORG\")]),\n",
        "    (\"Tesla secured $2 billion in new funding.\", [(14, 22, \"MONEY\")]),\n",
        "    (\"Amazon is opening a new office in Vancouver.\", [(0, 6, \"ORG\"), (36, 45, \"GPE\")]),\n",
        "    (\"Samsung released its new Galaxy S22 phone.\", [(0, 7, \"ORG\"), (23, 32, \"PRODUCT\")]),\n",
        "    (\"The Pacific Ocean is the largest body of water on Earth.\", [(4, 17, \"LOC\")]),\n",
        "    (\"The headquarters of IBM is in New York City.\", [(21, 24, \"ORG\"), (31, 44, \"GPE\")]),\n",
        "\n",
        "    (\"Satya Nadella leads Microsoft Corporation.\", [(0, 12, \"PERSON\"), (19, 38, \"ORG\")]),\n",
        "    (\"The FIFA World Cup will be held in Qatar in 2022.\", [(4, 18, \"EVENT\"), (34, 39, \"GPE\"), (43, 47, \"DATE\")]),\n",
        "    (\"Apple plans to invest $10 billion in manufacturing.\", [(23, 32, \"MONEY\")]),\n",
        "    (\"A new skyscraper is being built in Dubai.\", [(33, 38, \"GPE\")]),\n",
        "    (\"70% of the world's population is now online.\", [(0, 3, \"PERCENT\")]),\n",
        "    (\"Elon Musk founded SpaceX and Tesla.\", [(0, 9, \"PERSON\"), (17, 23, \"ORG\"), (28, 33, \"ORG\")]),\n",
        "    (\"The startup raised $50 million in Series B.\", [(15, 25, \"MONEY\")]),\n",
        "    (\"The next Apple event is scheduled for March 25th.\", [(9, 14, \"ORG\"), (39, 48, \"DATE\")]),\n",
        "    (\"The new company is aiming for a 15% market share.\", [(28, 31, \"PERCENT\")]),\n",
        "    (\"Apple's iPhone 14 is expected to launch in 2023.\", [(0, 5, \"ORG\"), (7, 15, \"PRODUCT\"), (46, 50, \"DATE\")]),\n",
        "\n",
        "]\n",
        "\n",
        "# Define a comprehensive preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Tokenization and stopword removal\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess training data\n",
        "preprocessed_data = [(preprocess_text(text), entities) for text, entities in training_data]\n",
        "\n",
        "# Initialize lists for storing true and predicted entities\n",
        "all_true_entities = []\n",
        "all_pred_entities = []\n",
        "\n",
        "# Iterate through preprocessed training data\n",
        "for text, true_entities in preprocessed_data:\n",
        "    # Run NER model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Predicted entities from the model\n",
        "    pred_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Store true and predicted entities for all examples\n",
        "    all_true_entities.extend([(ent[0], ent[1], ent[2]) for ent in true_entities])\n",
        "    all_pred_entities.extend([(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents])\n",
        "\n",
        "# Convert to binary classification (1 for correct entity, 0 for incorrect)\n",
        "y_true = [1 if ent in all_true_entities else 0 for ent in all_pred_entities]\n",
        "y_pred = [1 for _ in all_pred_entities]  # Assuming all predictions are correct for now\n",
        "\n",
        "# Calculate Precision, Recall, F1\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcVSPyJUJ6R0"
      },
      "source": [
        "## Part of Speech Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is same as the code from subset 1 but the training data contains a total of 20 samples. A detailed preprocessing function is provided, which aims at lowering the case of text, scrapping special signs and stopwords from the text respectively which aims at improving the quality of the text before it is fed for training."
      ],
      "metadata": {
        "id": "2Ldy4qqg810Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cqKnd3CJ9fH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92fdc350-2a5c-41d5-e991-2a6d9de16986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consolidated Metrics across Training, Validation, and Test Data:\n",
            "Accuracy: 13.3333%\n",
            "Precision: 7.8721%\n",
            "Recall: 13.3333%\n",
            "F1 Score: 9.6354%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "# Load spaCy's POS tagging model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample expanded training data: list of (text, true_pos_tags) pairs\n",
        "training_data = [\n",
        "    (\"She sells seashells by the seashore.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The quick brown fox jumps over the lazy dog.\", ['DET', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"I love coding in Python.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'PROPN']),\n",
        "    (\"Birds fly in the sky.\", ['NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"Alice and Bob went to the market.\", ['PROPN', 'CCONJ', 'PROPN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"Reading books is fun.\", ['VERB', 'NOUN', 'AUX', 'ADJ']),\n",
        "    (\"My car is very fast.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"We are going to the zoo.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"It is raining today.\", ['PRON', 'AUX', 'VERB', 'NOUN']),\n",
        "    (\"Programming languages are interesting.\", ['NOUN', 'NOUN', 'AUX', 'ADJ']),\n",
        "\n",
        "    (\"The cat sleeps on the mat.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"John likes to play soccer.\", ['PROPN', 'VERB', 'PART', 'VERB', 'NOUN']),\n",
        "    (\"She is learning French.\", ['PRON', 'AUX', 'VERB', 'PROPN']),\n",
        "    (\"The weather is nice today.\", ['DET', 'NOUN', 'AUX', 'ADJ', 'NOUN']),\n",
        "    (\"He bought a new laptop yesterday.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN']),\n",
        "    (\"They are swimming in the pool.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The pizza smells delicious.\", ['DET', 'NOUN', 'VERB', 'ADJ']),\n",
        "    (\"Can you help me with this project?\", ['AUX', 'PRON', 'VERB', 'PRON', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"This task is quite difficult.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"He enjoys reading books.\", ['PRON', 'VERB', 'VERB', 'NOUN']),\n",
        "]\n",
        "\n",
        "# Preprocess text: convert to lowercase and shuffle the training data\n",
        "training_data = [(text.lower(), tags) for text, tags in training_data]\n",
        "random.shuffle(training_data)\n",
        "\n",
        "# Split data into training, validation, and test sets (60% train, 20% validation, 20% test)\n",
        "train_data, temp_data = train_test_split(training_data, test_size=0.4, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Initialize lists to store true and predicted POS tags for all sets\n",
        "all_true_pos_tags_train, all_predicted_pos_tags_train = [], []\n",
        "all_true_pos_tags_val, all_predicted_pos_tags_val = [], []\n",
        "all_true_pos_tags_test, all_predicted_pos_tags_test = [], []\n",
        "\n",
        "# Function to preprocess and evaluate POS tagging\n",
        "def process_data(data, all_true_pos_tags, all_predicted_pos_tags):\n",
        "    for text, true_pos_tags in data:\n",
        "        # Special character removal\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        # Process the text with spaCy\n",
        "        doc = nlp(text)\n",
        "        # Extract predicted POS tags and filter out stopwords\n",
        "        predicted_pos_tags = [token.pos_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "        # Extend lists with true and predicted tags for evaluation\n",
        "        all_true_pos_tags.extend(true_pos_tags)\n",
        "        all_predicted_pos_tags.extend(predicted_pos_tags)\n",
        "\n",
        "# Process training, validation, and test data\n",
        "process_data(train_data, all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "process_data(val_data, all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "process_data(test_data, all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Ensure both lists are the same length to avoid ValueError\n",
        "def ensure_equal_length(true_tags, predicted_tags):\n",
        "    if len(true_tags) != len(predicted_tags):\n",
        "        min_length = min(len(true_tags), len(predicted_tags))\n",
        "        true_tags = true_tags[:min_length]\n",
        "        predicted_tags = predicted_tags[:min_length]\n",
        "    return true_tags, predicted_tags\n",
        "\n",
        "# Ensure correct lengths for all sets\n",
        "all_true_pos_tags_train, all_predicted_pos_tags_train = ensure_equal_length(all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "all_true_pos_tags_val, all_predicted_pos_tags_val = ensure_equal_length(all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "all_true_pos_tags_test, all_predicted_pos_tags_test = ensure_equal_length(all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Function to calculate metrics for a dataset\n",
        "def evaluate_metrics(true_tags, predicted_tags):\n",
        "    accuracy = accuracy_score(true_tags, predicted_tags)\n",
        "    precision = precision_score(true_tags, predicted_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, predicted_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, predicted_tags, average='weighted')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate on training, validation, and test sets\n",
        "metrics_train = evaluate_metrics(all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "metrics_val = evaluate_metrics(all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "metrics_test = evaluate_metrics(all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Combine all metrics into single print statement\n",
        "total_accuracy = (metrics_train[0] + metrics_val[0] + metrics_test[0]) / 3\n",
        "total_precision = (metrics_train[1] + metrics_val[1] + metrics_test[1]) / 3\n",
        "total_recall = (metrics_train[2] + metrics_val[2] + metrics_test[2]) / 3\n",
        "total_f1 = (metrics_train[3] + metrics_val[3] + metrics_test[3]) / 3\n",
        "\n",
        "# Print consolidated metrics\n",
        "print(\"Consolidated Metrics across Training, Validation, and Test Data:\")\n",
        "print(f\"Accuracy: {total_accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {total_precision * 100:.4f}%\")\n",
        "print(f\"Recall: {total_recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {total_f1 * 100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9uAyXdoJ_C3"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is same as the code from subset 1 but the training data contains a total of 20 samples. A detailed preprocessing function is provided, which aims at lowering the case of text, scrapping special signs and stopwords from the text respectively which aims at improving the quality of the text before it is fed for training."
      ],
      "metadata": {
        "id": "ODd8_uXn826f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9xewaDJKBHT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load a blank model and add text classifier\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGATIVE\")\n",
        "\n",
        "train_data = [\n",
        "    (\"I'm so frustrated with how slow my internet is.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I'm so happy with my new job!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The customer service at that store is excellent.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The movie was a complete waste of time.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That movie was truly heartwarming and beautiful.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been feeling really down lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The food at the new restaurant was absolutely delicious.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t get the job, and I feel so defeated.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The sunset this evening was breathtaking.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m really upset that I missed the deadline.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"I finally finished the book, and it was such a rewarding read.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"This weather is terrible, I can’t wait for it to end.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The surprise party was such a success!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My laptop crashed again, and I lost all my work.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The concert was absolutely mind-blowing!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been struggling with my workload lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The flowers you sent me are absolutely stunning.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I regret spending money on that product.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just found out I won the contest! I’m over the moon.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"Everything seems to be going wrong lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "]\n",
        "\n",
        "# Define a comprehensive preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = [token.text for token in nlpTC(text) if not token.is_stop]  # Directly extract tokens\n",
        "    # Join tokens back to a string\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text data from train_data\n",
        "text = [data[0] for data in train_data]\n",
        "labels = [data[1]['cats']['POSITIVE'] for data in train_data] # Extract labels\n",
        "\n",
        "# Vectorize text data using the extracted text list\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict sentiments\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")"
      ],
      "metadata": {
        "id": "sW2kZQWKdbse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a686e4-cd52-4d8a-abf4-f18996ff0472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 70.0000%\n",
            "Precision: 60.0000%\n",
            "Recall: 75.0000%\n",
            "F1 Score: 66.6667%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgHnq6XzKC61"
      },
      "source": [
        "## Text Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z9r9yZFKCo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1bb4fa-d288-4539-cbec-843c0fa1dde5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: Renewable energy sources such as solar and wind power offer hope, but their adoption has not been widespread enough to make a significant impact yet. \n",
            " As a result, glaciers are melting, sea levels are rising, and extreme weather events \n",
            "are becoming more frequent. Many governments around the world have pledged to reduce carbon emissions, but progress has been slow. \n",
            "\n",
            "Precision: 0.00%\n",
            "Recall: 0.00%\n",
            "F1 Score: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-148-ca68da43c171>:43: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  sentence_scores = [(sent, nlp(' '.join(preprocess_text(sent))).similarity(ref_doc)) for sent in sentences]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download the punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example text and reference summary\n",
        "text = \"\"\"Climate change is one of the most pressing issues of our time. The increasing levels of greenhouse gases in the atmosphere\n",
        "have led to rising global temperatures. As a result, glaciers are melting, sea levels are rising, and extreme weather events\n",
        "are becoming more frequent. Many governments around the world have pledged to reduce carbon emissions, but progress has been slow.\n",
        "Renewable energy sources such as solar and wind power offer hope, but their adoption has not been widespread enough to make a significant impact yet.\n",
        "Urgent action is needed to address this global crisis before it’s too late.\"\"\"\n",
        "\n",
        "reference_summary = \"\"\"Climate change is caused by greenhouse gases and is leading to rising temperatures and extreme weather.\n",
        "Renewable energy offers hope, but its adoption is slow.\"\"\"\n",
        "\n",
        "# Function to preprocess text: remove special characters, stopwords, and tokenize\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Keep only alphanumeric characters and spaces\n",
        "    doc = nlp(text.lower())  # Convert to lowercase and process with spaCy\n",
        "    # Remove stopwords and return tokens\n",
        "    tokens = [token.text for token in doc if not token.is_stop]\n",
        "    return tokens\n",
        "\n",
        "# Enhanced extractive summarization function\n",
        "def extractive_summary(text, reference_summary, num_sentences=3):\n",
        "    doc = nlp(text)  # Process the original text\n",
        "    sentences = [sent.text for sent in doc.sents]  # Extract original sentences\n",
        "    preprocessed_sentences = [preprocess_text(sent) for sent in sentences]  # Preprocess each sentence\n",
        "\n",
        "    # Score sentences based on similarity to the reference summary\n",
        "    ref_tokens = preprocess_text(reference_summary)  # Preprocess reference summary\n",
        "    ref_doc = nlp(' '.join(ref_tokens))  # Create a spaCy doc from the preprocessed tokens\n",
        "\n",
        "    # Score sentences based on similarity to the reference summary\n",
        "    sentence_scores = [(sent, nlp(' '.join(preprocess_text(sent))).similarity(ref_doc)) for sent in sentences]\n",
        "    ranked_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select top sentences\n",
        "    top_sentences = [sent[0] for sent in ranked_sentences[:num_sentences]]\n",
        "    return top_sentences\n",
        "\n",
        "# Tokenizing the reference and generated summaries into sentences\n",
        "generated_summary = extractive_summary(text, reference_summary)  # Summary in lowercase\n",
        "reference_sentences = [sent.lower() for sent in sent_tokenize(reference_summary)]  # Reference in lowercase\n",
        "\n",
        "# Convert to binary relevance: 1 if the sentence appears in the reference summary, 0 otherwise\n",
        "y_true = [1 if sent in reference_sentences else 0 for sent in sent_tokenize(text.lower())]\n",
        "y_pred = [1 if sent in generated_summary else 0 for sent in sent_tokenize(text.lower())]\n",
        "\n",
        "# Ensure y_true and y_pred are of the same length\n",
        "if len(y_true) != len(y_pred):\n",
        "    min_length = min(len(y_true), len(y_pred))\n",
        "    y_true = y_true[:min_length]\n",
        "    y_pred = y_pred[:min_length]\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred) * 100\n",
        "recall = recall_score(y_true, y_pred) * 100\n",
        "f1 = f1_score(y_true, y_pred) * 100\n",
        "\n",
        "# Output results\n",
        "print(f\"Generated Summary: {' '.join(generated_summary)}\")\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"Recall: {recall:.2f}%\")\n",
        "print(f\"F1 Score: {f1:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9YPMt1hGwa2"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVErSRgNKGzN"
      },
      "source": [
        "# **Subset 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRcquBeKKUUW"
      },
      "source": [
        "## Text Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is same as the code from subset 1 but the training data contains a total of 100 samples. A detailed preprocessing function is provided, which aims at lowering the case of text, scrapping special signs and stopwords from the text respectively which aims at improving the quality of the text before it is fed for training."
      ],
      "metadata": {
        "id": "41clgzsm846C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoGnYWwlKWQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1001101-809a-47c3-b428-582fc3f7772f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAINING SET (60% of the data):\n",
            "Text: dear user detected suspicious activity bank account prevent account suspended click link verify details wwwbanksecurecom failure 24 hours result account suspension - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: hello - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: discuss project - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: thank applying marketing manager position xyz company pleased invite interview monday october 5th 200 pm interview conducted zoom details sent soon - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: reschedule - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: m lemme know ready - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: quick reminder meeting tomorrow 1000 conference room bring project update documents - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: notes weekly team meeting held today review let know changes additions following action items week - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: hope wanted share quick update project status track complete phase end week ill schedule meeting monday discuss steps - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: wishing happy birthday hope fantastic day filled joy laughter cake lets catch soon - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: reminder family vacation coming soon flying florida 15th sure pack nt forget bring sunscreen camera - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: urgent mobile number awarded 2000 prize guaranteed 09058094455 land line claim 3030 valid 12hrs - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: invited annual block party saturday october 16th food games music 12 pm 6 pm bring family lets fun - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: travel itinerary confirmed scheduled depart new york flight 5678 900 october 20th return flight london flight 6789 500 pm october 27th - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: sleeping surfing - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: catch bus   frying egg   tea eating moms left dinner   feel love - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: winner valued network customer selected receivea 900 prize reward claim 09061701461 claim code kl341 valid 12 hours - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: waiting machan free - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: cool text ready - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: valued customer pleased advise following recent review mob awarded 1500 bonus prize 09066364589 - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: claim free 1999 bonus deposit download rg777 app 188p bonus install httpsbitly3tn80nc enjoy rewards - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: free message activate 500 free text messages replying message word free terms   conditions visit www07781482378com - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: welcome weekly newsletter week sharing tips improve productivity stay organized sure check latest articles join upcoming webinar time management - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text:  - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: update account details - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: thanks hope good day today - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: sent type food like - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: exclusive offer - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: hope wanted let know submitted final report course confirm receive - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: reminder upcoming dentist appointment monday october 18th 930 contact need reschedule - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: great news preapproved personal loan 10000 lowinterest rate credit check required apply today wwwgetmymoneycom instant cash - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: gcash account verification needed suspicious transaction kindly visit gcaresprotectphli continue services - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: invoice attached - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: thanks lot wishes birthday thanks making birthday truly memorable - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: reservation oceanview resort confirmed october 10th october 15th look forward welcoming need assistance special requests feel free contact - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: search i88j1l1 mag l0gin free b0nus p8888 promo code 2mqs0cs live n0wclaim unlimited b0nus n0w d0nt miss 0ut limited days 0nly - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: hope message finds wanted follow application software engineer position m interested role appreciate updates hiring process - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: nt worry guess s busy - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: nt pick phone right pls send message - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: congratulations ve selected - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: finally match heading draw prediction - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: problem - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: urgent won 1 week free membership 100000 prize jackpot txt word claim 81010 tc wwwdbuknet lccltd pobox 4403ldnw1a7rw18 - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: wanted let know arrived safely cabin weather beautiful planning hiking tomorrow ill send pictures later - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: invited annual company holiday party join december 15th 700 pm evening food drinks fun rsvp december 1st - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: good stuff - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: find attached agenda tomorrows meeting discussing q4 sales targets marketing strategy new product launch let know points add - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: peryagame 5 cashback unlimited bonus peryagame offer guaranteed highest daily rebates ph check promos httpsperyagamecom - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: important meeting agenda - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: hurry 90 discount products onetime offer visit wwwsuperdealscom use code save90 checkout nt miss amazing opportunity - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: wanted quick update project ve completed design phase moving development week let know questions need clarification tasks - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: youwhen wil reach - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: aight ill hit cash - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: reminder book borrowed catcher rye return october 7th return renew avoid late fees - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: sorry ill later - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: busy trying finish new year looking forward finally meeting - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: ve booked tickets trip hawaii flying 12th coming 18th let know interested joining usits going great trip - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: appointment dr smith confirmed thursday october 8th 1000 need reschedule contact 24 hours advance - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: note lecture scheduled tuesday october 12th moved thursday october 14th 3 pm classroom remains apologize inconvenience - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: accidentally deleted message resend - Label: {'SPAM': 0, 'HAM': 1}\n",
            "\n",
            "VALIDATION SET (20% of the data):\n",
            "Text: nt text anymore - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: want grab lunch today m thinking trying new italian place near office let know - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: free iphone today - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: claim free prize - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: customer service representative freephone 0808 145 4742 9am11pm won guaranteed 1000 cash 5000 prize - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: thank placing order order 78965 confirmed currently processed receive shipping notification dispatched - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: package shipped - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: planning family gathering grandmas house sunday lunch 1 pm ll great catch let know - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: quick reminder homework assignment chapter 3 friday sure review key concepts submit work time - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: quick reminder office potluck friday nt forget bring dish share colleagues looking forward seeing everyones culinary creations - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: hi selected allexpensepaid trip bahamas claim free vacation need fill quick survey click wwwfreevacationcom - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: lose 20 pounds 2 weeks miracle weight loss pills 100 natural safe order today special discount wwwweightlosspillscom hurry offer expires soon - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: won million dollars - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: hope enjoying recent purchase store d love hear feedback moment complete brief survey let know - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: congrats 1 year special cinema pass 2 09061209465 c suprman v matrix3 starwars3 etc 4 free bx420ip45we 150pm nt miss - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: dinner tonight ve reservation 730 pm new thai place downtown let know changes - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: book club meeting scheduled tuesday october 6th 600 pm discussing alchemist paulo coelho sure finish reading meeting bring thoughts questions discussion - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: congratulations selected winner 1000000 prize click claim reward wwwclaimprizecom act fast offer expires soon - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: guaranteed highest daily rebates ph bet daily games 08 unlimited bonus check promos httpsperyagamecom - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: thank recent purchase attached invoice order 45678 items shipped 35 business days - Label: {'SPAM': 0, 'HAM': 1}\n",
            "\n",
            "TESTING SET (20% of the data):\n",
            "Text: thanks sending initial draft ve changes document find updated version attached let know questions - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: hope s result consistently intelligent kind start asking practicum links ears open best ttyl - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: writing inform new software update available device version 43 includes bug fixes security enhancements new features update software earliest convenience ensure optimal performance - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: day great day - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: says hell friends got money s definitely buying end week - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: awesome ill bit - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: place man - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: reminder rsvp wedding november 12th excited celebrate special day family friends let know ll attending october 1st - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: limited time deal buy - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: spam - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: hello s saturday texting d decided tomo m trying invite - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: time coming later - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: d like invite upcoming webinar building effective remote teams session place september 30th 1100 ll learn tips strategies managing remote employees improving team collaboration - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: tired nt slept past nights - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: wanted follow meeting chance review proposal sent d love hear thoughts discuss steps - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: meeting 10 tomorrow - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: earn 5000 week working home experience needed start today money simply filling surveys click learn wwwearnmoneyathomecom - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: heads meeting library friday 300 pm review final exam ill bring notes key chapters let know time works - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Text: win trip hawaii - Label: {'SPAM': 1, 'HAM': 0}\n",
            "Text: sounds great home - Label: {'SPAM': 0, 'HAM': 1}\n",
            "Iteration 1/10 - Loss: 11.756702371581923\n",
            "Iteration 2/10 - Loss: 1.4257678190240881\n",
            "Iteration 3/10 - Loss: 0.0006297169221989307\n",
            "Iteration 4/10 - Loss: 1.5428859224009273e-05\n",
            "Iteration 5/10 - Loss: 6.957668792773042e-06\n",
            "Iteration 6/10 - Loss: 3.874358329492011e-06\n",
            "Iteration 7/10 - Loss: 2.3818521615037724e-06\n",
            "Iteration 8/10 - Loss: 1.5661585665571742e-06\n",
            "Iteration 9/10 - Loss: 1.0804656441210092e-06\n",
            "Iteration 10/10 - Loss: 7.746271382946812e-07\n",
            "\n",
            "Sample Prediction Output with probabilities:\n",
            "{'SPAM': 0.6613892316818237, 'HAM': 0.33861076831817627}\n",
            "\n",
            "Accuracy: 85.0000%\n",
            "Precision: 87.3684%\n",
            "Recall: 85.0000%\n",
            "F1 Score: 81.1429%\n",
            "\n",
            "Enter a sample email for classification (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.training import Example\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "# Create a blank SpaCy model and add the text classifier component\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"SPAM\")\n",
        "textcat.add_label(\"HAM\")\n",
        "\n",
        "#Example Training data\n",
        "train_data = [\n",
        "    (\"This is spam\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello, how are you?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You won a million dollars!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Claim your free prize now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Meeting at 10 AM tomorrow\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your invoice is attached\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Exclusive offer just for you!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Get a free iPhone today\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we reschedule our call?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Update your account details\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Limited time deal, buy now!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Your package has been shipped\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Win a trip to Hawaii now\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Important meeting agenda\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You've been selected\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Can we discuss this project?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I am waiting machan. Call me once you free.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Sorry, I'll call later\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"You will be in the place of that man\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now! C Suprman V, Matrix3, StarWars3, etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Please don't text me anymore. I have nothing else to say.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Thanks a lot for your wishes on my birthday. Thanks you for making my birthday truly memorable.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Aight, I'll hit you up when I get some cash\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Dont worry. I guess he's busy.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a £1500 Bonus Prize, call 09066364589\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Good stuff, will do.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"What time you coming down later?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Sounds great! Are you home now?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Finally the match heading towards draw as your prediction.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Tired. I haven't slept well the past few nights.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Where are you?when wil you reach here?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Please call our customer service representative on FREEPHONE 0808 145 4742 between 9am-11pm as you have WON a guaranteed £1000 cash or £5000 prize!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"What you doing? how are you?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I'm back, lemme know when you're ready\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Lose 20 pounds in just 2 weeks with our miracle weight loss pills! 100% natural and safe. Order today and get a special discount: www.weightlosspills.com. Hurry, offer expires soon!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Awesome, I'll see you in a bit\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Just sent it. So what type of food do you like?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I accidentally deleted the message. Resend please.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"FREE MESSAGE Activate your 500 FREE Text Messages by replying to this message with the word FREE For terms & conditions, visit www.07781482378.com\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"I cant pick the phone right now. Pls send a message\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"He says he'll give me a call when his friend's got the money but that he's definitely buying before the end of the week\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You made my day. Do have a great day too.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Great news! You have been pre-approved for a personal loan of $10,000 with a low-interest rate! No credit check required. Apply today at www.getmymoney.com and get instant cash!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"No problem. How are you doing?\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Just sleeping and surfing\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Busy here. Trying to finish for new year. I am looking forward to finally meeting you\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Cool, text me when you're ready\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"URGENT! Your Mobile number has been awarded with a £2000 prize GUARANTEED. Call 09058094455 from land line. Claim 3030. Valid 12hrs only\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Thanks for this hope you had a good day today\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I hope you that's the result of being consistently intelligent and kind. Start asking him about practicum links and keep your ears open and all the best. ttyl\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Just a quick reminder about our meeting tomorrow at 10:00 AM in the conference room. Please bring the project update documents with you.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Do you want to grab lunch today? I’m thinking of trying that new Italian place near the office. Let me know if you’re up for it!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Thank you for your recent purchase! Attached is the invoice for your order #45678. Your items will be shipped within 3-5 business days.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Guaranteed Highest Daily Rebates in the PH! Bet daily on any games and get up to 0.8% with UNLIMITED bonus! Check out more promos now: https://peryagame.com\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"I wanted to give you a quick update on the project. We’ve completed the design phase and will be moving into development next week. Let me know if you have any questions or need clarification on your tasks.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You’re invited to our annual company holiday party! Join us on December 15th at 7:00 PM for an evening of food, drinks, and fun. Please RSVP by December 1st.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Just wanted to let you know that we arrived safely at the cabin. The weather is beautiful, and we’re planning to go hiking tomorrow. I’ll send you some pictures later.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I hope this message finds you well. I wanted to follow up on my application for the software engineer position. I’m very interested in the role and would appreciate any updates on the hiring process.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Thank you for placing an order with us! Your order #78965 has been confirmed and is currently being processed. You will receive a shipping notification once it has been dispatched.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I’ve booked my tickets for the trip to Hawaii! We’re flying out on the 12th and coming back on the 18th. Let me know if you’re still interested in joining us—it’s going to be a great trip!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"PeryaGame 5% Cashback with Unlimited Bonus! PeryaGame offer Guaranteed Highest Daily Rebates in the PH! Check out more promos now: https://peryagame.com\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Welcome to our weekly newsletter! This week, we’re sharing tips on how to improve productivity and stay organized. Be sure to check out our latest articles and join our upcoming webinar on time management.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I hope you’re all doing well. I wanted to share a quick update on the project status. We’re on track to complete the next phase by the end of the week. I’ll schedule a meeting for next Monday to discuss the next steps.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Are we still on for dinner tonight? I’ve made a reservation at 7:30 PM at the new Thai place downtown. Let me know if anything changes.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Thank you for applying for the Marketing Manager position at XYZ Company. We are pleased to invite you for an interview on Monday, October 5th, at 2:00 PM. The interview will be conducted via Zoom, and the details will be sent to you soon.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Just a quick reminder that your homework assignment on Chapter 3 is due by Friday. Make sure to review the key concepts and submit your work on time.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"We are writing to inform you that a new software update is available for your device. Version 4.3 includes bug fixes, security enhancements, and new features. Please update your software at your earliest convenience to ensure optimal performance.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Claim your free 1999 bonus without deposit! Download the RG777 APP now for an 188P bonus. Install here: https://bit.ly/3Tn80Nc. Enjoy your rewards!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Dear User, we have detected suspicious activity in your bank account. To prevent your account from being suspended, please click on the link below and verify your details: www.banksecure.com. Failure to do so within 24 hours will result in account suspension.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"We’d like to invite you to our upcoming webinar on 'Building Effective Remote Teams.' The session will take place on September 30th at 11:00 AM. You’ll learn tips and strategies for managing remote employees and improving team collaboration.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your appointment with Dr. Smith has been confirmed for Thursday, October 8th, at 10:00 AM. If you need to reschedule, please contact us at least 24 hours in advance.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Just a reminder that our family vacation is coming up soon! We’ll be flying to Florida on the 15th, so make sure to pack everything by then. Also, don’t forget to bring sunscreen and your camera!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Earn $5,000 per week working from home! No experience needed. Start today and make money by simply filling out surveys. Click here to learn more: www.earnmoneyathome.com.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Just a heads up, we’re meeting at the library on Friday at 3:00 PM to review for the final exam. I’ll bring my notes, and we can go over the key chapters together. Let me know if that time works for everyone.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"We hope you’re enjoying your recent purchase from our store. We’d love to hear your feedback! Please take a moment to complete our brief survey, and let us know how we did.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your travel itinerary has been confirmed. You are scheduled to depart from New York on Flight 5678 at 9:00 AM on October 20th. Your return flight from London will be on Flight 6789 at 5:00 PM on October 27th.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"Just a reminder to RSVP for our wedding on November 12th! We’re so excited to celebrate this special day with our family and friends. Please let us know if you’ll be attending by October 1st.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Search I88J1L1! Mag l0gin Get FREE B0nus P8888! PROMO CODE: 2MQS0CS live N0w!Claim unlimited B0nus N0w D0nt Miss 0ut Limited Days 0nly\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Our next book club meeting is scheduled for Tuesday, October 6th, at 6:00 PM. We’ll be discussing “The Alchemist” by Paulo Coelho. Make sure to finish reading it before the meeting, and bring your thoughts and questions for the discussion.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Thanks for sending over the initial draft. I’ve made a few changes to the document, and you can find the updated version attached. Let me know if you have any questions.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Congratulations! You have been selected as the winner of our $1,000,000 prize! Click here to claim your reward now: www.claimprize.com. Act fast! Offer expires soon.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Just a quick reminder about the office potluck on Friday! Don’t forget to bring a dish to share with your colleagues. Looking forward to seeing everyone’s culinary creations!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"I hope you are doing well. I wanted to let you know that I have submitted my final report for the course. Please confirm when you receive it.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Please find attached the agenda for tomorrow’s meeting. We’ll be discussing the Q4 sales targets and the marketing strategy for the new product launch. Let me know if you have any points to add.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Hurry! Get a 90% discount on all our products! This is a one-time offer just for you! Visit www.superdeals.com and use code SAVE90 at checkout. Don't miss out on this amazing opportunity!\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Wishing you a very happy birthday! I hope you have a fantastic day filled with joy, laughter, and cake! Let’s catch up soon.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "\n",
        "    (\"I just wanted to follow up on our last meeting. Have you had a chance to review the proposal we sent over? We’d love to hear your thoughts and discuss the next steps.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"We’re planning a family gathering at Grandma’s house next Sunday. We’ll have lunch around 1 PM, and it’ll be great to catch up with everyone. Let me know if you can make it!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"GCash: Account verification needed due to suspicious transaction. Kindly Visit: gcares-protect-ph.li to continue using our services\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Please note that the lecture scheduled for Tuesday, October 12th, has been moved to Thursday, October 14th, at 3 PM. The classroom remains the same. I apologize for any inconvenience.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"This is a reminder of your upcoming dentist appointment on Monday, October 18th, at 9:30 AM. Please contact us if you need to reschedule.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"This is a reminder that the book you borrowed, “The Catcher in the Rye,” is due for return on October 7th. Please return or renew it by then to avoid any late fees.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"You’re all invited to our annual block party on Saturday, October 16th! We’ll have food, games, and music from 12 PM to 6 PM. Bring your family, and let’s have some fun!\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Hi there! You have been selected for an all-expense-paid trip to the Bahamas! To claim your FREE vacation, all you need to do is fill out a quick survey. Click here now: www.freevacation.com.\", {\"cats\": {\"SPAM\": 1, \"HAM\": 0}}),\n",
        "    (\"Here are the notes from our weekly team meeting held today. Please review and let me know if there are any changes or additions. We’ll be following up on these action items next week.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}}),\n",
        "    (\"Your reservation at Oceanview Resort has been confirmed for October 10th to October 15th. We look forward to welcoming you. If you need assistance or have special requests, feel free to contact us.\", {\"cats\": {\"SPAM\": 0, \"HAM\": 1}})\n",
        "]\n",
        "\n",
        "# Prepare training data into SpaCy's Example format\n",
        "train_examples = []\n",
        "for text, annotations in train_data:\n",
        "    doc = nlpTC.make_doc(text) # create doc prior to preprocessing\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    train_examples.append(example)\n",
        "\n",
        "# Initialize the textcat component with the training examples\n",
        "nlpTC.initialize(lambda: train_examples)\n",
        "\n",
        "# Define a comprehensive preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = [token.text for token in nlpTC(text) if not token.is_stop]  # Directly extract tokens\n",
        "    # Join tokens back to a string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply minimal text preprocessing to train_examples after initializing the pipeline\n",
        "train_examples_preprocessed = []\n",
        "for example in train_examples:\n",
        "    # Process the text and create a new Doc object\n",
        "    preprocessed_text = preprocess_text(example.reference.text)\n",
        "    preprocessed_doc = nlpTC.make_doc(preprocessed_text)\n",
        "\n",
        "    # Create a new Example with the preprocessed Doc and original annotations\n",
        "    # Instead of example.reference.cats, use Example.from_dict with manual setting of cats\n",
        "    new_example = Example.from_dict(preprocessed_doc, {\"cats\": example.reference.cats})\n",
        "    train_examples_preprocessed.append(new_example)\n",
        "\n",
        "train_examples = train_examples_preprocessed # replace with preprocessed examples\n",
        "# Apply the comprehensive text preprocessing\n",
        "train_data = [(preprocess_text(text), annotations) for text, annotations in train_data]\n",
        "\n",
        "# Prepare training data into SpaCy's Example format\n",
        "train_examples = []\n",
        "for text, annotations in train_data:\n",
        "    doc = nlpTC.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    train_examples.append(example)\n",
        "\n",
        "# Split data into training, validation, and testing sets\n",
        "train_examples, test_examples = train_test_split(train_examples, test_size=0.2, random_state=42)\n",
        "train_examples, val_examples = train_test_split(train_examples, test_size=0.25, random_state=42)  # 20% of the remaining data is used for validation\n",
        "\n",
        "# Print the split data to visualize each set\n",
        "print(\"TRAINING SET (60% of the data):\")\n",
        "for example in train_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "print(\"\\nVALIDATION SET (20% of the data):\")\n",
        "for example in val_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "print(\"\\nTESTING SET (20% of the data):\")\n",
        "for example in test_examples:\n",
        "    print(f\"Text: {example.reference.text} - Label: {example.reference.cats}\")\n",
        "\n",
        "# Training the model with more iterations for small datasets\n",
        "n_iter = 10  # Set iterations\n",
        "optimizer = nlpTC.initialize()\n",
        "\n",
        "for i in range(n_iter):\n",
        "    losses = {}\n",
        "    for batch in spacy.util.minibatch(train_examples, size=2):  # Small batch size for small data\n",
        "        for example in batch:\n",
        "            nlpTC.update([example], sgd=optimizer, losses=losses)\n",
        "    print(f\"Iteration {i+1}/{n_iter} - Loss: {losses['textcat']}\")\n",
        "\n",
        "# Testing the model\n",
        "print(\"\\nSample Prediction Output with probabilities:\")\n",
        "doc = nlpTC(\"Claim your prize now!\")\n",
        "print(doc.cats)\n",
        "\n",
        "# Function to classify user input emails\n",
        "def classify_email(email):\n",
        "    email = preprocess_text(email)\n",
        "    doc = nlpTC(email)\n",
        "    spam_score = doc.cats['SPAM']\n",
        "    ham_score = doc.cats['HAM']\n",
        "\n",
        "    if spam_score > ham_score:\n",
        "        return \"SPAM\"\n",
        "    else:\n",
        "        return \"HAM\"\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1 score on the test set\n",
        "true_labels = [1 if example.reference.cats['SPAM'] == 1 else 0 for example in test_examples]\n",
        "predicted_labels = [1 if classify_email(example.reference.text) == 'SPAM' else 0 for example in test_examples]\n",
        "\n",
        "# Calculate and print metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nAccuracy: {accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")\n",
        "\n",
        "# Allow users to test the model by inputting their own data\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter a sample email for classification (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    classification = classify_email(user_input)\n",
        "    print(f\"The email is classified as: {classification}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWAACAw-KXnB"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is same as the code from subset 1 but the training data contains a total of 100 samples. A detailed preprocessing function is provided, which aims at lowering the case of text, scrapping special signs and stopwords from the text respectively which aims at improving the quality of the text before it is fed for training."
      ],
      "metadata": {
        "id": "M1YC875c9LTq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEwnhnunKZP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1cbb863-2500-47b7-f531-4db3fdfc27fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Precision: 35.3933%\n",
            "Recall: 100.0000%\n",
            "F1 Score: 52.2822%\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load a pre-trained NER model\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Sample training data (text and true entity annotations)\n",
        "training_data = [\n",
        "    (\"Microsoft announced a new AI initiative in Seattle.\", [(0, 9, \"ORG\"), (39, 46, \"GPE\")]),\n",
        "    (\"Google I/O will take place in May 2023.\", [(0, 10, \"EVENT\"), (29, 37, \"DATE\")]),\n",
        "    (\"The unemployment rate in the U.S. dropped to 3.5%.\", [(34, 38, \"PERCENT\"), (27, 31, \"GPE\")]),\n",
        "    (\"The Chinese economy grew by 5% last year.\", [(4, 11, \"NORP\")]),\n",
        "    (\"Sundar Pichai is the CEO of Google.\", [(0, 13, \"PERSON\"), (28, 34, \"ORG\")]),\n",
        "    (\"Tesla secured $2 billion in new funding.\", [(14, 22, \"MONEY\")]),\n",
        "    (\"Amazon is opening a new office in Vancouver.\", [(0, 6, \"ORG\"), (36, 45, \"GPE\")]),\n",
        "    (\"Samsung released its new Galaxy S22 phone.\", [(0, 7, \"ORG\"), (23, 32, \"PRODUCT\")]),\n",
        "    (\"The Pacific Ocean is the largest body of water on Earth.\", [(4, 17, \"LOC\")]),\n",
        "    (\"The headquarters of IBM is in New York City.\", [(21, 24, \"ORG\"), (31, 44, \"GPE\")]),\n",
        "\n",
        "    (\"Satya Nadella leads Microsoft Corporation.\", [(0, 12, \"PERSON\"), (19, 38, \"ORG\")]),\n",
        "    (\"The FIFA World Cup will be held in Qatar in 2022.\", [(4, 18, \"EVENT\"), (34, 39, \"GPE\"), (43, 47, \"DATE\")]),\n",
        "    (\"Apple plans to invest $10 billion in manufacturing.\", [(23, 32, \"MONEY\")]),\n",
        "    (\"A new skyscraper is being built in Dubai.\", [(33, 38, \"GPE\")]),\n",
        "    (\"70% of the world's population is now online.\", [(0, 3, \"PERCENT\")]),\n",
        "    (\"Elon Musk founded SpaceX and Tesla.\", [(0, 9, \"PERSON\"), (17, 23, \"ORG\"), (28, 33, \"ORG\")]),\n",
        "    (\"The startup raised $50 million in Series B.\", [(15, 25, \"MONEY\")]),\n",
        "    (\"The next Apple event is scheduled for March 25th.\", [(9, 14, \"ORG\"), (39, 48, \"DATE\")]),\n",
        "    (\"The new company is aiming for a 15% market share.\", [(28, 31, \"PERCENT\")]),\n",
        "    (\"Apple's iPhone 14 is expected to launch in 2023.\", [(0, 5, \"ORG\"), (7, 15, \"PRODUCT\"), (46, 50, \"DATE\")]),\n",
        "\n",
        "    (\"A German scientist won the Nobel Prize.\", [(2, 8, \"NORP\")]),\n",
        "    (\"Facebook plans to launch new features in June.\", [(0, 7, \"ORG\"), (30, 35, \"DATE\")]),\n",
        "    (\"The CEO of Apple, Tim Cook, announced new products.\", [(14, 22, \"PERSON\"), (4, 9, \"ORG\")]),\n",
        "    (\"NASA's Perseverance rover landed on Mars.\", [(0, 4, \"ORG\"), (34, 38, \"GPE\")]),\n",
        "    (\"The 2024 Summer Olympics will take place in Paris.\", [(4, 24, \"EVENT\"), (40, 45, \"GPE\")]),\n",
        "    (\"The inflation rate reached 8.6% last month.\", [(28, 32, \"PERCENT\")]),\n",
        "    (\"Coca-Cola launched a new flavor this spring.\", [(0, 10, \"ORG\"), (36, 41, \"DATE\")]),\n",
        "    (\"The World Health Organization declared a health emergency.\", [(4, 30, \"ORG\")]),\n",
        "    (\"Berkshire Hathaway's stock price increased by $500.\", [(0, 23, \"ORG\"), (37, 40, \"MONEY\")]),\n",
        "    (\"In 2020, remote work became the new normal.\", [(3, 7, \"DATE\")]),\n",
        "\n",
        "    (\"Mark Zuckerberg met with world leaders to discuss technology.\", [(0, 15, \"PERSON\")]),\n",
        "    (\"The Great Wall of China is a popular tourist attraction.\", [(4, 20, \"LOC\")]),\n",
        "    (\"The Grammy Awards will be held in Los Angeles.\", [(0, 14, \"EVENT\"), (30, 43, \"GPE\")]),\n",
        "    (\"Intel announced a new chip that will improve processing speed.\", [(0, 5, \"ORG\")]),\n",
        "    (\"The stock market saw a decline of 4% today.\", [(28, 31, \"PERCENT\")]),\n",
        "    (\"Microsoft is acquiring LinkedIn for $26.2 billion.\", [(0, 9, \"ORG\"), (26, 39, \"ORG\"), (44, 57, \"MONEY\")]),\n",
        "    (\"SpaceX plans to launch its Starship rocket next year.\", [(0, 6, \"ORG\"), (34, 39, \"DATE\")]),\n",
        "    (\"The next big tech conference is set for September.\", [(9, 13, \"EVENT\"), (38, 47, \"DATE\")]),\n",
        "    (\"The United Nations addresses global challenges.\", [(4, 17, \"ORG\")]),\n",
        "    (\"Bill Gates founded Microsoft in 1975.\", [(0, 10, \"PERSON\"), (21, 29, \"ORG\"), (32, 36, \"DATE\")]),\n",
        "\n",
        "    (\"A recent study showed that 60% of students prefer online classes.\", [(36, 38, \"PERCENT\")]),\n",
        "    (\"The Louvre Museum is located in Paris.\", [(4, 22, \"ORG\"), (30, 35, \"GPE\")]),\n",
        "    (\"The 2022 World Cup will be hosted in Qatar.\", [(4, 18, \"EVENT\"), (35, 40, \"GPE\")]),\n",
        "    (\"Netflix added 8 million new subscribers in 2021.\", [(7, 14, \"ORG\"), (23, 24, \"MONEY\"), (29, 33, \"DATE\")]),\n",
        "    (\"The first electric car was launched by Tesla in 2008.\", [(29, 34, \"ORG\"), (39, 43, \"DATE\")]),\n",
        "    (\"Researchers found a new species of frog in Madagascar.\", [(36, 49, \"LOC\")]),\n",
        "    (\"In 2019, the world saw significant advancements in AI.\", [(3, 7, \"DATE\")]),\n",
        "    (\"The White House issued a statement regarding climate change.\", [(4, 15, \"GPE\")]),\n",
        "    (\"Elon Musk is the founder of SpaceX and Tesla.\", [(0, 9, \"PERSON\"), (23, 29, \"ORG\"), (34, 39, \"ORG\")]),\n",
        "    (\"Tesla plans to produce 20 million cars by 2030.\", [(0, 5, \"ORG\"), (34, 40, \"PERCENT\"), (44, 48, \"DATE\")]),\n",
        "\n",
        "    (\"The next FIFA World Cup will be in 2026.\", [(9, 13, \"EVENT\"), (25, 29, \"DATE\")]),\n",
        "    (\"Apple's market share reached an all-time high.\", [(0, 5, \"ORG\"), (27, 35, \"PERCENT\")]),\n",
        "    (\"Amazon Prime Video will launch new shows this fall.\", [(0, 6, \"ORG\"), (36, 41, \"DATE\")]),\n",
        "    (\"Google's headquarters is in Mountain View.\", [(0, 6, \"ORG\"), (29, 32, \"GPE\")]),\n",
        "    (\"Facebook was founded by Mark Zuckerberg.\", [(0, 8, \"ORG\"), (22, 36, \"PERSON\")]),\n",
        "    (\"The United Kingdom is hosting the G7 summit.\", [(4, 17, \"GPE\"), (31, 35, \"EVENT\")]),\n",
        "    (\"Sony released the PlayStation 5 in late 2020.\", [(0, 4, \"ORG\"), (16, 30, \"PRODUCT\"), (34, 38, \"DATE\")]),\n",
        "    (\"The next lunar eclipse will be on November 8th.\", [(9, 14, \"EVENT\"), (27, 34, \"DATE\")]),\n",
        "    (\"Elon Musk is developing a new satellite internet service.\", [(0, 9, \"PERSON\"), (30, 40, \"PRODUCT\")]),\n",
        "    (\"The Amazon rainforest is crucial for biodiversity.\", [(4, 10, \"LOC\")]),\n",
        "\n",
        "    (\"Toyota unveiled its electric car lineup this year.\", [(0, 6, \"ORG\"), (39, 43, \"DATE\")]),\n",
        "    (\"The Summer Olympics will take place in Tokyo in 2021.\", [(4, 24, \"EVENT\"), (38, 43, \"GPE\"), (46, 50, \"DATE\")]),\n",
        "    (\"The Eiffel Tower is one of the most visited monuments.\", [(4, 15, \"LOC\")]),\n",
        "    (\"NASA's Artemis program aims to return humans to the Moon.\", [(0, 4, \"ORG\"), (35, 50, \"EVENT\")]),\n",
        "    (\"The stock market experienced a significant downturn.\", [(4, 9, \"LOC\")]),\n",
        "    (\"Gold prices surged to an all-time high this week.\", [(0, 4, \"MONEY\"), (25, 35, \"DATE\")]),\n",
        "    (\"The Met Gala is a major fundraising event.\", [(4, 12, \"EVENT\")]),\n",
        "    (\"The Berlin Wall fell in 1989.\", [(4, 15, \"LOC\"), (19, 23, \"DATE\")]),\n",
        "    (\"Instagram was acquired by Facebook in 2012.\", [(0, 9, \"ORG\"), (23, 30, \"ORG\"), (34, 38, \"DATE\")]),\n",
        "    (\"Microsoft will invest in renewable energy projects.\", [(0, 9, \"ORG\")]),\n",
        "\n",
        "    (\"The World Cup is set to take place in Qatar.\", [(4, 10, \"EVENT\"), (26, 32, \"GPE\")]),\n",
        "    (\"The Great Barrier Reef is located off the coast of Australia.\", [(4, 21, \"LOC\")]),\n",
        "    (\"Bill Gates and Melinda Gates announced their divorce.\", [(0, 10, \"PERSON\"), (15, 27, \"PERSON\")]),\n",
        "    (\"The 2024 presidential election will be highly competitive.\", [(4, 36, \"EVENT\"), (40, 50, \"DATE\")]),\n",
        "    (\"SpaceX's Falcon Heavy launched successfully last year.\", [(0, 6, \"ORG\"), (7, 17, \"PRODUCT\"), (36, 41, \"DATE\")]),\n",
        "    (\"The new iPhone model features advanced camera technology.\", [(4, 9, \"ORG\"), (20, 27, \"PRODUCT\")]),\n",
        "    (\"Alibaba's revenue soared during the pandemic.\", [(0, 7, \"ORG\")]),\n",
        "    (\"The Cannes Film Festival is a prestigious event.\", [(4, 27, \"EVENT\")]),\n",
        "    (\"The Tesla Model 3 has become very popular.\", [(0, 5, \"ORG\"), (10, 22, \"PRODUCT\")]),\n",
        "    (\"Virtual reality is gaining traction in gaming.\", [(0, 7, \"LOC\")]),\n",
        "\n",
        "    (\"The COVID-19 vaccine rollout has accelerated globally.\", [(4, 12, \"EVENT\")]),\n",
        "    (\"Google's Android operating system dominates the market.\", [(0, 6, \"ORG\")]),\n",
        "    (\"The Nobel Peace Prize was awarded to Malala Yousafzai.\", [(4, 28, \"EVENT\"), (33, 50, \"PERSON\")]),\n",
        "    (\"The tech industry is evolving rapidly with AI advancements.\", [(4, 12, \"LOC\")]),\n",
        "    (\"Elon Musk plans to send humans to Mars.\", [(0, 9, \"PERSON\"), (23, 27, \"GPE\")]),\n",
        "    (\"The 2021 Tokyo Olympics faced many challenges.\", [(4, 25, \"EVENT\"), (31, 36, \"DATE\")]),\n",
        "    (\"The British Royal Family attended the funeral of Prince Philip.\", [(4, 31, \"GPE\"), (39, 50, \"PERSON\")]),\n",
        "    (\"Netflix is producing a new documentary series.\", [(0, 7, \"ORG\")]),\n",
        "    (\"The Paris Agreement addresses climate change issues.\", [(4, 18, \"EVENT\")]),\n",
        "    (\"The Olympic Games in Paris are highly anticipated.\", [(4, 20, \"EVENT\"), (26, 31, \"GPE\")]),\n",
        "\n",
        "    (\"The smartphone market is becoming saturated.\", [(4, 14, \"LOC\")]),\n",
        "    (\"Amazon is facing increased competition from Walmart.\", [(0, 6, \"ORG\"), (33, 39, \"ORG\")]),\n",
        "    (\"The United Nations General Assembly meets annually.\", [(4, 36, \"ORG\")]),\n",
        "    (\"The 2023 Cricket World Cup will be hosted by India.\", [(4, 27, \"EVENT\"), (40, 45, \"GPE\")]),\n",
        "    (\"Tesla's stock prices have fluctuated dramatically.\", [(0, 5, \"ORG\")]),\n",
        "    (\"The Grammy Awards are held every year.\", [(4, 18, \"EVENT\")]),\n",
        "    (\"The Eiffel Tower attracts millions of tourists every year.\", [(4, 15, \"LOC\"), (36, 41, \"DATE\")]),\n",
        "    (\"NASA's Mars Rover is searching for signs of life.\", [(0, 4, \"ORG\")]),\n",
        "    (\"The 2024 U.S. Presidential election is coming up.\", [(4, 26, \"EVENT\"), (30, 34, \"DATE\")]),\n",
        "    (\"Tesla is set to launch its new Cybertruck.\", [(0, 5, \"ORG\"), (30, 34, \"PRODUCT\")])\n",
        "]\n",
        "\n",
        "\n",
        "# Define a comprehensive preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Tokenization and stopword removal\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Initialize lists for storing true and predicted entities\n",
        "all_true_entities = []\n",
        "all_pred_entities = []\n",
        "\n",
        "# Iterate through training data\n",
        "for text, true_entities in training_data:\n",
        "    # Run NER model\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Predicted entities from the model\n",
        "    pred_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    # Store true and predicted entities for all examples\n",
        "    all_true_entities.extend([(ent[0], ent[1], ent[2]) for ent in true_entities])\n",
        "    all_pred_entities.extend([(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents])\n",
        "\n",
        "# Convert to binary classification (1 for correct entity, 0 for incorrect)\n",
        "y_true = [1 if ent in all_true_entities else 0 for ent in all_pred_entities]\n",
        "y_pred = [1 for _ in all_pred_entities]  # Assuming all predictions are correct for now\n",
        "\n",
        "# Calculate Precision, Recall, F1\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApJgEvFfKZ7Q"
      },
      "source": [
        "## Part of Speech Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is same as the code from subset 1 but the training data contains a total of 100 samples. A detailed preprocessing function is provided, which aims at lowering the case of text, scrapping special signs and stopwords from the text respectively which aims at improving the quality of the text before it is fed for training."
      ],
      "metadata": {
        "id": "_J2RRYXt9Mst"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJXXVo58KcT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2bebb4b-790b-45e0-e7bd-f2155f2b9e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m853.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Consolidated Metrics across Training, Validation, and Test Data:\n",
            "Accuracy: 21.6055%\n",
            "Precision: 11.8210%\n",
            "Recall: 21.6055%\n",
            "F1 Score: 15.1812%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load spaCy's POS tagging model\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Sample expanded training data: list of (text, true_pos_tags) pairs\n",
        "training_data = [\n",
        "    (\"She sells seashells by the seashore.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The quick brown fox jumps over the lazy dog.\", ['DET', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"I love coding in Python.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'PROPN']),\n",
        "    (\"Birds fly in the sky.\", ['NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"Alice and Bob went to the market.\", ['PROPN', 'CCONJ', 'PROPN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"Reading books is fun.\", ['VERB', 'NOUN', 'AUX', 'ADJ']),\n",
        "    (\"My car is very fast.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"We are going to the zoo.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"It is raining today.\", ['PRON', 'AUX', 'VERB', 'NOUN']),\n",
        "    (\"Programming languages are interesting.\", ['NOUN', 'NOUN', 'AUX', 'ADJ']),\n",
        "\n",
        "    (\"The cat sleeps on the mat.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"John likes to play soccer.\", ['PROPN', 'VERB', 'PART', 'VERB', 'NOUN']),\n",
        "    (\"She is learning French.\", ['PRON', 'AUX', 'VERB', 'PROPN']),\n",
        "    (\"The weather is nice today.\", ['DET', 'NOUN', 'AUX', 'ADJ', 'NOUN']),\n",
        "    (\"He bought a new laptop yesterday.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN']),\n",
        "    (\"They are swimming in the pool.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The pizza smells delicious.\", ['DET', 'NOUN', 'VERB', 'ADJ']),\n",
        "    (\"Can you help me with this project?\", ['AUX', 'PRON', 'VERB', 'PRON', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"This task is quite difficult.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"He enjoys reading books.\", ['PRON', 'VERB', 'VERB', 'NOUN']),\n",
        "\n",
        "    (\"The dog barked loudly at the strangers.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"I have a meeting tomorrow.\", ['PRON', 'AUX', 'DET', 'NOUN', 'ADJ']),\n",
        "    (\"They will travel to Spain next year.\", ['PRON', 'AUX', 'VERB', 'ADP', 'PROPN', 'ADV', 'NOUN']),\n",
        "    (\"He plays the guitar beautifully.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADV']),\n",
        "    (\"The book on the shelf is mine.\", ['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'AUX', 'PRON']),\n",
        "    (\"Jessica ran a marathon last summer.\", ['PROPN', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN']),\n",
        "    (\"Cooking is a wonderful hobby.\", ['VERB', 'AUX', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"The stars shine brightly in the night sky.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN', 'NOUN']),\n",
        "    (\"I am learning how to code.\", ['PRON', 'AUX', 'VERB', 'ADV', 'ADP', 'VERB']),\n",
        "    (\"The flowers bloom in spring.\", ['DET', 'NOUN', 'VERB', 'ADP', 'NOUN']),\n",
        "\n",
        "    (\"My friends enjoy hiking on weekends.\", ['DET', 'NOUN', 'VERB', 'VERB', 'ADP', 'NOUN']),\n",
        "    (\"Dogs are great companions.\", ['NOUN', 'AUX', 'ADJ', 'NOUN']),\n",
        "    (\"She wrote an amazing story.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"The sun rises in the east.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"He plays soccer every weekend.\", ['PRON', 'VERB', 'NOUN', 'ADV', 'NOUN']),\n",
        "    (\"Reading novels helps improve vocabulary.\", ['VERB', 'NOUN', 'VERB', 'VERB', 'NOUN']),\n",
        "    (\"My family enjoys movie nights.\", ['DET', 'NOUN', 'VERB', 'NOUN', 'NOUN']),\n",
        "    (\"She is very talented in music.\", ['PRON', 'AUX', 'ADV', 'ADJ', 'ADP', 'NOUN']),\n",
        "    (\"We will celebrate his birthday soon.\", ['PRON', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADV']),\n",
        "    (\"The wind blew fiercely during the storm.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "\n",
        "    (\"They went hiking in the mountains.\", ['PRON', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"This recipe is quite easy.\", ['DET', 'NOUN', 'AUX', 'ADV', 'ADJ']),\n",
        "    (\"The teacher explains concepts clearly.\", ['DET', 'NOUN', 'VERB', 'NOUN', 'ADV']),\n",
        "    (\"We have been working on this project.\", ['PRON', 'AUX', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"He prefers tea over coffee.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'NOUN']),\n",
        "    (\"The child laughed joyfully at the joke.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She likes to dance at parties.\", ['PRON', 'VERB', 'PART', 'VERB', 'ADP', 'NOUN']),\n",
        "    (\"They are playing video games right now.\", ['PRON', 'AUX', 'VERB', 'NOUN', 'ADV', 'ADV']),\n",
        "    (\"The cat chased the mouse.\", ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']),\n",
        "    (\"Jack and Jill went up the hill.\", ['PROPN', 'CCONJ', 'PROPN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "\n",
        "    (\"The children are laughing in the park.\", ['DET', 'NOUN', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She loves to read novels.\", ['PRON', 'VERB', 'PART', 'VERB', 'NOUN']),\n",
        "    (\"The fish swims in the ocean.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"He wrote a letter yesterday.\", ['PRON', 'VERB', 'DET', 'NOUN', 'NOUN']),\n",
        "    (\"They are playing soccer after school.\", ['PRON', 'AUX', 'VERB', 'NOUN', 'ADP', 'NOUN']),\n",
        "    (\"The chef prepares delicious meals.\", ['DET', 'NOUN', 'VERB', 'ADJ', 'NOUN']),\n",
        "    (\"We will visit our grandparents next weekend.\", ['PRON', 'AUX', 'VERB', 'PRON', 'NOUN', 'ADV', 'NOUN']),\n",
        "    (\"The dog fetches the ball.\", ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']),\n",
        "    (\"She enjoys painting landscapes.\", ['PRON', 'VERB', 'VERB', 'NOUN']),\n",
        "    (\"The phone rang unexpectedly.\", ['DET', 'NOUN', 'VERB', 'ADV']),\n",
        "\n",
        "    (\"They will join us for dinner.\", ['PRON', 'AUX', 'VERB', 'PRON', 'ADP', 'NOUN']),\n",
        "    (\"He is running very fast.\", ['PRON', 'AUX', 'VERB', 'ADV', 'ADJ']),\n",
        "    (\"The train arrives at the station.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She sings beautifully on stage.\", ['PRON', 'VERB', 'ADV', 'ADP', 'NOUN']),\n",
        "    (\"The baby cried all night.\", ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']),\n",
        "    (\"We are going shopping tomorrow.\", ['PRON', 'AUX', 'VERB', 'VERB', 'NOUN']),\n",
        "    (\"He finished his homework before dinner.\", ['PRON', 'VERB', 'PRON', 'NOUN', 'ADP', 'NOUN']),\n",
        "    (\"The sun sets in the west.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She has a beautiful voice.\", ['PRON', 'AUX', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"The garden is full of flowers.\", ['DET', 'NOUN', 'AUX', 'ADJ', 'ADP', 'NOUN']),\n",
        "\n",
        "    (\"They watched a movie last night.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN']),\n",
        "    (\"The children played happily at the playground.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"We are studying for the exam.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"He kicked the ball into the goal.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She is going to the concert tonight.\", ['PRON', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN', 'ADV']),\n",
        "    (\"The computer crashed during the update.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"I saw a shooting star.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"They will attend the meeting next week.\", ['PRON', 'AUX', 'VERB', 'DET', 'NOUN', 'ADV', 'NOUN']),\n",
        "    (\"The mountain trail is steep.\", ['DET', 'NOUN', 'AUX', 'ADJ']),\n",
        "    (\"He traveled to Paris last summer.\", ['PRON', 'VERB', 'ADP', 'PROPN', 'ADJ', 'NOUN']),\n",
        "\n",
        "    (\"She baked cookies for her friends.\", ['PRON', 'VERB', 'NOUN', 'ADP', 'PRON', 'NOUN']),\n",
        "    (\"The artist painted a stunning mural.\", ['DET', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"We enjoy exploring new places.\", ['PRON', 'VERB', 'VERB', 'ADJ', 'NOUN']),\n",
        "    (\"He repaired the broken fence.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"They discovered a hidden treasure.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"The baby laughed at the puppy.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She danced gracefully across the stage.\", ['PRON', 'VERB', 'ADV', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The team won the championship.\", ['DET', 'NOUN', 'VERB', 'DET', 'NOUN']),\n",
        "    (\"I found a great restaurant.\", ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"He enjoys hiking during the summer.\", ['PRON', 'VERB', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "\n",
        "    (\"The car sped down the highway.\", ['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"She likes to play the piano.\", ['PRON', 'VERB', 'PART', 'VERB', 'DET', 'NOUN']),\n",
        "    (\"They ran a marathon in record time.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'NOUN']),\n",
        "    (\"The flowers bloomed beautifully in spring.\", ['DET', 'NOUN', 'VERB', 'ADV', 'ADP', 'NOUN']),\n",
        "    (\"She is reading a fascinating book.\", ['PRON', 'AUX', 'VERB', 'DET', 'ADJ', 'NOUN']),\n",
        "    (\"The dog wagged its tail excitedly.\", ['DET', 'NOUN', 'VERB', 'PRON', 'NOUN', 'ADV']),\n",
        "    (\"We are planning a trip next month.\", ['PRON', 'AUX', 'VERB', 'DET', 'NOUN', 'ADJ', 'NOUN']),\n",
        "    (\"He solved the puzzle in no time.\", ['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN']),\n",
        "    (\"The storm caused significant damage.\", ['DET', 'NOUN', 'VERB', 'ADJ', 'NOUN']),\n",
        "    (\"They are learning new skills at work.\", ['PRON', 'AUX', 'VERB', 'ADJ', 'NOUN', 'ADP', 'NOUN'])\n",
        "]\n",
        "# Preprocess text: convert to lowercase and shuffle the training data\n",
        "training_data = [(text.lower(), tags) for text, tags in training_data]\n",
        "random.shuffle(training_data)\n",
        "\n",
        "# Split data into training, validation, and test sets (60% train, 20% validation, 20% test)\n",
        "train_data, temp_data = train_test_split(training_data, test_size=0.4, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Initialize lists to store true and predicted POS tags for all sets\n",
        "all_true_pos_tags_train, all_predicted_pos_tags_train = [], []\n",
        "all_true_pos_tags_val, all_predicted_pos_tags_val = [], []\n",
        "all_true_pos_tags_test, all_predicted_pos_tags_test = [], []\n",
        "\n",
        "# Function to preprocess and evaluate POS tagging\n",
        "def process_data(data, all_true_pos_tags, all_predicted_pos_tags):\n",
        "    for text, true_pos_tags in data:\n",
        "        # Special character removal\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        # Process the text with spaCy\n",
        "        doc = nlp(text)\n",
        "        # Extract predicted POS tags and filter out stopwords\n",
        "        predicted_pos_tags = [token.pos_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "        # Extend lists with true and predicted tags for evaluation\n",
        "        all_true_pos_tags.extend(true_pos_tags)\n",
        "        all_predicted_pos_tags.extend(predicted_pos_tags)\n",
        "\n",
        "# Process training, validation, and test data\n",
        "process_data(train_data, all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "process_data(val_data, all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "process_data(test_data, all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Ensure both lists are the same length to avoid ValueError\n",
        "def ensure_equal_length(true_tags, predicted_tags):\n",
        "    if len(true_tags) != len(predicted_tags):\n",
        "        min_length = min(len(true_tags), len(predicted_tags))\n",
        "        true_tags = true_tags[:min_length]\n",
        "        predicted_tags = predicted_tags[:min_length]\n",
        "    return true_tags, predicted_tags\n",
        "\n",
        "# Ensure correct lengths for all sets\n",
        "all_true_pos_tags_train, all_predicted_pos_tags_train = ensure_equal_length(all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "all_true_pos_tags_val, all_predicted_pos_tags_val = ensure_equal_length(all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "all_true_pos_tags_test, all_predicted_pos_tags_test = ensure_equal_length(all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Function to calculate metrics for a dataset\n",
        "def evaluate_metrics(true_tags, predicted_tags):\n",
        "    accuracy = accuracy_score(true_tags, predicted_tags)\n",
        "    precision = precision_score(true_tags, predicted_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, predicted_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, predicted_tags, average='weighted')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate on training, validation, and test sets\n",
        "metrics_train = evaluate_metrics(all_true_pos_tags_train, all_predicted_pos_tags_train)\n",
        "metrics_val = evaluate_metrics(all_true_pos_tags_val, all_predicted_pos_tags_val)\n",
        "metrics_test = evaluate_metrics(all_true_pos_tags_test, all_predicted_pos_tags_test)\n",
        "\n",
        "# Combine all metrics into single print statement\n",
        "total_accuracy = (metrics_train[0] + metrics_val[0] + metrics_test[0]) / 3\n",
        "total_precision = (metrics_train[1] + metrics_val[1] + metrics_test[1]) / 3\n",
        "total_recall = (metrics_train[2] + metrics_val[2] + metrics_test[2]) / 3\n",
        "total_f1 = (metrics_train[3] + metrics_val[3] + metrics_test[3]) / 3\n",
        "\n",
        "# Print consolidated metrics\n",
        "print(\"Consolidated Metrics across Training, Validation, and Test Data:\")\n",
        "print(f\"Accuracy: {total_accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {total_precision * 100:.4f}%\")\n",
        "print(f\"Recall: {total_recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {total_f1 * 100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6flaVXcKdCm"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is same as the code from subset 1 but the training data contains a total of 100 samples. A detailed preprocessing function is provided, which aims at lowering the case of text, scrapping special signs and stopwords from the text respectively which aims at improving the quality of the text before it is fed for training."
      ],
      "metadata": {
        "id": "1-2YKAdW9OEc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcizQDRyKfyP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load a blank model and add text classifier\n",
        "nlpTC = spacy.blank(\"en\")\n",
        "textcat = nlpTC.add_pipe(\"textcat\")\n",
        "\n",
        "# Add labels for classification\n",
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGATIVE\")\n",
        "\n",
        "train_data = [\n",
        "    (\"I'm so frustrated with how slow my internet is.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I'm so happy with my new job!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The customer service at that store is excellent.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The movie was a complete waste of time.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That movie was truly heartwarming and beautiful.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been feeling really down lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The food at the new restaurant was absolutely delicious.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t get the job, and I feel so defeated.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The sunset this evening was breathtaking.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m really upset that I missed the deadline.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"I finally finished the book, and it was such a rewarding read.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"This weather is terrible, I can’t wait for it to end.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The surprise party was such a success!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My laptop crashed again, and I lost all my work.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The concert was absolutely mind-blowing!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been struggling with my workload lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The flowers you sent me are absolutely stunning.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I regret spending money on that product.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just found out I won the contest! I’m over the moon.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"Everything seems to be going wrong lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"You did a fantastic job on that presentation.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m tired of dealing with all this stress.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I couldn’t be happier with how everything turned out.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My car broke down again, and I’m so frustrated.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That was one of the most enjoyable dinners I’ve had in ages.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m feeling really overwhelmed with everything going on.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m incredibly grateful for the support I’ve received.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t enjoy the event; it was a total letdown.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m so proud of everything we’ve accomplished this year.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"That comment really hurt my feelings.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"That was the best coffee I’ve had in a while!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I can’t believe how rude they were to me.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I got a promotion at work, and I couldn’t be more thrilled.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My phone screen cracked, and now I have to get it replaced.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"What a beautiful and sunny day!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m really anxious about everything going on.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"Spending time with family over the holidays was perfect.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t get enough sleep last night, and now I’m exhausted.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"This new app makes my life so much easier.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been really unmotivated lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"That vacation was exactly what I needed.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"That presentation did not go well at all.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I appreciate all the effort you put into this project.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My relationship with my friends hasn’t been great lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’ve made some great new friends recently.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The traffic was horrible, and I barely made it on time.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I had such a fun time with the kids at the park today.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I didn’t enjoy the book at all; it was so boring.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just got my dream job, and I’m beyond excited!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I feel like I’ve been making one mistake after another.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "\n",
        "    (\"I had a terrible experience with the customer service rep.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’ve been feeling so energetic and positive lately!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I'm feeling completely hopeless today.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That was the most amazing concert I’ve ever attended!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I can’t believe I lost my wallet again.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"My birthday party was so much fun, I loved it!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The service at the restaurant was terrible, I’m never going back.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’ve been so productive today, I got everything done!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I had an argument with my best friend, and now I feel awful.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I received a surprise gift, and it made my entire day.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "\n",
        "    (\"I didn’t get the promotion, and now I’m feeling defeated.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That was the best vacation I’ve had in years.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been feeling anxious and restless lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m extremely proud of how far I’ve come.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I don’t know why, but I’m feeling really down today.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"The presentation went really well, I’m so relieved.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m struggling to stay positive with everything going wrong.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I feel incredibly blessed to have such supportive friends.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"My flight got canceled, and now I’m stuck at the airport.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I got a big raise at work, I’m so happy!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "\n",
        "    (\"I’ve been feeling very isolated and alone recently.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just adopted a puppy, and I’m beyond excited!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve had the worst headache all day, it won’t go away.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m thrilled to have finished that project ahead of time.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been getting really stressed about all my deadlines.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That new movie was so entertaining, I loved every minute.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m so frustrated with how long this process is taking.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’ve never been more excited for the future.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"The noise in my neighborhood is driving me crazy.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I finally achieved my fitness goals, I feel amazing.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "\n",
        "    (\"I’ve been having such a hard time balancing work and life.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I got to reconnect with an old friend today, it was so great.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m feeling really insecure about everything right now.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That new café is fantastic, I’ll definitely be going back.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m so upset that my favorite show got canceled.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just finished a great workout, I feel so energized!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m so disappointed in how things turned out.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That speech was truly inspiring, I’m so motivated now.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m incredibly tired of dealing with all these problems.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just finished reading an amazing book, I couldn’t put it down.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "\n",
        "    (\"I feel like everything is falling apart.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just won tickets to see my favorite band live!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’ve been feeling really disconnected from everyone lately.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I just got engaged, and I couldn’t be happier!\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m really disappointed with the service I received today.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m so excited to start this new chapter in my life.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m feeling really anxious about the upcoming event.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"That was the most fun I’ve had in years.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}}),\n",
        "    (\"I’m feeling really low and unsure about everything right now.\", {\"cats\": {\"POSITIVE\": 0, \"NEGATIVE\": 1}}),\n",
        "    (\"I’m grateful for all the blessings in my life.\", {\"cats\": {\"POSITIVE\": 1, \"NEGATIVE\": 0}})\n",
        "]\n",
        "\n",
        "# Define a comprehensive preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens = [token.text for token in nlpTC(text) if not token.is_stop]  # Directly extract tokens\n",
        "    # Join tokens back to a string\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text data from train_data\n",
        "text = [data[0] for data in train_data]\n",
        "labels = [data[1]['cats']['POSITIVE'] for data in train_data] # Extract labels\n",
        "\n",
        "# Vectorize text data using the extracted text list\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5, random_state=42)\n",
        "\n",
        "# Train a Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict sentiments\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.4f}%\")\n",
        "print(f\"Precision: {precision * 100:.4f}%\")\n",
        "print(f\"Recall: {recall * 100:.4f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.4f}%\")"
      ],
      "metadata": {
        "id": "Y5hqfAhXeaLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e6bc2cb-1c2e-4e07-edbc-b35e50ec80b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 80.0000%\n",
            "Precision: 73.3333%\n",
            "Recall: 91.6667%\n",
            "F1 Score: 81.4815%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwFCsIY6KhZI"
      },
      "source": [
        "## Text Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C9DsDHguKi10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d24381a-3939-453a-e370-9f008d30b6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: Renewable energy sources such as solar and wind power offer hope, but their adoption has not been widespread enough to make a significant impact yet.\n",
            " The increasing levels of greenhouse gases in the atmosphere\n",
            "have led to rising global temperatures. Many governments around the world have pledged to reduce carbon emissions, but progress has been slow.\n",
            "\n",
            "Precision: 0.00%\n",
            "Recall: 0.00%\n",
            "F1 Score: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download the punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "# Example text and reference summary\n",
        "text = \"\"\"Climate change is one of the most pressing issues of our time. The increasing levels of greenhouse gases in the atmosphere\n",
        "have led to rising global temperatures. As a result, glaciers are melting, sea levels are rising, and extreme weather events\n",
        "are becoming more frequent. Many governments around the world have pledged to reduce carbon emissions, but progress has been slow.\n",
        "Renewable energy sources such as solar and wind power offer hope, but their adoption has not been widespread enough to make a significant impact yet.\n",
        "Urgent action is needed to address this global crisis before it’s too late.\"\"\"\n",
        "\n",
        "reference_summary = \"\"\"Climate change is caused by greenhouse gases and is leading to rising temperatures and extreme weather.\n",
        "Renewable energy offers hope, but its adoption is slow.\"\"\"\n",
        "\n",
        "# Function to preprocess text: remove special characters, stopwords, and tokenize\n",
        "def preprocess_text(text):\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Keep only alphanumeric characters and spaces\n",
        "    doc = nlp(text.lower())  # Convert to lowercase and process with spaCy\n",
        "    # Remove stopwords and return tokens\n",
        "    tokens = [token.text for token in doc if not token.is_stop]\n",
        "    return tokens\n",
        "\n",
        "# Enhanced extractive summarization function\n",
        "def extractive_summary(text, reference_summary, num_sentences=3):\n",
        "    doc = nlp(text)  # Process the original text\n",
        "    sentences = [sent.text for sent in doc.sents]  # Extract original sentences\n",
        "    preprocessed_sentences = [preprocess_text(sent) for sent in sentences]  # Preprocess each sentence\n",
        "\n",
        "    # Score sentences based on similarity to the reference summary\n",
        "    ref_tokens = preprocess_text(reference_summary)  # Preprocess reference summary\n",
        "    ref_doc = nlp(' '.join(ref_tokens))  # Create a spaCy doc from the preprocessed tokens\n",
        "\n",
        "    # Score sentences based on similarity to the reference summary\n",
        "    sentence_scores = [(sent, nlp(' '.join(preprocess_text(sent))).similarity(ref_doc)) for sent in sentences]\n",
        "    ranked_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select top sentences\n",
        "    top_sentences = [sent[0] for sent in ranked_sentences[:num_sentences]]\n",
        "    return top_sentences\n",
        "\n",
        "# Tokenizing the reference and generated summaries into sentences\n",
        "generated_summary = extractive_summary(text, reference_summary)  # Summary in lowercase\n",
        "reference_sentences = [sent.lower() for sent in sent_tokenize(reference_summary)]  # Reference in lowercase\n",
        "\n",
        "# Convert to binary relevance: 1 if the sentence appears in the reference summary, 0 otherwise\n",
        "y_true = [1 if sent in reference_sentences else 0 for sent in sent_tokenize(text.lower())]\n",
        "y_pred = [1 if sent in generated_summary else 0 for sent in sent_tokenize(text.lower())]\n",
        "\n",
        "# Ensure y_true and y_pred are of the same length\n",
        "if len(y_true) != len(y_pred):\n",
        "    min_length = min(len(y_true), len(y_pred))\n",
        "    y_true = y_true[:min_length]\n",
        "    y_pred = y_pred[:min_length]\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred) * 100\n",
        "recall = recall_score(y_true, y_pred) * 100\n",
        "f1 = f1_score(y_true, y_pred) * 100\n",
        "\n",
        "# Output results\n",
        "print(f\"Generated Summary: {' '.join(generated_summary)}\")\n",
        "print(f\"Precision: {precision:.2f}%\")\n",
        "print(f\"Recall: {recall:.2f}%\")\n",
        "print(f\"F1 Score: {f1:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}