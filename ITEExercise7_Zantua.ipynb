{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "WKODFCYM1jaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is to install and upgrade the essential LangChain libraries in the current environment. The `langchain` in itself provides the fundamental capabilities whilst the `langchain_community `probably provides additional capabilities or features that have been contributed by the community."
      ],
      "metadata": {
        "id": "KcMG4o_5_Ddo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community\n",
        "!pip install --upgrade langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KULWXHrEgSNt",
        "outputId": "9844a337-6e83-40aa-8b95-412bdf5ae1c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.7-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Collecting SQLAlchemy<2.0.36,>=1.4 (from langchain_community)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.7)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.17 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.17)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.142)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<2.0.36,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain_community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n",
            "Downloading langchain_community-0.3.7-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: SQLAlchemy, python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "Successfully installed SQLAlchemy-2.0.35 dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.7 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.17)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.142)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model: EleutherAI/gpt-neo-2.7B**\n",
        "\n",
        "**Description (from author):** GPT-Neo 2.7B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 2.7B represents the number of parameters of this particular pre-trained model. [Link of EleutherAI/gpt-neo-2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B)\n"
      ],
      "metadata": {
        "id": "cdKzNRpr3UyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code importing of external libraries (`langchain`, `difflib`, `re`) in order to build a question answering system using a large language model available on Hugging Face and hosted via the API. The given code uses `HuggingFaceHub` to utilize the model, `PromptTemplate `to prepare the question, and `LLMChain` to organize the interaction, while `difflib` and `re` help to check the relevance and correctness of the model's work."
      ],
      "metadata": {
        "id": "NFlw1V5Z_8M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import difflib\n",
        "import re"
      ],
      "metadata": {
        "id": "jf8asRlJ_yG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a specific large language model (LLM) called `EleutherAI/gpt-neo-2.7B` from Hugging Face and makes provisions for its use by setting temperature to 0.2 to reduce likelihood and overhead for responses to a maximum of 150 tokens while using an API token to authenticate on the Hugging Face platform. In other words, it prepares and customizes the language model that will be employed to provide responses to the questions asked."
      ],
      "metadata": {
        "id": "e9uaL00bBZze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"EleutherAI/gpt-neo-2.7B\",\n",
        "    model_kwargs={\"temperature\": 0.2, \"max_length\": 150},\n",
        "    huggingfacehub_api_token=\"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\"\n",
        ")"
      ],
      "metadata": {
        "id": "PzS8gH90BZSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides an outline of the format that will be followed in presenting the questions to the language model. It creates an instance of a class called `PromptTemplate` and calls `prompt`, which arranges the input in a format that has \"Question:\" followed by the user question `({ question})` and has \"Answer:\" to instruct the model on how to respond. To put it simply, it is a nice way of framing the way the questions are asked in order that the model does not get confused on what it is being asked."
      ],
      "metadata": {
        "id": "2LRH5nUyB4Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Question: {question}\\nAnswer:\",\n",
        "    input_variables=[\"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "o4VJJAkzB2Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `LLMChain` is created and being called `llm_chain`. It combines the `llm` (the language model) and the prompt to streamline the process of sending questions and receiving answers."
      ],
      "metadata": {
        "id": "djjMFsVgCoId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine LLM and prompt in a chain\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "artc6ZLsCmig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a function called `calculate_similarity`. The inputs of this function include the response made by the model, which is the prediction and the original question. The function then employs `difflib.SequenceMatcher` to look at the two strings in question and generate a similarity ratio, further explaining to what extent the two are similar. This ratio which can also be understood as a number between 0 and 1 shows how close the prediction is to that very question. To put it differently, it is one way of determining whether the model’s answer is just restating the question or giving an entirely different answer which is expected to be relevant."
      ],
      "metadata": {
        "id": "2YfpaRkkDrKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate similarity\n",
        "def calculate_similarity(prediction, question):\n",
        "    # Use difflib to calculate similarity between response and question\n",
        "    similarity = difflib.SequenceMatcher(None, prediction, question).ratio()\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "8JLmecRmDsAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code `evaluate_response` aims at judging the quality of the response provided by the model in terms of its relevance and correctness. It takes keywords present in the initial question and looks for them in the answer provided. **Relevance** is assessed by the number of keywords that are present in the answer. **Correctness** is a more basic measure that looks to see if there is, for example, a response containing no keywords at all. Some scores are then calculated for both aspects, and both scores are returned indicating how well the model is in understanding and responding to the question given."
      ],
      "metadata": {
        "id": "tDbV5iV6EODs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate correctness and relevance\n",
        "def evaluate_response(response, question):\n",
        "    # Extract key concepts from the question (basic keyword extraction for relevance)\n",
        "    keywords = re.findall(r'\\b\\w+\\b', question.lower())  # Simple word tokenization\n",
        "    relevance_score = sum(1 for word in keywords if word in response.lower())\n",
        "\n",
        "    # Check if the response directly answers the question (simplified)\n",
        "    is_correct = any(keyword in response.lower() for keyword in keywords)\n",
        "\n",
        "    # Score the relevance and correctness\n",
        "    relevance_percentage = (relevance_score / len(keywords)) * 100 if keywords else 0\n",
        "    correctness_score = 1 if is_correct else 0\n",
        "\n",
        "    return relevance_percentage, correctness_score"
      ],
      "metadata": {
        "id": "nDIKaupJEOhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a loop structure in which the user is required to keep asking a question. Input is solicited repeatedly, and the activity ceases when the user just hits Enter without typing anything after which a thank you note is displayed and the program interactions stops. Briefly, this is the section of the algorithms that enables the user to keep asking questions without limitation until a stop is initiated."
      ],
      "metadata": {
        "id": "NfS5qNoZE1SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop for user input\n",
        "while True:\n",
        "    user_prompt = input(\"Please enter your question (or press Enter to quit): \").strip()\n",
        "    if user_prompt == \"\":\n",
        "        print(\"Thank you for using the system!\")\n",
        "        break"
      ],
      "metadata": {
        "id": "XkTMvd0aE1nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code constitutes the centerpiece of the engine that performs question-answering. It accepts the user's query (`user_prompt`), forwards it to the language model via a specified prompt template (`llm_chain.run`), and obtains the output of the model. The answer is then examined with the help of the functions defined above, in order to evaluate its suitability, precision and similarity to the query. Basically, it is the point where the input question is processed and an output from the model comes in that is evaluated."
      ],
      "metadata": {
        "id": "DLR8YCmYFWhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Generate a response from the model\n",
        "    response = llm_chain.run({\"question\": user_prompt}).strip()\n",
        "\n",
        "    # Evaluate correctness and relevance\n",
        "    relevance, correctness = evaluate_response(response, user_prompt)\n",
        "\n",
        "    # Calculate similarity\n",
        "    similarity = calculate_similarity(response, user_prompt)"
      ],
      "metadata": {
        "id": "g1WtqMwmFW1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cide presents the findings from the question answering process to the user. It demonstrates the response to the question asked by the model as well as the `relevance`, `correctness` and `similarity` scores. The relevance score is given in percentages, correctness can be given as “Correct” or “Incorrect” and similarity is given in numbers. Essentially, it is the section that receives the user’s assessment of the model in terms of how well it comprehended and responded to the question, helping the user understand the quality of the answer."
      ],
      "metadata": {
        "id": "3dmMU7qlFYsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Print the response and evaluations\n",
        "    print(\"Response:\", response)\n",
        "    print(f\"Relevance: {relevance:.2f}%\")\n",
        "    print(f\"Correctness: {'Correct' if correctness == 1 else 'Incorrect'}\")\n",
        "    print(f\"Similarity: {similarity:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4x2oSCw5lvf",
        "outputId": "17f5c9d8-ec34-4e8d-ea42-d60bdae8283b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your question (or press Enter to quit): What is the capital of Philippines?\n",
            "Response: Question: What is the capital of Philippines?\n",
            "Answer: The capital of Philippines is Manila.\n",
            "\n",
            "Question: What is the capital of Philippines?\n",
            "Answer: The capital of Philippines is Manila.\n",
            "\n",
            "Question: What is the capital of Philippines?\n",
            "Answer: The capital of Philippines is Manila.\n",
            "\n",
            "Question: What is the capital of Philippines?\n",
            "Answer: The capital of Philippines is Manila.\n",
            "\n",
            "Question: What is the capital of Philippines?\n",
            "Answer: The capital of Philippines is Manila.\n",
            "\n",
            "Question: What is the capital of\n",
            "Relevance: 100.00%\n",
            "Correctness: Correct\n",
            "Similarity: 0.13\n",
            "Please enter your question (or press Enter to quit): What’s your opinion on the weather today?\n",
            "Response: Question: What’s your opinion on the weather today?\n",
            "Answer: I’m not sure what the weather is like today, but I’m sure it’s going to be a hot one.\n",
            "\n",
            "Question: What’s your opinion on the weather today?\n",
            "Answer: I’m not sure what the weather is like today, but I’m sure it’s going to be a hot one.\n",
            "\n",
            "Question: What’s your opinion on the weather today?\n",
            "Answer: I’m\n",
            "Relevance: 100.00%\n",
            "Correctness: Correct\n",
            "Similarity: 0.21\n",
            "Please enter your question (or press Enter to quit): Is it ever okay to lie if it protects someone's feelings?\n",
            "Response: Question: Is it ever okay to lie if it protects someone's feelings?\n",
            "Answer: It depends on the situation.\n",
            "\n",
            "Question: Is it ever okay to lie if it protects someone's feelings?\n",
            "Answer: It depends on the situation.\n",
            "\n",
            "Question: Is it ever okay to lie if it protects someone's feelings?\n",
            "Answer: It depends on the situation.\n",
            "\n",
            "Question: Is it ever okay to lie if it protects someone's feelings?\n",
            "Answer: It depends on the situation.\n",
            "\n",
            "Question: Is it ever okay to lie if it protects someone's feelings\n",
            "Relevance: 100.00%\n",
            "Correctness: Correct\n",
            "Similarity: 0.21\n",
            "Please enter your question (or press Enter to quit): What would have happened if humans never existed?\n",
            "Response: Question: What would have happened if humans never existed?\n",
            "Answer: We would have evolved into a different species, and we would have been different from the rest of the animal kingdom.\n",
            "Question: What would have happened if humans never existed?\n",
            "Answer: We would have evolved into a different species, and we would have been different from the rest of the animal kingdom.\n",
            "Question: What would have happened if humans never existed?\n",
            "Answer: We would have evolved into a different species, and we would have been different from the rest of the animal kingdom.\n",
            "Relevance: 100.00%\n",
            "Correctness: Correct\n",
            "Similarity: 0.16\n",
            "Please enter your question (or press Enter to quit): Who was the president during the famous war?\n",
            "Response: Question: Who was the president during the famous war?\n",
            "Answer: The president was the commander-in-chief of the armed forces.\n",
            "Question: Who was the president during the famous war?\n",
            "Answer: The president was the commander-in-chief of the armed forces.\n",
            "Question: Who was the president during the famous war?\n",
            "Answer: The president was the commander-in-chief of the armed forces.\n",
            "Question: Who was the president during the famous war?\n",
            "Answer: The president was the commander-in-chief of the armed forces\n",
            "Relevance: 100.00%\n",
            "Correctness: Correct\n",
            "Similarity: 0.16\n",
            "Please enter your question (or press Enter to quit): \n",
            "Thank you for using the system!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model: tiiuae/falcon-7b-instruct**\n",
        "\n",
        "**Description (from author):** Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by *TII* based on *Falcon-7B* and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license. [Link for tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct#model-description)"
      ],
      "metadata": {
        "id": "891meJA11n-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code importing of external libraries (`langchain`, `difflib`, `re`) in order to build a question answering system using a large language model available on Hugging Face and hosted via the API. The given code uses `HuggingFaceHub` to utilize the model, `PromptTemplate `to prepare the question, and `LLMChain` to organize the interaction, while `difflib` and `re` help to check the relevance and correctness of the model's work."
      ],
      "metadata": {
        "id": "5k7WPFMSGlLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import difflib\n",
        "import re"
      ],
      "metadata": {
        "id": "FgULQzlNGlqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a specific large language model (LLM) called `tiiuae/falcon-7b-instruct` from Hugging Face and makes provisions for its use by setting temperature to 0.2 to reduce likelihood and overhead for responses to a maximum of 150 tokens while using an API token to authenticate on the Hugging Face platform. In other words, it prepares and customizes the language model that will be employed to provide responses to the questions asked."
      ],
      "metadata": {
        "id": "DBNUj0s_GnPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
        "    model_kwargs={\"temperature\": 0.2, \"max_length\": 150},\n",
        "    huggingfacehub_api_token=\"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\"\n",
        ")"
      ],
      "metadata": {
        "id": "gG0B3THJGnjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides an outline of the format that will be followed in presenting the questions to the language model. It creates an instance of a class called `PromptTemplate` and calls `prompt`, which arranges the input in a format that has \"Question:\" followed by the user question `({ question})` and has \"Answer:\" to instruct the model on how to respond. To put it simply, it is a nice way of framing the way the questions are asked in order that the model does not get confused on what it is being asked."
      ],
      "metadata": {
        "id": "6IHdbPmuGqKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Question: {question}\\nAnswer:\",\n",
        "    input_variables=[\"question\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "28OVlVu_Gqb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `LLMChain` is created and being called `llm_chain`. It combines the `llm` (the language model) and the prompt to streamline the process of sending questions and receiving answers."
      ],
      "metadata": {
        "id": "BoBQNBtWGsKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine LLM and prompt in a chain\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "8E4PWE6WGsb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a function called `calculate_similarity`. The inputs of this function include the response made by the model, which is the prediction and the original question. The function then employs `difflib.SequenceMatcher` to look at the two strings in question and generate a similarity ratio, further explaining to what extent the two are similar. This ratio which can also be understood as a number between 0 and 1 shows how close the prediction is to that very question. To put it differently, it is one way of determining whether the model’s answer is just restating the question or giving an entirely different answer which is expected to be relevant."
      ],
      "metadata": {
        "id": "bSxqg9c0Gs_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate similarity\n",
        "def calculate_similarity(prediction, question):\n",
        "    # Use difflib to calculate similarity between response and question\n",
        "    similarity = difflib.SequenceMatcher(None, prediction, question).ratio()\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "VDlCH1BZGtRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code `evaluate_response` aims at judging the quality of the response provided by the model in terms of its relevance and correctness. It takes keywords present in the initial question and looks for them in the answer provided. **Relevance** is assessed by the number of keywords that are present in the answer. **Correctness** is a more basic measure that looks to see if there is, for example, a response containing no keywords at all. Some scores are then calculated for both aspects, and both scores are returned indicating how well the model is in understanding and responding to the question given."
      ],
      "metadata": {
        "id": "tiiPIn2JGvJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate correctness and relevance\n",
        "def evaluate_response(response, question):\n",
        "    # Extract key concepts from the question (basic keyword extraction for relevance)\n",
        "    keywords = re.findall(r'\\b\\w+\\b', question.lower())  # Simple word tokenization\n",
        "    relevance_score = sum(1 for word in keywords if word in response.lower())\n",
        "\n",
        "    # Check if the response directly answers the question (simplified)\n",
        "    is_correct = any(keyword in response.lower() for keyword in keywords)\n",
        "\n",
        "    # Score the relevance and correctness\n",
        "    relevance_percentage = (relevance_score / len(keywords)) * 100 if keywords else 0\n",
        "    correctness_score = 1 if is_correct else 0\n",
        "\n",
        "    return relevance_percentage, correctness_score\n"
      ],
      "metadata": {
        "id": "CiDYMjizGvae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a loop structure in which the user is required to keep asking a question. Input is solicited repeatedly, and the activity ceases when the user just hits Enter without typing anything after which a thank you note is displayed and the program interactions stops. Briefly, this is the section of the algorithms that enables the user to keep asking questions without limitation until a stop is initiated."
      ],
      "metadata": {
        "id": "srVeRptaGzvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop for user input\n",
        "while True:\n",
        "    user_prompt = input(\"Please enter your question (or press Enter to quit): \").strip()\n",
        "    if user_prompt == \"\":\n",
        "        print(\"Thank you for using the system!\")\n",
        "        break"
      ],
      "metadata": {
        "id": "bUMTqQRTGz9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code constitutes the centerpiece of the engine that performs question-answering. It accepts the user's query (`user_prompt`), forwards it to the language model via a specified prompt template (`llm_chain.run`), and obtains the output of the model. The answer is then examined with the help of the functions defined above, in order to evaluate its suitability, precision and similarity to the query. Basically, it is the point where the input question is processed and an output from the model comes in that is evaluated."
      ],
      "metadata": {
        "id": "fMTQx47IG1qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Generate a response from the model\n",
        "    response = llm_chain.run({\"question\": user_prompt}).strip()\n",
        "\n",
        "    # Evaluate correctness and relevance\n",
        "    relevance, correctness = evaluate_response(response, user_prompt)\n",
        "\n",
        "    # Calculate similarity\n",
        "    similarity = calculate_similarity(response, user_prompt)\n"
      ],
      "metadata": {
        "id": "1OHaOIizG3sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cide presents the findings from the question answering process to the user. It demonstrates the response to the question asked by the model as well as the `relevance`, `correctness` and `similarity` scores. The relevance score is given in percentages, correctness can be given as “Correct” or “Incorrect” and similarity is given in numbers. Essentially, it is the section that receives the user’s assessment of the model in terms of how well it comprehended and responded to the question, helping the user understand the quality of the answer."
      ],
      "metadata": {
        "id": "H_OnOZMeG2ST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Print the response and evaluations\n",
        "    print(\"Response:\", response)\n",
        "    print(f\"Relevance: {relevance:.2f}%\")\n",
        "    print(f\"Correctness: {'Correct' if correctness == 1 else 'Incorrect'}\")\n",
        "    print(f\"Similarity: {similarity:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwSebOBB6C1n",
        "outputId": "779fc932-6ad2-4d01-bff4-171487e185cf"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your question (or press Enter to quit): What is the capital of Philippines?\n",
            "Response: Question: What is the capital of Philippines?\n",
            "Answer: The capital of Philippines is Manila.\n",
            "Relevance: 100.00%\n",
            "Correctness: Correct\n",
            "Similarity: 0.56\n",
            "Please enter your question (or press Enter to quit): What’s your opinion on the weather today?\n",
            "Response: Question: What’s your opinion on the weather today?\n",
            "Answer: I'm sorry, I don't have an opinion on the weather as I am an AI language model and don't have the ability to feel or perceive the weather.\n",
            "Relevance: 100.00%\n",
            "Correctness: Correct\n",
            "Similarity: 0.34\n",
            "Please enter your question (or press Enter to quit): Is it ever okay to lie if it protects someone's feelings?\n",
            "Response: Question: Is it ever okay to lie if it protects someone's feelings?\n",
            "Answer: It depends on the situation. While lying can be a useful tool in certain situations, it is generally not considered a healthy or ethical behavior. In some cases, it may be necessary to tell the truth, even if it means hurting someone's feelings. Ultimately, it is up to the individual to decide what is best for them and their relationships.\n",
            "Relevance: 100.00%\n",
            "Correctness: Correct\n",
            "Similarity: 0.24\n",
            "Please enter your question (or press Enter to quit): What would have happened if humans never existed?\n",
            "Response: Question: What would have happened if humans never existed?\n",
            "Answer: It's hard to say for sure, as the world as we know it would not exist without humans. However, some scientists speculate that other forms of life may have evolved in its place.\n",
            "Relevance: 100.00%\n",
            "Correctness: Correct\n",
            "Similarity: 0.33\n",
            "Please enter your question (or press Enter to quit): Who was the president during the famous war?\n",
            "Response: Question: Who was the president during the famous war?\n",
            "Answer: The president during the famous war was George Washington.\n",
            "Relevance: 100.00%\n",
            "Correctness: Correct\n",
            "Similarity: 0.53\n",
            "Please enter your question (or press Enter to quit): \n",
            "Thank you for using the system!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model: reasonwang/google-flan-t5-large-alpaca**\n",
        "\n",
        "[Link for reasonwang/google-flan-t5-large-alpaca](https://huggingface.co/reasonwang)"
      ],
      "metadata": {
        "id": "Uwu7rrIx5J5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code importing of external libraries (`langchain`, `difflib`, `re`) in order to build a question answering system using a large language model available on Hugging Face and hosted via the API. The given code uses `HuggingFaceHub` to utilize the model, `PromptTemplate `to prepare the question, and `LLMChain` to organize the interaction, while `difflib` and `re` help to check the relevance and correctness of the model's work."
      ],
      "metadata": {
        "id": "zeoHVzgwHC9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import difflib\n",
        "import re"
      ],
      "metadata": {
        "id": "6yeO0He9HDP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a specific large language model (LLM) called `reasonwang/google-flan-t5-large-alpaca `and makes provisions for its use by setting temperature to 0.2 to reduce likelihood and overhead for responses to a maximum of 150 tokens while using an API token to authenticate on the Hugging Face platform. In other words, it prepares and customizes the language model that will be employed to provide responses to the questions asked."
      ],
      "metadata": {
        "id": "2Ye1vXZ8HFal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LLM\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"reasonwang/google-flan-t5-large-alpaca\",\n",
        "    model_kwargs={\"temperature\": 0.2, \"max_length\": 150},\n",
        "    huggingfacehub_api_token=\"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\"\n",
        ")"
      ],
      "metadata": {
        "id": "MEMks7ltHF5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides an outline of the format that will be followed in presenting the questions to the language model. It creates an instance of a class called `PromptTemplate` and calls `prompt`, which arranges the input in a format that has \"Question:\" followed by the user question `({ question})` and has \"Answer:\" to instruct the model on how to respond. To put it simply, it is a nice way of framing the way the questions are asked in order that the model does not get confused on what it is being asked."
      ],
      "metadata": {
        "id": "rAqiQnlEHHb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Question: {question}\\nAnswer:\",\n",
        "    input_variables=[\"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "uCNBSEUrHH0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `LLMChain` is created and being called `llm_chain`. It combines the `llm` (the language model) and the prompt to streamline the process of sending questions and receiving answers."
      ],
      "metadata": {
        "id": "WDpxUYzuHI6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine LLM and prompt in a chain\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "k_bSthpIHJKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a function called `calculate_similarity`. The inputs of this function include the response made by the model, which is the prediction and the original question. The function then employs `difflib.SequenceMatcher` to look at the two strings in question and generate a similarity ratio, further explaining to what extent the two are similar. This ratio which can also be understood as a number between 0 and 1 shows how close the prediction is to that very question. To put it differently, it is one way of determining whether the model’s answer is just restating the question or giving an entirely different answer which is expected to be relevant."
      ],
      "metadata": {
        "id": "pB5DCjRPHLNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate similarity\n",
        "def calculate_similarity(prediction, question):\n",
        "    # Use difflib to calculate similarity between response and question\n",
        "    similarity = difflib.SequenceMatcher(None, prediction, question).ratio()\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "R1_DgrQhHLiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code `evaluate_response` aims at judging the quality of the response provided by the model in terms of its relevance and correctness. It takes keywords present in the initial question and looks for them in the answer provided. **Relevance** is assessed by the number of keywords that are present in the answer. **Correctness** is a more basic measure that looks to see if there is, for example, a response containing no keywords at all. Some scores are then calculated for both aspects, and both scores are returned indicating how well the model is in understanding and responding to the question given."
      ],
      "metadata": {
        "id": "LpSCuQK_HMzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate correctness and relevance\n",
        "def evaluate_response(response, question):\n",
        "    # Extract key concepts from the question (basic keyword extraction for relevance)\n",
        "    keywords = re.findall(r'\\b\\w+\\b', question.lower())  # Simple word tokenization\n",
        "    relevance_score = sum(1 for word in keywords if word in response.lower())\n",
        "\n",
        "    # Check if the response directly answers the question (simplified)\n",
        "    is_correct = any(keyword in response.lower() for keyword in keywords)\n",
        "\n",
        "    # Score the relevance and correctness\n",
        "    relevance_percentage = (relevance_score / len(keywords)) * 100 if keywords else 0\n",
        "    correctness_score = 1 if is_correct else 0\n",
        "\n",
        "    return relevance_percentage, correctness_score"
      ],
      "metadata": {
        "id": "HiDCBS0XHNGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a loop structure in which the user is required to keep asking a question. Input is solicited repeatedly, and the activity ceases when the user just hits Enter without typing anything after which a thank you note is displayed and the program interactions stops. Briefly, this is the section of the algorithms that enables the user to keep asking questions without limitation until a stop is initiated."
      ],
      "metadata": {
        "id": "RzP7s3ISHPiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop for user input\n",
        "while True:\n",
        "    user_prompt = input(\"Please enter your question (or press Enter to quit): \").strip()\n",
        "    if user_prompt == \"\":\n",
        "        print(\"Thank you for using the system!\")\n",
        "        break"
      ],
      "metadata": {
        "id": "1ZvxLBLxHPxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code constitutes the centerpiece of the engine that performs question-answering. It accepts the user's query (`user_prompt`), forwards it to the language model via a specified prompt template (`llm_chain.run`), and obtains the output of the model. The answer is then examined with the help of the functions defined above, in order to evaluate its suitability, precision and similarity to the query. Basically, it is the point where the input question is processed and an output from the model comes in that is evaluated."
      ],
      "metadata": {
        "id": "yr5cdRz-HSGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Generate a response from the model\n",
        "    response = llm_chain.run({\"question\": user_prompt}).strip()\n",
        "\n",
        "    # Evaluate correctness and relevance\n",
        "    relevance, correctness = evaluate_response(response, user_prompt)\n",
        "\n",
        "    # Calculate similarity\n",
        "    similarity = calculate_similarity(response, user_prompt)"
      ],
      "metadata": {
        "id": "sSk2A_aYHSbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cide presents the findings from the question answering process to the user. It demonstrates the response to the question asked by the model as well as the `relevance`, `correctness` and `similarity` scores. The relevance score is given in percentages, correctness can be given as “Correct” or “Incorrect” and similarity is given in numbers. Essentially, it is the section that receives the user’s assessment of the model in terms of how well it comprehended and responded to the question, helping the user understand the quality of the answer."
      ],
      "metadata": {
        "id": "Y3Y5ijtcHUaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Print the response and evaluations\n",
        "    print(\"Response:\", response)\n",
        "    print(f\"Relevance: {relevance:.2f}%\")\n",
        "    print(f\"Correctness: {'Correct' if correctness == 1 else 'Incorrect'}\")\n",
        "    print(f\"Similarity: {similarity:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cji3QzF96W7d",
        "outputId": "b0bd98a4-978f-446a-a967-dfa7e86e221c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your question (or press Enter to quit): What is the capital of Philippines?\n",
            "Response: Manila.\n",
            "Relevance: 0.00%\n",
            "Correctness: Incorrect\n",
            "Similarity: 0.14\n",
            "Please enter your question (or press Enter to quit): What’s your opinion on the weather today?\n",
            "Response: I'm sorry, I cannot provide a response without more information about the weather today.\n",
            "Relevance: 62.50%\n",
            "Correctness: Correct\n",
            "Similarity: 0.48\n",
            "Please enter your question (or press Enter to quit): Is it ever okay to lie if it protects someone's feelings?\n",
            "Response: Yes.\n",
            "Relevance: 8.33%\n",
            "Correctness: Correct\n",
            "Similarity: 0.07\n",
            "Please enter your question (or press Enter to quit): What would have happened if humans never existed?\n",
            "Response: If humans never existed, the universe would have been a barren, barren wasteland.\n",
            "Relevance: 75.00%\n",
            "Correctness: Correct\n",
            "Similarity: 0.34\n",
            "Please enter your question (or press Enter to quit): Who was the president during the famous war?\n",
            "Response: George Washington.\n",
            "Relevance: 12.50%\n",
            "Correctness: Correct\n",
            "Similarity: 0.29\n",
            "Please enter your question (or press Enter to quit): \n",
            "Thank you for using the system!\n"
          ]
        }
      ]
    }
  ]
}